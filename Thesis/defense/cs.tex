\section{Compressed Sensing: Support recovery}
\subsection{Main result: Comparison with known limits}
\begin{frame}{Compressed Sensing}
\begin{equation*}
\mathbf{y=Ax +w}
\end{equation*}


\begin{itemize}
\item $\mbf{x}$ -$N \times 1$ sparse signal
\item $\mbf{A}$ -$M \times N$ measurement matrix
\item $\mbf{w}$ -additive noise
\item $\mbf{y}$ -$M \times 1$ measurement vector
\onslide<2->
\item $\text{supp}(\mbf{x})\coleq \{i: x_i\neq 0, i\in [N]\}$
\item $K=\card{\text{supp}(\mbf{x})}$
\item Sparsity- $K\ll N$ 
\end{itemize}
\end{frame}

%-------------------------------------------25@#$@#$^%$##$%^%$^%^_----------------------------------------------------------------
\begin{frame}{Problem Statement}
\begin{itemize}
\item Decoder: Given $\mbf{y}$ reconstruct the vector $\mbf{x}$ denoted by $\widehat{\mbf{x}}$
\item Prob. of failure of support recovery $\mbb{P}_{F}\coleq \text{Pr}(\text{supp}(\widehat{\mbf{x}})\neq \text{supp}(\mbf{x}))$
\item Metrics of interest:
\begin{itemize}
\item Sample complexity ($M$)
\item Decoding complexity
\item $\mbb{P}_{F}$
\end{itemize} 
\end{itemize}
\vspace{5ex}

\onslide<2->
\begin{block}{Objective}
Devise a scheme with minimizing num. of measurements $M$ and decoding complexity such that $\mbb{P}_{F}\rightarrow 0$ as $N (\text{and } K) \rightarrow \infty$
\end{block}
\end{frame}
%-------------------------------------------25@#$@#$^%$##$%^%$^%^_----------------------------------------------------------------
%\subsection{Known Limits}
\begin{frame}
\begin{block}{Optimal order for Support Recovery [1]}
\begin{itemize}
\item In the sub-linear sparsity regime, $K=o(N)$, necessary and sufficient conditions are shown to be:
\begin{equation*}
C_1 K\log\left(\frac{N}{K}\right)<M<C_2 K\log\left(\frac{N}{K}\right)
\end{equation*}
\item In the linear sparsity regime, $K=\alpha N,$ it was shown that $M=\Theta(N)$ measurements are sufficient for asymptotically reliable recovery. 
 \end{itemize}
\end{block}
\vspace{7ex}

\onslide<2->{
\begin{itemize}
\item In [1], the minimum value of the signal space affects the bounds on $M$
\end{itemize}
\begin{align*}
x_i\in\mc{X}&\defeq\{A e^{i\theta}: A\in \mc{A},\theta\in \Omega\}\cup \{0\} ,\\
\mc{A}&=\{A_{\min}+\rho l\}_{l=0}^{L_1}, \Omega =\left\lbrace 2\pi l/L_2\right\rbrace_{l=0}^{L_2}
\end{align*}
}

[1] Information Theoretic Limits of Support Recovery- Wainwright-2007
\end{frame}
%-------------------------------------------25@#$@#$^%$##$%^%$^%^_----------------------------------------------------------------

%\subsection{Main Result}
\begin{frame}{Main result}

\begin{block}{Sub-linear Sparsity-Optimal Sample and Decoding Complexities}
In the sub-linear sparsity regime, for a given SNR of $\frac{A^{2}_{\text{min}}}{\sigma^{2}}$, our scheme has 
\begin{itemize}
\item Sample complexity of $M=c_1 K\log (\frac{c_2 N}{K})$
\item Decoding complexity of $O\left(K\log(\frac{N}{K})\right)$ 
\item $\mbb{P}_{\text{F}}\rightarrow 0$ asymptotically in $K$
\end{itemize} 
where the constants $c_{1}$ and $c_{2}$ are dependent on SNR, desired rate of decay of $\mbb{P}_{\text{F}}$ and left degree $\ell$.
\end{block}
\onslide<2->
\begin{block}{Linear Sparsity Regime}
In the linear sparsity regime our scheme has 
\begin{itemize}
\item Sample complexity of $M=c_3 K\log K$
\item Decoding complexity of $O\left(K\log(K)\right)$ 
\item $\mbb{P}_{\text{F}}\rightarrow 0$ asymptotically in $K$
\end{itemize} 
where the constant $c_{3}>1$ is a parameter dependent on left degree $\ell$.
\end{block}
\end{frame}
%-------------------------------------------25@#$@#$^%$##$%^%$^%^_----------------------------------------------------------------

%\subsection{Prior Work}
\begin{frame}\frametitle{Prior Work}
\begin{itemize}
\item Zhang and Pfister, ``Verification Decoding of High-Rate LDPC Codes With Applications in Compressed Sensing", 2008
\item  Jafarpour, Xu, Hassibi and Calderbank, ``Efficient and robust compressed sensing using optimized expander graphs", 2009
\begin{itemize}
\item Sample complexity of $O(K\log N)$ and decoding complexity of $O(K)$ for noiseless setting
\end{itemize}
\item Dimakis, Smarandache and Vontobel, ``LDPC codes for Compressed Sensing", 2011

\vspace{3ex}
\onslide<2->
\item Li, Pedarsani and Ramchandran, ``Sub-linear compressed sensing for support recovery using sparse-graph codes", 2014[LPR14]
\begin{itemize}
\item Introduced sparse-graph codes peeling decoder framework to CS
\item Sample and measurement complexities of $O(K\log N)$ for noisy setting
\item Sample and measurement complexities of $2K$ and $O(K)$ for noiseless setting
\end{itemize}
\end{itemize}
\end{frame}
%-------------------------------------------25@#$@#$^%$##$%^%$^%^_----------------------------------------------------------------

\subsection{Design scheme}
%\subsection{Sensing Matrix}
\begin{frame}{Graphical Representation}
$(N,\ell,r,\mbf{S})$ ensemble. $\ell N=rM_1$. 
\onslide<2->{$\text{dim}(\mbf{S})=P\times r$.}
\begin{figure}
\scalebox{1.3}{\input{\cs_figpath/TannerGraph.tex}}
\end{figure}
\end{frame}
%-------------------------------------------25@#$@#$^%$##$%^%$^%^_----------------------------------------------------------------

\begin{frame}{Matrix Representation}
$(N,\ell,r,\mbf{S})$ ensemble. 
\begin{itemize}
\item $\mbf{H}$ be the adjacency matrix (binning operation)- $M_1 \times N$
\item $\mbf{S}$ be the bin-detection matrix at each bin - $P \times r$
\end{itemize}
\begin{align*}
\mbf{\tilde{y}=H(x)}&= 
\begin{bmatrix}
   \tilde{\mbf{ y}}_{1} \\
   \tilde{ \mbf{y}}_{2} \\
    \vdots \\
   \tilde{\mbf{y}}_{M_1}
\end{bmatrix}
,\text{dim}(\tilde{\mbf{y}}_{i} )=r \times 1,
\\
\vspace{10pt}
\mbf{y}&= 
\begin{bmatrix}
   \mbf{ y}_{1}\\
    \mbf{y}_{2}  \\
    \vdots \\
    \mbf{y}_{M_1}
\end{bmatrix}
, \text{where } \mbf{y}_i= \mbf{S \tilde{y}}_{i}, \text{dim} (\mbf{y}_i)=P \times 1   
\end{align*}
\begin{itemize}
\item We define a tensor operation such that 
\begin{equation*}
\mbf{y=(S\boxplus H)  x}
\end{equation*}
\end{itemize}
\end{frame}
%-------------------------------------------25@#$@#$^%$##$%^%$^%^_----------------------------------------------------------------

\begin{frame}{Tensor Operation}
\begin{itemize}
\item Sensing matrix $\mbf{A}_{M_{1}P\times N}= S_{P\times r}\boxplus H_{M_{1}\times N}$ where
\vspace{2ex}
\onslide<2->
\item $\forall i\in [1:M_1]$, define a $P\times N$ matrix
\begin{equation*}
\mbf{S}_i=\mbf{h}_i \boxtimes \mbf{S}\defeq [\mbf{0},\ldots,\mbf{0},\mbf{s}_1,\mbf{0},\ldots,\mbf{s}_2,\ldots,\mbf{0},\mbf{s}_r,\mbf{0}]
\end{equation*}
where the $r$ columns are placed in the $r$ non-zero indices of $\mbf{h}_i$.
\vspace{2ex}
\item $S\boxplus H=\begin{bmatrix}
\mbf{S}_1\\
\mbf{S}_2\\
\vdots\\
\mbf{S}_{M_1}
\end{bmatrix}
$
\end{itemize}
\end{frame}
%-------------------------------------------25@#$@#$^%$##$%^%$^%^_----------------------------------------------------------------

\begin{frame}{Example}
\begin{columns}
\begin{column}{0.5\textwidth}
\[\mbf{H} = \begin{bmatrix}
1 & 0 & 0 & 1 & 0 & 1    \\
0 & 1 & 1 & 0 & 1 & 0 \\
1 & 1 & 0 & 1 & 0 & 0 \\
0 & 0 & 1 & 0 & 1 & 1
\end{bmatrix} \] 
\onslide<2->{
and 
\[ \mbf{S} = \begin{bmatrix}
+1 & -1 & -1\\
-1 & +1 & -1
\end{bmatrix}. \]}
\onslide<3->{
Sensing matrix $\bf A$ with $M = 8$: 
\[ \mbf{A = H \boxplus S} \ = \begin{bmatrix}
+1 & \ \ 0 & \ \ 0 & -1 & \ \ 0 & -1\\
-1 & \ \ 0 & \ \ 0 & +1 & \ \ 0 & -1\\
\ \ 0 & +1 & -1 & \ \ 0 & -1 & \ \ 0\\
\ \ 0 & -1 & +1 & \ \ 0 & -1 & \ \ 0\\
+1 & -1 & \ \ 0 & -1 & \ \ 0 &\ \ 0\\
-1 & +1 & \ \ 0 & -1 & \ \ 0 &\ \ 0\\
\ \ 0 & \ \ 0 & +1 & \ \ 0 & -1 & -1\\
\ \ 0 & \ \ 0 & -1 & \ \ 0 & +1 & -1
\end{bmatrix}
 \]
 }
\end{column}
\begin{column}{0.48\textwidth}
\only<1-2>{
\begin{figure}
\scalebox{3}{\input{\cs_figpath/TannerMatrixExample}}
\end{figure}
}
\only<3>{
\begin{figure}
\scalebox{3}{\input{\cs_figpath/SensingGraph}}
\end{figure}
}
\end{column}
\end{columns}
\end{frame}
%-------------------------------------------25@#$@#$^%$##$%^%$^%^_----------------------------------------------------------------

%\subsection{Decoding}
\begin{frame}{Bin Decoding}
At each bin, input to the decoder is $\mbf{y}_i=\sum_{j=1}^{r}x_{\mbf{h}_{i}^{j}}\mbf{s}_j+\mbf{w}_i$
\begin{itemize}
\onslide<2->
\item Zero-ton: Is it just noise?
\begin{equation*}
\widehat{\mc{H}}_i=\mc{H}_{Z}, ~~\text{if } \frac{1}{P}\norm{\mbf{y}_i}^2\leq (1+\gamma)\sigma^2
\end{equation*}
\onslide<3->
\item Singleton: If a single variable is non-zero? $\mbf{y}_i=x_j\mbf{s}_j+\mbf{w}_i$
\begin{align*}
\alpha_{k}&=\frac{\mbf{s}_k^{\dagger}\mbf{y}_i}{\norm{\mbf{s}_k}^2}\\
\hat{k}&=\arg \min_{k}~~ \norm{\mbf{y}_i-\alpha_{k}s_k}\\
\hat{x}[\hat{k}]&=\arg \min_{x\in\mc{X}} \norm{x-\alpha_{\hat{k}}}
\end{align*}
\onslide<4->
\item Multi-ton: More than one non-zero variable?
\begin{equation*}
\widehat{\mc{H}}_i=\mc{H}_{S}(\hat{k},\hat{x}[\hat{k}]), ~~\text{if } \frac{1}{P}\norm{\mbf{y}_{i}-\hat{x}[\hat{k}]\mbf{s}_{\hat{k}}}^2\leq (1+\gamma)\sigma^2
\end{equation*}
\end{itemize}
\end{frame}
%-------------------------------------------25@#$@#$^%$##$%^%$^%^_----------------------------------------------------------------

\begin{frame}{Peeling Decoding}
\begin{columns}
\column{0.55\textwidth}
%\begin{algorithmic}
%\While {$\exists i\in[M_1]: \mc{H}_i=\mc{H}_Z ~\text{or } \mc{H}_S $, }
%\If {$\mc{H}_i=\mc{H}_Z$}
%    \State Remove the bin $i$\\
%   \hspace{2ex} Assign $0$ to all the variables connected
%\ElsIf {$\mc{H}_i=\mc{H}_S(k,x[k])$}   
%   \hspace{4ex} Assign $x[k]$ to $k^{\text{th}}$ variable in bin $i$\\
%  \hspace{2ex} Subtract $x[k]\mbf{s}_k$ from connected $\mbf{y}_i$  \\
%  \hspace{2ex} Remove the bin and all variables connected
%\EndIf
%\EndWhile
%\end{algorithmic}


\column{0.45\textwidth}
\begin{figure}
\scalebox{5}{\input{\cs_figpath/PeelingAnimation}}
\end{figure}

\end{columns}
\end{frame}
%-------------------------------------------25@#$@#$^%$##$%^%$^%^_----------------------------------------------------------------

\subsection{Analysis}
%\subsection{Peeling Decoder}
\begin{frame}{Oracle based Peeling Decoder}
\begin{itemize}
\item Assume the hypothesis detection in each bin decoder is correct
\item Equivalence to peeling decoder on pruned graph- all zero variables are removed
\end{itemize}
\begin{block}{Equivalence to $(N,l,r)$ LDPC on BEC($\epsilon=\frac{K}{N}$)}
If $\text{supp}(\mbf{x})=\{i:y_i=\mc{E}\}$, then $P^{(i)}_{\text{BEC}}(\mbf{y})=P^{(i)}_{\text{SR}}(\mbf{z})$  for $\mbf{z=Hx}$.
\end{block}
\onslide<2->
\begin{itemize}
\item Choose $M_1=\eta K$ thus $r=\frac{\ell N}{\eta K}$
\end{itemize}
\vspace{1ex}
\begin{block}{DE for Peeling decoder on LDPC -BEC channel}
Fractional number of degree one checks remaining
\begin{equation*}
\tilde{R}_1(y)=r\epsilon y^{l-1}[y-1+(1-\epsilon y^{l-1})^{r-1}]
\end{equation*}
where $\epsilon=\frac{K}{N}$ and $r=\frac{\ell N}{\eta K}$
\end{block}
\end{frame}
%-------------------------------------------25@#$@#$^%$##$%^%$^%^_----------------------------------------------------------------

\begin{frame}{}
\begin{block}{Peeling threshold}
$\eta^{\text{Th}}$ is defined to be the minimum value of $\eta$ for which there is no non-zero solution for the equation:
\begin{align*}
y&=\lim_{\frac{N}{K}\rightarrow\infty}1-\left(1-\frac{Ky^{\ell-1}}{N}\right)^{\frac{\ell N}{\eta K}}\\
  &=1-e^{\frac{-\ell y^{\ell-1}}{\eta}}
\end{align*}
in the range $y\in [0,1]$.
\end{block}
\vspace{3ex}
\onslide<2->
\begin{block}{Threshold behavior}
For $M_1>\eta^{\text{Th}}K$ bin nodes, the peeling decoder will be successful with probability $1-O\left(\frac{1}{K^{\ell-2}}\right)$
\end{block}
Note that $\eta^{\text{Th}}$ is a function of just the left degree $\ell$.
\end{frame}
%-------------------------------------------25@#$@#$^%$##$%^%$^%^_------------------------------------------------------------------

%\subsection{Bin Decoder}
\begin{frame}{Bin detection matrix}
\begin{itemize}
\item Singleton detection is the crucial part of bin decoding:
\begin{equation*}
\mbf{y}_i=x_{k}\mbf{s}_k +\mbf{w}_i
\end{equation*} 
\item Error correction coding: $\mbf{S}$ be the codebook, where each $\mbf{s}_i$ is a codeword.
\item Block length =$P$.  $\#$ codewords $\geq \frac{N\ell}{\eta K}$
\item Choose a code with rate $R(\beta)$ s.t. fractional minimum distance $\beta$ s.t. $\frac{\beta}{2} >\mbb{P}_{e}\coleq e^{-\frac{A_{\text{min}}^{2}}{2\sigma^2}}$
\item Thus $P=\frac{\lceil {\log_2(\frac{N\ell}{\eta K})}\rceil}{R(\beta)}$.
\end{itemize} 

\begin{block}{Sample Complexity}
 \begin{align*}
  M&=M_1\times P \\
   &\geq \left[\frac{\eta^{\text{Th}}}{R(2\mbb{P}_{e})}\right] K\log\left(\frac{\ell N}{\eta^{\text{Th}} K}\right)
\end{align*} 
\end{block}
\end{frame}
%-------------------------------------------25@#$@#$^%$##$%^%$^%^_----------------------------------------------------------------

\begin{frame}{Analysis of Bin Decoding}
\begin{itemize}
\item Let $\text{E}_{\text{bin}}$ be the event an error was made in overall bin decoding
\item Union bounding: $\text{E}_{\text{bin}}\leq (\eta K+\ell K)\text{Pr(E)}$
\end{itemize}
\onslide<2->
\begin{block}{Error Probability of a bin [LPR14]}
$\text{Pr(E)}\leq 3e^{-\frac{P}{4}\frac{\gamma^2}{1+4 \gamma}}+2e^{-\frac{P}{4}(\sqrt{1+2\gamma}-1)^2}+4e^{-c_6 P\left(1-\frac{2\gamma\sigma^2}{A^{2}_{\text{min}}}\right)}+2e^{-P\frac{\left(\beta-\mbb{P}_{e}\right)^2}{2\mbb{P}_{e}(1-\mbb{P}_{e})}}$
\end{block}
\vspace{2ex}
\onslide<3->
\begin{description}
    \item[\textbf{Sub-Linear sparsity}]
\end{description}
\begin{itemize}
\item Order optimal sample complexity with precise constants given
\item $\mbb{P}_{\text{F}}\rightarrow 0$ as $N (\text{and } K)\rightarrow \infty$
\item Trade-off between the constants in $M$, rate of decay of  $\mbb{P}_{\text{F}}$ and SNR
\item Optimal decoding complexity of $O\left(K\log\left(\frac{N}{K}\right)\right)$
\end{itemize}
\end{frame}
%-------------------------------------------25@#$@#$^%$##$%^%$^%^_----------------------------------------------------------------

\begin{frame}{Implications}
\begin{block}{Error Probability of a bin - Ramchandran \textit{et al}, 2014}
$\text{Pr(E)}\leq 3e^{-\frac{P}{4}\frac{\gamma^2}{1+4 \gamma}}+2e^{-\frac{P}{4}(\sqrt{1+2\gamma}-1)^2}+4e^{-c_6 P\left(1-\frac{2\gamma\sigma^2}{A^{2}_{\text{min}}}\right)}+2e^{-P\frac{\left(\beta-\mbb{P}_{e}\right)^2}{2\mbb{P}_{e}(1-\mbb{P}_{e})}}$
\end{block}

\vspace{2ex}
\begin{description}
    \item [\textbf{Linear sparsity: $K=\alpha N$}]
\end{description}
\begin{itemize}
\item Choice of $P=c_{1}\log\left(c_2 \frac{N}{K}\right)$ doesn't work
\item We choose $P=\log K$ and rate $R(\beta)$ as earlier
\item A sub-code of size $\frac{\ell}{\alpha\eta}$ of the codebook is chosen as $\mbf{S}$
\item Sample complexity of $\eta^{\text{Th}}K\log K$
\item Can we do $\Theta(K)$ with simple decoding?
\end{itemize}
\end{frame}
%-------------------------------------------25@#$@#$^%$##$%^%$^%^_----------------------------------------------------------------

%\section{Simulation Results}
\begin{frame}{Simulation Results}
\begin{itemize}
\item $K=50, N=10^5$. $\mc{X}=\{+1,-1\}$
\item $\ell=4, \eta =2 (M_1 =2K)$. Note that $\eta^{\text{Th}}(\ell=4)=1.295$
\item $r=\frac{N\ell}{\eta K}=4000$. $\log_2(r)=12$
\item $(12,24)$ Golay code and $(12,n)$ convolutional codes for $n=24,48,96$ with QAM form $\mbf{S}$.
\end{itemize}

\begin{figure}
\resizebox{0.5\textwidth}{!}{
\begin{centering}
\input{\cs_figpath/succ_prb_Slides}
\end{centering}
}
\end{figure}
\end{frame}