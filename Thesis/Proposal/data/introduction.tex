%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%  New template code for TAMU Theses and Dissertations starting Fall 2016.  
%
%
%  Author: Sean Zachary Roberson
%  Version 3.17.06
%  Last Updated: 6/15/2017
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                           SECTION I
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\pagestyle{plain} % No headers, just page numbers
\pagenumbering{arabic} % Arabic numerals
\setcounter{page}{1}

\chapter{INTRODUCTION}
\indent We are entering an era of \emph{massive} by every metric of interest whether it be the total number of internet users, the total number of networked devices or the amount of data that needs to be stored and accessed. By 2020 the total number of connected smart devices in the world (excluding smart phones, tablets and computers) that are embedded with electronics and sensors is estimated to reach 20.8 billion according to Gartner analytics or 28.1 billion according to International Data Corporation (IDC). Similarly in regards to data storage and traffic Cisco estimates that cloud traffic could rise to 14.1 zettabytes (ZB) by 2020 from 3.9 ZB in 2015. Note that $1$ ZB=$10^{21}$ bytes=1 trillion gigabytes. According to IDC the total amount of digital data created worldwide could rise to 44 ZB by 2020. The Internet of Things (IoT) and the associated big data are a big part of this growth. By any standard this is a massive number of devices and an enormous amount of data and this provides for exciting opportunities in various engineering and scientific domains like advancing health care, resource utilization patterns, understanding the demands of certain demographics etc., via data-mining. However there are two significant challenges that need to be addressed before any such advances are feasible:
\begin{itemize}
\item design of efficient communication protocols for a large number of smart devices
\item data mining the available massive data sets in an algorithmically efficient manner.
\end{itemize}
In this thesis we attempt to tackle some problems that fall under these two issues and provide practical, low-complexity solutions for such problems. In the following section we summarize the contributions of this dissertation.

\section{Organization}
\subsection*{Background}
The overarching theme of this dissertation is leveraging the sparse bipartite Tanner graph structure and the associated low-complexity iterative peeling decoding algorithms to construct design schemes for the multiple access communication and big data problems outlined above. %In Chapter~\ref{chap:background} we review these tools in detail. In particular we describe bipartite Tanner graphs in the context of LDPC codes \cite{richardson2008modern} and iterative peeling decoding algorithm for binary erasure channels. We then outline the important results regarding the performance of peeling decoder specifically the threshold characteristic. Most of the results in this chapter can be found in \cite{richardson2008modern}.

%However due to the sporadic nature of the transmission requests at any given time the number of active devices is fairly small. The transmission scheme  

\subsection*{Massive Multiple Access}
In Chapters \ref{chap:MAC} and \ref{chap:uncoord_mac}, we consider the massive multiple access problem.  The imminent advent of IoT gives rise to a framework consisting of a large number of sensor devices that have brief but sporadic messages to communicate. This poses a vastly different set of challenges for radio resource management in wireless infrastructures. Currently deployed scheduling policies and wireless protocols which are suitable for fairly small number of sustained connections are ill-equipped to deal with such IoT traffic since they rely on gathering information about channel quality and queue length for every active user and hence pose a significant overhead to the system. This paradigm is unsustainable in environments with myriad devices, each sending a brief message.  This points to an urgent need for design of practical uncoordinated schemes for the massive multiple access setup. 

In the uncoordinated multiple access setup, each device in the system wants to transmit a message of certain length to the access point in an uncoordinated fashion. The total available time for communication is divided into slots of constant length where the users are assumed to know the structure of time slots. The access point is interested in recovering the messages transmitted by each user. In 1970 Abramson in his pioneering work \cite{abramson1970aloha} proposed a random access scheme, known as ALOHA, that achieves a throughput of $1/e\approx 0.37$. In the slotted version of the ALOHA scheme each user repeats the intended message in a certain number of slots, slots being chosen randomly and independently of other users in an uncoordinated fashion. All the slots in which there is no collision i.e., there is only one user transmitting, the message can be decoded successfully and the slots with collisions are simply discarded. This remained the state-of-the-art until a decade ago. In 2007 \cite{casini2007contention} Cassini \textit{et al} showed that higher throughput can be achieved by not discarding the slots where the transmissions of distinct users \textit{collide} but by using these slots to decode the colliding users via iterative successive interference cancellation (SIC) process. 

In 2011, Liva demonstrated a close connection between the analysis of such random access schemes under the SIC decoding process and the design of low density generator matrix codes \cite{liva2011graph}. Strengthening this connection, in Chapter~\ref{chap:uncoord_mac} we introduce an analytical framework for analyzing the evolution of the iterative SIC process as a function of the random access strategy employed by each user in the system. In 2012, Narayanan and Pfister showed that by choosing the repetition parameter randomly according to a Soliton distribution and using SIC decoder the optimal throughput of one can be achieved asymptotically\cite{narayanan2012iterative}. However, this paradigm of choosing according to Soliton distribution is known to perform poorly when the number of active devices is not very large. We took the first step in addressing this issue. In Chapter~\ref{chap:uncoord_mac}, given a probability distribution with finite maximum degree (not necessarily Soliton),  we provide analytic expressions to compute the probability of error for the SIC decoder in the random uncoordinated access problem. The analytic evaluation of the error performance offers a possible solution path to designing optimal random access strategies for this practical setup.
%expression aimed at computing the error performance of uncoordinated random access scheme as 

The unsourced formulation of the massive multiple access corresponds to the scenario where an access point only wishes to recover the collection of sent messages, and not the identity of the respective sources.  Although the sourced formulation of the uncoordinated multiple access described earlier has been around for nearly four decades, Polyanskiy in 2017 for the first time considered the unsourced formulation of the multiple access \cite{polyanskiy2017perspective}. Polyanskiy and Ordentlich \cite{ordentlich2017low} proposed a coding scheme for the unsourced multiple access based on concatenated codes. The authors show that this scheme outperforms all the other existing schemes available for the uncoordinated multiple access. In Chapter~\ref{chap:MAC} we propose a coding scheme for the unsourced MAC in which the transmitted codeword is purely a function of the message being transmitted thus exploiting  the unsourced nature of the problem. The main differentiating ingredient in our scheme when compared to \cite{ordentlich2017low} is that we use successive interference cancellation decoding process. We show that our proposed scheme not only improves substantially on the performance in \cite{ordentlich2017low} but is also only $\approx 6$dB away from the achievable limit based on random Gaussian coding and joint typical decoder which has exponential complexity \cite{polyanskiy2017perspective}. 

%Due to the sporadic nature of the transmission of these devices in spite of the total number of devices in the system is high the total number of \textit{active} devices at any particular time is fairly small. This presents a new opportunity for the application of techniques from compressive sensing. Yet naive sparsity-exploiting solutions for the overall problem are impractical as the decoding complexity of such schemes increases exponentially with the length of the messages. This work explores this challenge


\iflonger
In the unsourced MAC setup, there is one central receiver and a large total number of devices in the system where each device want to communicate a message to the central receiver infrequently. The salient features of the problem setup are 

\begin{itemize}
\item The total available time is divided into frames of equal duration whereby each frame is further divided into sub-blocks. Each devices intended message needs to be transmitted in the next available frame and this transmission schematic is known to all the devices in the system. This ensures a slotted framework.
\item Owing to the massive number of devices residing in the system, the control overhead in trying to transmit the identity of the transmitting device is huge to render any such scheme impractical. The receiver is only interested in the set of messages transmitted by the devices in a particular frame disregarding the source of the respective messages, hence the term \textit{unsourced}
\item Even though the total number of users in the system is very large, the number of users transmitting simultaneously during any particular time frame is small
\end{itemize} 
The main features of the coding scheme we propose in this thesis for the unsourced MAC are: (i) The transmitted message consists of two parts. The first part chooses an interleaver for a low density parity check type (LDPC) code. For encoding the first part we choose a code that is designed to be decoded efficiently by a compressed sensing type decoder (ii) The second part of the message is encoded by the LDPC code for which the interleaver is chosen by the first part of the message. A joint message passing type decoder designed for the T-user binary input real adder channel is used to decode the interfering LDPC codewords at each slot. (iii) Each user repeats the transmitted  codewords in multiple slots where the repetition pattern is determined solely by the message being encoded. Thus we can employ a successive interference cancellation decoder where a codeword decoded at any one slot can be removed  (\textit{peeling-off}) from all other slots it was transmitted in. This coding scheme improves upon the practical coding scheme introduced by Ordentlich and Polyanskiy in \cite{ordentlichlow} which is the best when compared to the other multiple access solutions such as treat interference as noise(TIN) and slotted-ALOHA. The proposed coding scheme is only $\approx 6$dB away from the achievable limit based on random Gaussian coding and the ideal joint typical decoder which has exponential complexity and is impractical.
\fi


\subsection*{Interference Channel}
While the massive multiple access framework is important in the context of IoT, many-to-many communication setups with a small number of users such as Gaussian interference channel are still relevant. Finding the capacity of the Gaussian interference channel has been a long standing open problem in information theory\cite{cover2012elements}. The capacity is derived under certain conditions such as (i) two-user interference channel with \textit{very strong} interference \cite{carleial1978interference,sato1981capacity}, (ii) characterization of capacity region to within one bit per channel use \cite{etkin2008gaussian} (iii) \textit{approximate} characterization of many-to-one and one-to-many interference channels etc. Since we do not yet know the characterization of the full capacity region few attempts were made at designing practical coding schemes for the Gaussian interference channel. In \cite{sridharan2008capacity} it was shown that lattice coding achieves the capacity of the two-user symmetric interference channel under very strong interference. However no practical lattice coding schemes were provided. In Chapter~\ref{chap:SCLDPClattices} we attempt to bridge this gap by constructing a new class of lattices using construction-D where the underlying linear codes are nested binary spatially-coupled low-density parity-check codes (SC-LDPC) codes from the uniform left and right degree ensembles. By leveraging results on the optimality of spatially-coupled codes for binary input memoryless symmetric channels and Forney {\em et al.}'s earlier results on the optimality of construction-D, we show that the proposed lattices achieve the Poltyrev limit under low-complexity iterative multistage belief propagation decoding. We then show that the lattice codes derived from the proposed lattices via hyper cube shaping perform upto a shaping loss of 1.53dB for the three user symmetric interference channel. 


In Chapters \ref{chap:cs} $\&$ \ref{chap:gt} we focus on the sparse signal estimation problems.
\vspace{-2pt}
\subsection*{Compressed Sensing}
Compressed sensing is a signal processing technique for efficiently acquiring linear measurements, traditionally referred to as \textit{sensing}, of a sparse signal and reconstructing the signal from the acquired measurements. In Chapter \ref{chap:cs}, we focus on the support recovery problem in compressed sensing wherein the objective is to recover the set of signal dimensions with non-zero power and not necessarily the whole signal. In 2015 Li, Pawar and Ramchandran proposed two schemes to recover the support of a $K$-sparse $N$-dimensional signal from noisy linear measurements \cite{li2015subdraft,li2015subisit}. Both the schemes employ left-regular sparse bipartite graph code based matrices for sensing the signal and a peeling based reconstruction algorithm. Both the schemes require $O(K \log N)$ measurements and the first scheme requires $O(N \log N)$ total computations whereas the second scheme requires $O(K \log N)$ total computations (sub-linear computational complexity when $K$ is sub-linear in $N)$. We show that by replacing the left-regular ensemble with left-and-right regular ensemble, we can reduce the number of measurements required of these schemes to the optimal order of $\Theta\left(K \log \frac{N}{K} \right)$ with optimal decoding complexities of $O(K \log \frac{N}{K})$ and $O(N \log \frac{N}{K})$ respectively.

\subsection*{Group Testing}
The group testing problem was first introduced to the fields of applied mathematics and statistics by Dorfman \cite{dorfman1943detection} during World War II for testing the soldiers for syphilis without having to test each soldier individually. The aim of the problem is to detect $K$ defective items out of a large population of $N$ total items where grouping multiple items together for a single test is possible. The output of the test is \textit{negative} if all the grouped items are non-defective or else the output is \textit{positive}. In Chapter \ref{chap:gt}, we focus on the non-adaptive version of group testing where the testing scheme is pre-determined and is independent of the test results. We propose a testing scheme based on left-and-right regular sparse bipartite graphs that admit a simple iterative recovery scheme and show that for any arbitrarily small $\epsilon>0$ our scheme requires only $m=\ceps K\log \frac{c_1N}{K}$ tests to recover $(1-\epsilon)$ fraction of the defective items with high probability (w.h.p) i.e., with probability approaching $1$ asymptotically in $N$ and $K$, where the value of constants $\ceps$ and $\ell$ are a function of the desired error floor $\epsilon$ and constant $c_1=\frac{\ell}{\ceps}$ (observed to be approximately equal to 1 for various values of $\epsilon$). More importantly the iterative decoding algorithm has a sub-linear computational complexity of $O(K\log \frac{N}{K})$ which is known to be optimal. Also for $m=c_2 K\log K\log \frac{N}{K}$ tests our scheme recovers the \textit{whole} set of defective items w.h.p. These results are valid for both noiseless and noisy versions of the problem as long as the number of defective items scale sub-linearly with the total number of items, i.e., $K=o(N)$. The simulation results validate the theoretical results by showing a substantial improvement in the number of tests required when compared to the testing scheme based on the left regular sparse graphs.


\iflonger
The contributions of this thesis can be categorized into four parts:
\begin{enumerate}
\item The first part considers the unsourced version of the massive multiple access problem, which will be referred to as unsourced MAC. In the unsourced MAC problem setup, a large number of devices are present in the system where each device has a brief message to communicate to a central receiver sporadically. Distinguishing from the traditional MAC is the fact that the receiver is interested only in the set of messages transmitted and disinterested in the source of the respective messages thus the term \textit{unsourced}. Due to the sporadic nature of the data to be transmitted, the number of active devices at any particular time is fairly small. The main components of the proposed coding scheme for the unsourced MAC are: (i) The transmitted message consists of two parts. The first part chooses an interleaver for a low density parity check (LDPC) type code. For encoding the first part, a code that is designed to be decoded efficiently by a compressed sensing type decoder is chosen (ii) The second part of the message is encoded by the LDPC code for which the interleaver is chosen by the first part of the message. A joint message passing type decoder designed for the T-user binary input real adder channel is used to decode the interfering LDPC codewords at each slot. (iii) Each user repeats the transmitted codeword in multiple time slots where the repetition pattern is determined solely by the message being encoded. Thus this scheme is amenable to a successive interference cancellation decoder where a codeword decoded at any one slot can be removed  (\textit{peeling-off}) from all other slots it was transmitted in. This coding scheme improves upon the practical coding scheme introduced by Ordentlich and Polyanskiy in \cite{ordentlich2017low} which is the best when compared to the other multiple access solutions available in literature such as treat interference as noise (TIN) and slotted-ALOHA. The proposed coding scheme is only $\approx 6$dB away from the achievable limit based on random Gaussian coding and the ideal joint typical decoder which has exponential complexity and is impractical.
The unsourced MAC is followed by the consideration of uncoordinated MAC problem wherein no power constraints are imposed on the individual users. In the non-asymptotic case i.e. when the number of users is fixed and finite, an analytic expression is found to compute the error performance of the successive interference cancellation scheme as a function of the random access strategy employed by each user. The analytic expression proposed is verified to be accurate through simulations. This provides a possible solution path, using gradient ascent approach, to derive a random access strategy that is optimal in terms of the total throughput achievable.

\item In the second part, the design of construction-D lattices for the interference channel is considered. The proposed construction scheme uses spatially-coupled low-density parity check (LDPC) codes \cite{felstrom1999time,kudekar2011threshold}. The fact that these codes achieve the channel capacity on the class of binary memory-less symmetric channels is leveraged to achieve the Poltyrev limit for lattices. The lattice codes derived from this class of lattices are shown to provide excellent performance for the symmetric Gaussian interference channel. The simulation results demonstrate that the proposed lattice codes under the low-complexity multistage belief propagation decoder can perform within 1.02dB (discounting the 1.53dB shaping loss) of the Shannon limit for the symmetric interference channel 

\item The third part considers the design of a family of sensing matrices for the problem of support recovery in compressed sensing. In the proposed design scheme the sensing matrix when represented using graph structure has a close resemblance to the left and right regular LDPC codes. An iterative algorithm similar to the peeling decoder is employed for the recovery of the sparse signal. The proposed design scheme enables to achieve the number of measurements and decoding complexity that match the best possible lower bounds upto a constant.% that are order optimal in the number of measurements and decoding complexity.
  

\item In the fourth part testing schemes for the non-adaptive group testing problem are considered. Similar to the compressed sensing, the testing scheme resembles left and right regular bipartite graph in structure and the algorithm for recovering the defective items is similar to a peeling decoder. With the proposed testing scheme the number of tests required is order optimal and more importantly the recovery algorithm has only a sub-linear time complexity.
\end{enumerate} 
\fi