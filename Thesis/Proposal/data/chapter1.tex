%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%  New template code for TAMU Theses and Dissertations starting Fall 2016.  
%
%
%  Author: Sean Zachary Roberson
%  Version 3.17.06
%  Last Updated: 6/15/2017
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                           SECTION I
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\pagestyle{plain} % No headers, just page numbers
\pagenumbering{arabic} % Arabic numerals
\setcounter{page}{1}

\chapter{\uppercase {Introduction}}

\indent In the digital world we are entering an era of \textit{massive} by any metric of interest whether it be the total number of internet users, the total number of networked devices or the amount of data that needs to be stored and accessed. By 2020 the total number of connected smart devices in the world (excluding smartphones, tablets and computers) that are embedded with electronics and sensors is \textit{estimated} to reach 20.8 billion according to Gartner analytics or 28.1 billion according to IDC (International Data Corporation). Similarly in regards to data storage and traffic, Cisco estimates that cloud traffic could rise to 14.1 zettabytes (ZB) by 2020 where 1 zettabyte=$10^{21}$ bytes=1 trillion gigabytes, from 3.9 ZB in 2015. The total amount of digital data created worldwide, according to IDC, could rise to 44 ZB by 2020. The Internet of Things (IoT) and the associated big data are a big part of this growth. By any standard this is a massive number of devices and an enormous amount of data and this provides for exciting opportunities in various areas like advancing health care, resource utilization patterns, understanding the demands of certain demographics etc.,  via data-mining. There are two significant challenges to achieving this:
\begin{itemize}
\item the communication of data from a large number of smart devices in a radio resource efficient manner and 
\item data mining these massive datasets in an algorithmically efficient manner.
\end{itemize}
In this thesis we attempt to provide solutions for these two issues.
\section{Organization}
\subsection*{Massive Multiple Access}
In chapter 2, we consider the massive multiple access problem.  The imminent advent of IoT gives rise to a framework consisting of a large number of sensor devices that have brief but sporadic messages to communicate. This poses a vastly different set of challenges for radio resource management in wireless infrastructures. Currently deployed scheduling policies and wireless protocols which are suitable for fairly small number of sustained connections are ill-equipped to deal with such IoT traffic since they rely on gathering information about channel quality and queue length for every active user and hence pose a significant overhead to the system. This paradigm is unsustainable in environments with myriad devices, each sending a brief message.  As such, there has been recent interest in the design of massive uncoordinated and unsourced multiple access schemes. The unsourced formulation of the multiple access corresponds to the scenario where an access point only wishes to recover the collection of sent messages, and not the identity of the respective sources whereas in the sourced formulation of the uncoordinated multiple-access, which we will refer to as uncoordinated MAC for simplicity, the receiver is interested in the message along with the identity of the transmitter. In 1970, Abramson in his pioneering work \cite{abramson1970aloha} proposed a multiple-access scheme for the uncoordinated MAC that achieves a throughput of $1/e\approx 0.37$. This remained the state-of-the-art until a decade ago. In 2007 \cite{casini2007contention} Cassini \textit{et al} showed that higher throughput can be achieved by not discarding the slots where the transmissions of distinct users \textit{collide} but by using these slots to decode the colliding users via successive interference cancellation (SIC) process. Although the sourced formulation of the uncoordinated MAC has been around nearly four decades, Polyanskiy in 2017 first considered the unsourced formulation of the MAC \cite{polyanskiy2017perspective}. We propose a coding scheme for the unsourced MAC in which the transmitted codeword is purely a function of the message being transmitted thus exploiting  the unsourced nature of the problem. The scheme uses successive interference cancellation decoding process. We show that the proposed scheme is only $\approx 6$dB away from the achievable limit based on random Gaussian coding and the ideal joint typical decoder which has exponential complexity. 

%Due to the sporadic nature of the transmission of these devices in spite of the total number of devices in the system is high the total number of \textit{active} devices at any particular time is fairly small. This presents a new opportunity for the application of techniques from compressive sensing. Yet naive sparsity-exploiting solutions for the overall problem are impractical as the decoding complexity of such schemes increases exponentially with the length of the messages. This work explores this challenge


\iflonger
In the unsourced MAC setup, there is one central receiver and a large total number of devices in the system where each device want to communicate a message to the central receiver infrequently. The salient features of the problem setup are 

\begin{itemize}
\item The total available time is divided into frames of equal duration whereby each frame is further divided into sub-blocks. Each devices intended message needs to be transmitted in the next available frame and this transmission schematic is known to all the devices in the system. This ensures a slotted framework.
\item Owing to the massive number of devices residing in the system, the control overhead in trying to transmit the identity of the transmitting device is huge to render any such scheme impractical. The receiver is only interested in the set of messages transmitted by the devices in a particular frame disregarding the source of the respective messages, hence the term \textit{unsourced}
\item Even though the total number of users in the system is very large, the number of users transmitting simultaneously during any particular time frame is small
\end{itemize} 
The main features of the coding scheme we propose in this thesis for the unsourced MAC are: (i) The transmitted message consists of two parts. The first part chooses an interleaver for a low density parity check type (LDPC) code. For encoding the first part we choose a code that is designed to be decoded efficiently by a compressed sensing type decoder (ii) The second part of the message is encoded by the LDPC code for which the interleaver is chosen by the first part of the message. A joint message passing type decoder designed for the T-user binary input real adder channel is used to decode the interfering LDPC codewords at each slot. (iii) Each user repeats the transmitted  codewords in multiple slots where the repetition pattern is determined solely by the message being encoded. Thus we can employ a successive interference cancellation decoder where a codeword decoded at any one slot can be removed  (\textit{peeling-off}) from all other slots it was transmitted in. This coding scheme improves upon the practical coding scheme introduced by Ordentlich and Polyanskiy in \cite{ordentlichlow} which is the best when compared to the other multiple access solutions such as treat interference as noise(TIN) and slotted-ALOHA. The proposed coding scheme is only $\approx 6$dB away from the achievable limit based on random Gaussian coding and the ideal joint typical decoder which has exponential complexity and is impractical.
\fi

A close connection has been established between the analysis of multiple access schemes and the design of low density generator matrix codes. Strengthening this connection, we introduce an analytical expression aimed at computing the error performance of uncoordinated random access scheme as a function of the random access strategy employed by each user in the system. In 2012 Narayanan and Pfister showed that by choosing the repetition parameter randomly according to a Soliton distribution and using SIC decoder the optimal throughput of $1$ can be achieved asymptotically\cite{narayanan2012iterative}. However, this paradigm is known to perform poorly when the number of active devices is not large. The analytic evaluation of the error performance offers a possible solution path to designing optimal strategies for this practical setting.

\subsection*{Interference Channel}
In Chapter 3, we propose a new class of lattices constructed using construction-D where the underlying linear codes are nested binary spatially-coupled low-density parity-check codes (SC-LDPC) codes with uniform left and right degrees. By leveraging results on the optimality of spatially-coupled codes for binary input memoryless channels and Forney {\em et al.}'s earlier results on the optimality of construction-D, we show that the proposed lattices achieve the Poltyrev limit under multistage belief propagation decoding. We also show that the lattice codes constructed from these lattices provide excellent performance for the three user symmetric interference channel and bi-directional relay channel. 

\subsection*{Compressive Sensing}
In Chapter 4, we consider the support recovery problem for compressive sensing. In 2015 Li, Pawar and Ramchandran proposed two schemes to recover the support of a $K$-sparse $N$-dimensional signal from noisy linear measurements \cite{li2015subdraft,li2015subisit}. Both schemes use left-regular sparse-graph code based sensing matrices and a simple peeling-based decoding algorithm. Both the schemes require $O(K \log N)$ measurements and the first scheme requires $O(N \log N)$ total computations whereas the second scheme requires $O(K \log N)$ total computations (sub-linear computational complexity when $K$ is sub-linear in $N)$. We show that by replacing the left-regular ensemble with left-and-right regular ensemble, we can reduce the number of measurements required of these schemes to the optimal order of $\Theta\left(K \log \frac{N}{K} \right)$ with decoding complexities of $O(K \log \frac{N}{K})$ and $O(N \log \frac{N}{K})$, respectively.

\subsection*{Group Testing}
In Chapter 5, we consider the problem of non-adaptive group testing of $N$ items out of which $K$ or less items are known to be defective. We propose a testing scheme based on left-{\em and}-right-regular sparse-graph codes and a simple iterative decoder and show that for any arbitrarily small $\epsilon>0$ our scheme requires only $m=\ceps K\log \frac{c_1N}{K}$ tests to recover $(1-\epsilon)$ fraction of the defective items with high probability (w.h.p) i.e., with probability approaching $1$ asymptotically in $N$ and $K$, where the value of constants $\ceps$ and $\ell$ are a function of the desired error floor $\epsilon$ and constant $c_1=\frac{\ell}{\ceps}$ (observed to be approximately equal to 1 for various values of $\epsilon$). More importantly the iterative decoding algorithm has a sub-linear computational complexity of $O(K\log \frac{N}{K})$ which is known to be optimal. Also for $m=c_2 K\log K\log \frac{N}{K}$ tests our scheme recovers the \textit{whole} set of defective items w.h.p. These results are valid for both noiseless and noisy versions of the problem as long as the number of defective items scale sub-linearly with the total number of items, i.e., $K=o(N)$. The simulation results validate the theoretical results by showing a substantial improvement in the number of tests required when compared to the testing scheme based on left-regular sparse-graphs.

