\documentclass[10pt,xcolor=table]{beamer}
%\usetheme{warsaw}
\usetheme{CambridgeUS}
\usepackage{etex}
\usepackage[english]{babel}
\usepackage{amstext,amsthm,amsmath,amsfonts,latexsym,graphicx,amssymb,epsfig,epsf,psfrag,mathtools}
\usepackage{pgfpages,xcolor,subfigure,pstricks}
\usepackage{comment}
\usepackage{pgfplots}
\usepackage{url}
\usepackage{tikz,tikz-cd}
\pgfplotsset{compat=newest}
%\def\ProvidesPackageRCS#1{}
%% the following commands are sometimes needed
%\usetikzlibrary{plotmarks}
\usetikzlibrary{arrows,backgrounds,plotmarks,decorations.pathmorphing,decorations.footprints,fadings,calc,trees,mindmap,shadows,decorations.text,patterns,positioning,shapes,matrix,fit}
\input{graphical_settings}		
\usepackage{grffile}
\usepackage{fancyhdr}
\usepackage{pst-all}

\graphicspath{{./../figures/}}
\makeatletter
\def\input@path{{./../figures/}}
\makeatother

\usepackage{tikz,pgfplots}
\pgfplotsset{compat=newest}
\pgfplotsset{plot coordinates/math parser=false}
\usetikzlibrary{shapes.geometric, arrows}
\usetikzlibrary{decorations.markings}
\tikzstyle{startstop} = [rectangle, rounded corners, minimum width=1.5cm, minimum height=0.5cm,text centered, draw=black, fill=red!30]
\tikzstyle{io} = [trapezium, trapezium left angle=70, trapezium right angle=110, minimum width=1.5cm, minimum height=0.5cm, text centered, draw=black, fill=blue!30]
\tikzstyle{process} = [rectangle, minimum width=1.5cm, minimum height=0.5cm, text centered, draw=black, fill=orange!30]
\tikzstyle{decision} = [diamond, minimum width=1.5cm, minimum height=0.5cm, text centered, draw=black, fill=green!30]
\tikzstyle{arrow} = [thick,->,>=stealth]


%\hypersetup{pdfpagemode=FullScreen}
\usefonttheme{professionalfonts}
\setbeamertemplate{footline}[frame number]
\setbeamertemplate{theorem}[ams style]
\setbeamertemplate{theorems}[numbered]
\definecolor{purple}{RGB}{255,0,204}

\newcommand{\defeq}{\triangleq}
\newcommand{\Pp}{\mathbb{P}}
\newcommand{\Gm}{\mathbf{G}}
\newcommand{\Hm}{\mathbf{H}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\Zp}{\mathbb{Z}_{+}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\Rp}{\R_{+}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\F}{\mathbb{F}}
\newcommand{\Zw}{\mathbb{Z}[\omega]}
\newcommand{\Zi}{\mathbb{Z}[i]}
\newcommand{\mc}{\mathcal}
\newcommand{\mbb}{\mathbb}

\newcommand{\wv}{\underline{w}}
\newcommand{\cv}{\underline{c}}
\newcommand{\ev}{\underline{e}}
\newcommand{\sv}{\underline{s}}
\newcommand{\xv}{\underline{x}}
\newcommand{\bv}{\underline{b}}
\newcommand{\kv}{\underline{k}}
\newcommand{\zv}{\underline{z}}
\newcommand{\yv}{\underline{y}}
\newcommand{\hv}{\underline{h}}
\newcommand{\rv}{\underline{r}}
\newcommand{\mv}{\underline{m}}

\newcommand{\Yv}{\underline{Y}}
\newcommand{\Xv}{\underline{X}}
\newcommand{\Rv}{\underline{R}}
\newcommand{\mfk}[1]{\mathfrak{#1}}


\newcommand{\RNum}[1]{\uppercase\expandafter{\romannumeral #1\relax}}

\usecolortheme[RGB={0,100,0}]{structure}
\setbeamertemplate{itemize item}[circle]
\setbeamertemplate{itemize subitem}[rectangle]
\setbeamertemplate{itemize subsubitem}{$-$}
\setbeamertemplate{blocks}[rounded][shadow=true]
\setbeamertemplate{navigation symbols}{} % get rid of navigation symbols
%\setbeamertemplate{footline}[page number]
%\logo{\includegraphics[height=1cm,keepaspectratio]{greentouch.eps}}

\definecolor{DarkFern}{HTML}{407428}
\definecolor{DarkCharcoal}{HTML}{4D4944}
\colorlet{Fern}{DarkFern!85!white}
\colorlet{Charcoal}{DarkCharcoal!85!white}
\colorlet{LightCharcoal}{Charcoal!50!white}
\colorlet{DarkRed}{red!70!black}
\colorlet{AlertColor}{DarkRed!80!black}
\colorlet{DarkBlue}{blue!70!black}
\colorlet{DarkGreen}{green!70!black}
% Use the colors:
\setbeamercolor{title}{fg=DarkRed}
\setbeamercolor{frametitle}{fg=DarkRed}
\setbeamercolor{normal text}{fg=black}
\setbeamercolor{block title}{fg=black,bg=DarkBlue!35!white}
\setbeamercolor{block body}{fg=black,bg=DarkBlue!15!white}
%\setbeamercolor{block title}{fg=black,bg=Fern!25!white}
%\setbeamercolor{block body}{fg=black,bg=Fern!25!white}
\setbeamercolor{alerted text}{fg=AlertColor}
\setbeamercolor{itemize item}{fg=Charcoal}

\usepackage[backend=bibtex,style=authoryear]{biblatex}

\usepackage{chemarrow}	

\addbibresource{IEEEabrv.bib}
\addbibresource{sparseestimation.bib}

\AtBeginSection[]{
	\begin{frame}
	\vfill
	\centering
	\begin{beamercolorbox}[sep=8pt,center,shadow=true,rounded=true]{title}
		\usebeamerfont{title}\insertsectionhead\par%
	\end{beamercolorbox}
	\vfill
\end{frame}
}


\def\fig_path{../Figures}
\def\peeling_figpath{../Figures/peeling}
\begin{document}
\title{Sub-string/Pattern Matching in Sub-linear Time Using a Sparse Fourier Transform Approach}
\author{ Nagaraj Thenkarai Janakiraman \\
{\bf Advisor:} Dr. Krishna R. Narayanan \\
}
\titlegraphic{
\includegraphics[width=0.75in]{TAMULogoBox}
}
\institute{Department of Electrical and Computer Engineering \\ Texas A\&M University}
\date{}
\frame{\titlepage}

%------------------------------

\section*{Overview}

\begin{frame}\frametitle{Research Background}
\begin{itemize}\itemsep15pt
	\item Joined A \& M in 2013
	\item Applications of Coding Theory in Sparse Estimation Problems
		\begin{itemize}\itemsep10pt
			\item Sub-linear time Sparse Fourier Transform
			\item Sub-linear time Sparse Hadamard Transform
			\item Sparse Estimation for Signals on Graphs
		\end{itemize}
	\item Sub-linear time Sub-string/Pattern Matching - Focus of this talk
	
\end{itemize}
\end{frame}


\section{Sub-linear Time Sub-string/Pattern Matching}

\begin{frame}\frametitle{Motivation: Data vs Computing - Who's winning?}
\input{datacomputingcomparison}
\begin{itemize}
	\item Linear complexity is too expensive, sub-linear complexity is required
\end{itemize}

\end{frame}


%-----------------------------------------------------------------------------------------------
\begin{frame}\frametitle{Pattern Matching - Problem Statement}
 	\vspace{-0.4cm}
	\begin{figure}[t]
		\centering
		\includegraphics[width=2.8in]{Pattern_matching_ex.pdf}
	\end{figure}
	\vspace{-10pt}
	\begin{block}{}
\begin{itemize}\itemsep5pt
	\item {\color{blue} Database/String}: $\xv = [x[0], x[1], \cdots, x[N-1]]$ \ (length $N$)
	\item { \color{blue} Query/Substring}: $\yv = [y[0], y[1], \cdots, y[M-1]]$ \ (length $M = N^\mu$)
	\item {\color{blue} Signal Model:} $x[i]$'s are i.i.d ~ r.v. from $\mathcal{A} = \{+1,-1\}$ (extensions possible)
\end{itemize}
\end{block}

\vspace{-3pt}
\pause
\begin{block}{}
Determine all the {\color{blue} $L$ locations} $\underline{\tau} = [\tau_1, \tau_2, \cdots \tau_L]$ with  {\color{blue}high probability}  where
	\begin{enumerate}
		\item \alert{Exact Matching}:~  $\yv$ appears {\color{blue}exactly} in $\xv$
		\begin{itemize}
				\item [-]  $\yv := \xv[\tau:\tau+M-1]$
		\end{itemize}
        \pause
		\item \alert{Approximate Matching:} ~ $\yv$ is a {\color{blue}noisy substring} of $\xv$
		\begin{itemize}\itemsep3pt
				\item [-] $\yv := \xv[\tau:\tau+M-1] \odot \bv$
				\item [-] $\bv$ is a noise sequence with $d_H(\yv,\xv[\tau:\tau+M-1]) \leq K$
		\end{itemize}
	\end{enumerate}

	\end{block}
	 \end{frame}
%%-------------------------------------------------------------------------------------------------------------

\begin{frame} \frametitle{Sketching Model}
		\vspace{-10pt}
	\begin{figure}
		\includegraphics[width=2.25in]{SketchingModel.pdf}
	\end{figure}
	\vspace{-8pt}
	\begin{block}{Sketch}
		\begin{itemize}
			\item Succinct fingerprint (compressed form) of the data - min communication cost
			\item Allows Pattern Matching without decompression
			\item Could be both lossless or lossy (probabilistic recovery)
			\item \alert{Sketching Complexity} - space in bits to store the sketch of database
            \item \alert{Computational Complexity} - operations to find the matching locations
		\end{itemize}
	\end{block}
\end{frame}
% ----------------------------------------Notations---------------------------------------

\begin{frame}\frametitle{Notations - scaling}

	\begin{figure}[t]
		\centering
		\includegraphics[width=2.5in]{Pattern_matching_ex.pdf}
	\end{figure}
	\vspace{-8pt}
	{\small
	\begin{table}[h!]
		\label{Table:Notations3}
		\begin{center}
			\begin{tabular}{|c|c|} 	
				\hline		
				\textit{Symbol}		&  \textit{Meaning} \\		
				\hline
				$N$           		& Size of the string or database in symbols \\
				\hline
				$M = N^{\mu}$       & Length of the query in symbols \\
				\hline
				$L = N^\lambda$    &   Number of matches \\
				\hline
				$K$             &$\max_{\tau}d_{H}(\xv[\tau:\tau+M-1],\yv)$\\
				\hline
				$\eta$             &$\frac{K}{M}$\\
				\hline
				%$G = N^\gamma$    & Number of blocks \\
				%\hline
				%$\tilde{N} = N^{1-\gamma}$   & Length of one block \\
				%\hline
				%$f_i = N^\alpha$     & Length of smaller point IDFT at each branch\\
				%\hline
				%$g_i = N/f_i$     	   &  Sub-sampling parameter \\
				%\hline
				%$B$   					    & Number of shifts also referred to as branches  \\
				%\hline
				%$d$           				& Number of stages in the FFAST algorithm \\
				%\hline
			\end{tabular}
		\end{center}
	\end{table}
    }
    \begin{block}{Probabilistic recovery}
    $\mbb{P}(\hat{\underline{\tau}} \neq \underline{\tau}) \rightarrow 0$ as $N \rightarrow \infty$
    \end{block}
	\end{frame}	

%------------------------------------------------------------------------------------------------------------
%\begin{frame}\frametitle{Big data applications}
%\begin{block}{Metabolomics}
%\begin{itemize}
%  \item Database of genomes contains a sequence $\vec{x}$ of length $N=10^{6}-10^{12}$
%\end{itemize}
%
%\end{block}
%  \begin{figure}[h]
%  \includegraphics[width=4.0in]{genomesizes.pdf}
%  \end{figure}
%
%\url{http://www.biology-pages.info/G/GenomeSizes.html}
%\end{frame}
%------------------------------------------------------------------------------------------------------------
\begin{frame}\frametitle{Big data applications}
\begin{block}{Metabolomics}
\begin{itemize}
  \item Database of genomes contains a sequence $\vec{x}$ of length $N=10^{6}-10^{12}$
  \item $M=1000$ to $M = 100000$ gives $\mu = 0.25$ to $\mu = 0.42$
\end{itemize}

  \begin{figure}[h]
  \includegraphics[width=4.0in]{genomebiology.pdf}
  \end{figure}


\end{block}

\end{frame}
%------------------------------------------------------------------------------------------------------------
\begin{frame}\frametitle{Big data applications}
\begin{block}{Audio}
Let us consider the case of an audio signal which is sampled at a moderate sampling rate of $10$~KHz and let the signal $\xv$ correspond to 1 hour of audio and hence, $N = 3.6 \times 10^7$. Let the query $\yv$ be a 10 second clip which corresponds to $M = 10^4$ resulting in a $\mu = 0.529$.
\end{block}
\begin{block}{}
\begin{figure}
\includegraphics[width=3.5in]{ScaledVoice.pdf}
\end{figure}
\end{block}
\end{frame}
%%-------------------------------------------------------------------------------------------------------------
\begin{frame} \frametitle{Lower Bounds on Sketching Complexity}	
	\begin{block}{Approximate Matching:}
		For a query of length $M$ and a Hamming distance $K= \eta M$ ($0<\eta<1$), any sketching algorithm that can determine the matches with a constant probability of error requires \alert{$s = \Omega (N/M \log N)$} bits of memory.
	\end{block}
	
	\begin{block}{Note}
		 Minor variations in the model\footfullcite{bar2004sketching} assumption and definition of error probability.
	\end{block}
\end{frame}
%%-------------------------------------------------------------------------------------------------------------
\begin{frame} \frametitle{Some Prior Work}
	\vspace{-0.2cm}
	 \begin{block}{Exact Matching - Non-sketching versions}	 	
	 	\begin{itemize}
            \item  {Rabin-Karp}: Match a set of strings - $O(N)$ complexity
	 		\item  {Boyer'77}: First occurrence of the match (\alert{only $\tau_1$})	 		
	 		\begin{itemize}
	 			\item[-] Average complexity - $O(N^{1-\mu} \log N)$ (sub-linear)
	 			\item[-] Worst case complexity - $O(N \log N)$
	 		\end{itemize}	
	 	\end{itemize}
	 \end{block}
	
	 \begin{block}{Exact Matching - Sketching versions - Bio informatics} 	
	 	\begin{itemize}
	 		\item  {Goodrich'05}: BWT, suffix-arrays based indexing
	 		\begin{itemize}
	 			\item[-] Time complexity - $O(M + L)$ (sub-linear)
	 			\item[-] Storage Complexity - $O(N~H_k(X) \log^\epsilon N) + o(N)$ bits  (linear)
	 			\item[-] Read alignment in Bio-informatics community[ {\color{blue}Li'09,Li'10}]
	 		\end{itemize}	 		
	 	\end{itemize}
	 \end{block}
\end{frame}
%-------------------------------------------------------------------------------------------------------------
\begin{frame} \frametitle{Some Prior Work}
	\vspace{-0.2cm}
	\begin{block}{\alert{Approximate Matching}}
		
	 	\begin{itemize}
	 		\item {Chang'94}: Generalization of Boyer'77
	 		\begin{itemize}
	 			\item[-] Average time complexity - $O(NK/M \log N)$ (sub-linear only when $K \ll M$ )
	 		\end{itemize}
	 		
	 		\item {Zhang'03}: Approximate Matching using BWT
	 			\begin{itemize}
	 				\item[-] Worst case time complexity: $O(\min\{M(M-K){|\mc{A}|}^k\log \frac{N}{|\mc{A}|} , NM \log \frac{N}{|\mc{A}|}\})$
	 				\item[-] Complexity grows with $|\mc{A}|$ and $K$
	 			\end{itemize}
	 		\pause	
	 		\item {Andoni, Hassanieh, Indyk and Katabi'13}: Sub-linear for $K = O(M)$
	 		\begin{itemize}
                \item[-] \textcolor[rgb]{0.00,0.00,1.00}{$O\left(N^{1-0.359\mu}\right)$ (sub-linear even when $K = O(M)$)}
	 			\item[-] Combinatorial in nature
	 		\end{itemize}
	 	\end{itemize}
	 	
	 \end{block}
\end{frame}
	
%---------------------------------------------------------------------------------------------------------------------------

%--------------------------------------------- Main Result -------------------------------------------------------------------
	 \begin{frame} \frametitle{Our Main Result}

	{\small
	\begin{table}[h!]
		\begin{center}
			\begin{tabular}{|c|c|} 	
				\hline		
				\textit{Symbol}		&  \textit{Meaning} \\		
				\hline
				$N$           		& Size of the string or database in symbols \\
				\hline
				$M = N^{\mu}$       & Length of the query in symbols \\
				\hline
				$L = N^\lambda$    &   Number of matches \\
				\hline
				$K$             &$\max_{\tau}d_{H}(\xv[\tau:\tau+M-1],\yv)$\\
				\hline
				$\eta$             &$\frac{K}{M}$\\
				\hline
			\end{tabular}
		\end{center}
	\end{table}
    }	
     	
    \vspace{-2.5mm}
	 \begin{theorem}
	 	Assume that a sketch of $\xv$ can be precomputed and stored. Then for the {\it exact pattern matching} and {\it approximate pattern matching} (with $K = \eta M,~ 0 \leq \eta \leq 1/6$) problems, our algorithm has
	 	\begin{itemize}
	 		\item \alert{Sketching complexity:} {\color{blue} $O(\frac{N}{M}\log N)=O(N^{1-\mu}\log N)$} \alert{samples}
	 		\item \alert{Computational complexity:}
	 		{\color{blue}$O(\max(N^{1-\mu}\log^2 N, N^{\mu+\lambda}\log N ))$}	 		
	 		\item a decoder for which $\mbb{P}(\hat{{\mathcal{T}}} \neq \mathcal{T}) \rightarrow 0$ as $N \rightarrow \infty$
	 	\end{itemize}
	 \end{theorem}
	 \pause
	 \vspace{-0.5mm}
	 \begin{block}{\alert{Note}}
	 	When $L<\frac{N}{M}$ (i.e. $\lambda<1-\mu$) our algorithm has a {\color{blue}sub-linear time} complexity.
	 \end{block}	
\end{frame}



\begin{frame} \frametitle{Improved Result}
	
	{\small
		\begin{table}[h!]
			\begin{center}
				\begin{tabular}{|c|c|} 	
					\hline		
					\textit{Symbol}		&  \textit{Meaning} \\		
					\hline
					$N$           		& Size of the string or database in symbols \\
					\hline
					$M = N^{\mu}$       & Length of the query in symbols \\
					\hline
					$L = N^\lambda$    &   Number of matches \\
					\hline
					$K$             &$\max_{\tau}d_{H}(\xv[\tau:\tau+M-1],\yv)$\\
					\hline
					$\eta$             &$\frac{K}{M}$\\
					\hline
				\end{tabular}
			\end{center}
		\end{table}
	}	
	
	\vspace{-2.5mm}
	\begin{theorem}
		Assume that a sketch of $\xv$ can be precomputed and stored. Then for the {\it exact pattern matching} and {\it approximate pattern matching} (with $K = \eta M,~ 0 \leq \eta \leq 1/6$) problems, our algorithm has
		\begin{itemize}
		 \item	For $\mu \leq 0.5:$
		\begin{itemize}
			\item \alert{Sketching complexity:} 
			{\color{blue} $O(\frac{N}{M}\log N)=O(N^{1-\mu}\log N)$} \alert{samples}
			\item \alert{Computational complexity:}
			{\color{blue}$O(\max(N^{1-\mu}\log^2 N, N^{\mu+\lambda}\log N ))$}
		\end{itemize}	
		\item	For $\mu > 0.5$ and $\lambda<\mu-0.5$ :

		\begin{itemize}
			\item \alert{Sketching complexity:} 
			{\color{blue} $O(\sqrt{N}\log N)$} samples
			\item \alert{Computational complexity:}
			{\color{blue}$O(\sqrt{N}\log^2 N)$}
		\end{itemize}	
				 		
			\item a decoder for which $\mbb{P}(\hat{{\mathcal{T}}} \neq \mathcal{T}) \rightarrow 0$ as $N \rightarrow \infty$
		\end{itemize}
	\end{theorem}
\end{frame}

%-------------------------------------------------------------------------------------------------------------
\begin{frame}\frametitle{Signal Processing View}

	\begin{figure}[t]
		\centering
		\includegraphics[width=3.5in]{Pattern_matching_ex.pdf}
	\end{figure}

			\begin{block}{}			
				\begin{itemize}
					\item {Cross-correlation} ($\rv$): $\displaystyle{r[m]=(\xv*\yv)[m] \defeq \sum_{i=0}^{M-1} x[m+i] y[i] }$
			        \pause
					\item {Naive implementation}: $O(MN) = O(N^{1+\mu})$ ({\color{blue} super-linear} complexity)
                    \pause
					\item {Fourier Transform Approach}: 1970's - $O(N \log N)$ complexity
					\begin{equation}\nonumber
					\rv = \mathcal{F}_{N}^{-1} \{~  \mathcal{F}_{N}\{\xv\} ~ \odot ~  \mathcal{F}_{N}\{\yv'\} ~ \}, \ \ \yv' = \yv^{*}[-n]
					\end{equation}
					
				\end{itemize}
			\end{block}												
\end{frame}
%-------------------------------------------------------------------------------------------------------------
\begin{frame}\frametitle{Main Idea}
	\vspace{-0.4cm}
			\begin{block}{}
			
				\begin{itemize}
					\item {Cross-correlation} ($\rv$):
					
                    \begin{eqnarray}
                    \nonumber
					r[m] & = & (\xv*\yv)[m] \defeq \sum_{i=0}^{M-1} x[m+i] y[i], ~ ~ \ 0 \leq m \leq N-1\\
                    \nonumber
					\rv  & = & \mathcal{F}_{N}^{-1} \{~  \mathcal{F}_{N}\{\xv\} ~ \odot ~  \mathcal{F}_{N}\{\yv'\} ~ \}, \ \ \yv' = \yv^{*}[-n]
					\end{eqnarray}					
				\end{itemize}
			\end{block}
			
				\begin{columns}
					\column{0.45\columnwidth}
			\begin{block}{\alert{ \bf Key Observation}}
			\vspace{0.2cm}
				\begin{itemize}
					\item $\rv$ is {\color{blue}Sparse} with some noise.
				\end{itemize}
				 \begin{equation} \label{eqn:RXY_sparse}\nonumber
				 r[m] \ = \left\{
				 \begin{array}{ll}
				 &M,~~  \text{if} \ m \in \mathcal{T} \\
				 & n_m,~~ m \in [N]-\mathcal{T}
				 \end{array}
				 \right.  			
				 \end{equation}
			\end{block}
						
					\column[]{0.45\columnwidth}
					\begin{figure}
						\centering
						\scalebox{0.35}{\input{cross_corr.tex}}
					\end{figure}
					
				\end{columns}
				
					
\end{frame}
%--------------------------------------------------------------------------------------

%%--------------------------------------------------------------------------------------
%\begin{frame}{Spectral Estimation}
%	
%	\begin{columns}
%		
%		\column{0.35\textwidth}
%		\begin{figure}
%			\centering
%			\scalebox{0.60}{\input{monotone_time_plain.tex}}
%		\end{figure}
%		
%		\column{0.45\textwidth}
%		\begin{figure}
%			\centering
%			\scalebox{0.60}{\input{multitone_freq.tex}}
%		\end{figure}
%
%        \begin{itemize}
%        \item $N=75$
%        \item $x[n] = \displaystyle{\sum_{i=1}^L A_i \ e^{j \frac{2 \pi {\color{blue}{k_i}} n}{N}}}$
%		\end{itemize}
%	\end{columns}	
%\end{frame}
%%--------------------------------------------------------------------------------------
%\begin{frame}{Spectral Estimation}
%\begin{block}{Prony's method}
%Solved by {\color{blue}{Baron Gaspard Riche de Prony (1795)}}. Essai éxperimental et analytique: sur les lois de la dilatabilité de fluides élastique et sur celles de la force expansive de la vapeur de l'alkool, à différentes températures. Journal de l'école Polytechnique, volume 1, cahier 22, 24-76.
%\end{block}
%\begin{figure}
%  \centering
%  \includegraphics[width=2.0in]{Gaspard_de_Prony.pdf}
%\end{figure}
%\end{frame}
%--------------------------------------------------------------------------------------
\begin{frame}{Spectral Estimation}
	
	\begin{columns}
		
		\column{0.35\textwidth}
		\begin{figure}
			\centering
			\scalebox{0.60}{\input{monotone_time.tex}}
		\end{figure}
		
		\column{0.45\textwidth}
		\begin{figure}
			\centering
			\scalebox{0.60}{\input{monotone_freq.tex}}
		\end{figure}

        \begin{itemize}
        \item $N=75$
        \item $x[n] = A e^{j \frac{2 \pi {\color{blue}{k}} n}{N}}$, $\omega = e^{j \frac{2 \pi}{N}}$
        \item $x[n_1] = A \omega^{{\color{red}{k}}n_1}$
        \item $x[n_1+1] = A \omega^k \ \omega^{{\color{red}{k}}n_1}$
		\end{itemize}
	\end{columns}	
\end{frame}
%--------------------------------------------------------------------------------------
\begin{frame}{Spectral Estimation}
	
        \begin{columns}		
		\column{0.35\textwidth}
		\begin{figure}
			\centering
			\scalebox{0.60}{\input{multitone_time.tex}}
		\end{figure}
		
		\column{0.55\textwidth}
		\begin{figure}
			\centering
			\scalebox{0.60}{\input{multitone_freq.tex}}
		\end{figure}

        \begin{itemize}
        \item $x[n] = A_1 \omega^{k_1n} + A_2 \omega^{k_2 n}$
        \item $x[n_1] = A_1 \omega^{k_1n_1} + A_2 \omega^{k_2 n_2}$
        \item $x[n_1+1] =  A_1 \omega^{k_1} \omega^{k_1n_1} + A_2 \omega^{k_2} \omega^{k_2 n_2}$
		\end{itemize}
	\end{columns}	
\pause
\begin{block}{Prony's method does not really work}
\begin{itemize}
  \item Not robust to noise
\end{itemize}
\end{block}
\end{frame}
%-----------------------------------------
\begin{frame}\frametitle{Sparse Fourier Transform Approach}
        \begin{block}{Robust Sparse Fourier Transform - Pawar and Ramchandran'14}
	 		\begin{itemize}
	 			\item[-] Sparse graph code approach
	 			\item[-] Computational complexity : $O(N \log N)$
	 		\end{itemize}
	 	\end{block}
\pause
\begin{block}{Key modifications}
   \begin{itemize}
   	\item Optimized for the induced noise model
   	\item Correlation peak is always {\color{blue} positive}
   	\item Take advantage in decoding algorithm - {\color{blue}sub-linear} time complexity
   \end{itemize}
\end{block}
\pause
        \begin{block}{Faster GPS receiver - Hassanieh '12}
	 		\begin{itemize}
	 			\item[-] Exploited sparsity in Correlation function $R_{XY}$
                \item[-] Algorithm based on hashing
                \item[-] In this application, complexity is still $O(N \log N)$
	 		\end{itemize}		
	 	\end{block}	 	


\end{frame}

%% ------------

\begin{frame}{Main Idea}
\begin{columns}
	\column{0.45\columnwidth}
	\begin{figure}[t]
		\centering
		\includegraphics[width=2.3in]{Factorgraph_example}
	\end{figure}
	\column{0.45\columnwidth}
	\begin{figure}
		\centering
		\scalebox{0.23}{\input{Factorgraph_1sparse_example.tex}}
	\end{figure}
	
\end{columns}
\begin{columns}
	\column{0.75\textwidth}
	\begin{itemize}
		\item Compute hashes - sums of groups of indices (aliasing)
		\item Chinese remainder theorem - unique identification
		\item E.g. $N = 6 = 2 \times 3$, every $i \cong (j_1,j_2)$
		\item Pawar and Ramchandran' 13 - Structured computation through FFT
	\end{itemize}
	\column{0.25\textwidth}
	\begin{tabular}{|c|c|c|}
		\hline
		0 & 2 & 4 \\
		\hline
		3 & 5 & 1 \\
		\hline
		%  \caption{}\label{}
	\end{tabular}
\end{columns}
\end{frame}

%--------------------------------------------------------------------------------------
\def\fracty{0.45}
\def\fractx{0.8}
\begin{frame}{Main idea - $\rv = \underset{\text{\color{red} 3 } } {\mathcal{F}_{N}^{-1}} \ \{ \underset{\text{ \color{red} 1 } }{  \mathcal{F}_{N}\{\xv\}}  \odot \ \underset{\text{ \color{red} 2 } }{ \mathcal{F}_{N}\{\yv'\}}  \}$}
	%	\begin{figure}[t]
	%		\centering
	%		\includegraphics[width=4.8in]{Example_full_framework}
	%	\end{figure}
	
	\begin{figure}[t]
		\centering
		\includegraphics[width=4.8in]{Example_full_framework.pdf}
	\end{figure}
	
	%\begin{columns}
	%\vspace{-2.0in}
	%\begin{column}{0.48\textwidth}
	%%	Signal Domain
	%\begin{figure}
	%\centering
	%%\text{$x[n]:$}
	%%\resizebox{\fractx\columnwidth}{\fracty\columnwidth}{\input{\fig_path/database_x.tex}}
	%\input{\fig_path/database_x.tex}
	%\end{figure}
	%\vspace{-0.2in}
	%
	%\begin{figure}
	%\centering
	%%\resizebox{\fractx\columnwidth}{\fracty\columnwidth}{\input{\fig_path/query_y.tex}}
	%\input{\fig_path/query_y.tex}
	%\end{figure}
	%
	%\vspace{-0.2in}
	%
	%\begin{figure}
	%\centering
	%%\resizebox{\fractx\columnwidth}{\fracty\columnwidth}{\input{\fig_path/corr_est.tex}}
	%\input{\fig_path/corr_est.tex}
	%\end{figure}
	%\end{column}
	%
	%%\begin{column}{0.48\textwidth}
	%%\tiny{Fourier Domain ($N$-pt FFT)}
	%%\begin{figure}
	%%\centering
	%%\resizebox{0.7\columnwidth}{!}{\input{\fig_path/database_sketch.tex}}
	%%\end{figure}
	%%
	%%\begin{figure}
	%%\centering
	%%\resizebox{0.7\columnwidth}{!}{\input{\fig_path/query_sketch.tex}}
	%%\end{figure}
	%%
	%%\begin{figure}
	%%\centering
	%%\resizebox{0.7\columnwidth}{!}{\input{\fig_path/corr_fft.tex}}
	%%\end{figure}
	%%\end{column}
	%
	%\end{columns}
\end{frame}
%%---------------------------------------------------------
%\begin{frame}\frametitle{Sparse Fourier Transform Approach}
%	\begin{figure}[t]
%		\centering
%		\scalebox{0.28}{\input{notional_diag.tex}}
%	\end{figure}
%\vspace{-0.2cm}
%	 	\begin{block}{}
%	 		\begin{equation}\label{eqn:Rxy_fourier}\nonumber
%	 		\boxed{\rv = \underset{\text{\color{red} 3 } } {\mathcal{F}_{N}^{-1}} \ \{ \underset{\text{ \color{red} 1 } }{  \mathcal{F}_{N}\{\xv\}}  \odot \ \underset{\text{ \color{red} 2 } }{ \mathcal{F}_{N}\{\yv'\}}  \} }
%	 		\end{equation}
%	 		
%	 		\begin{itemize}
%	 			\item[\color{red} 1.] \textit{\color{blue} Sketch of $\xv$ : }  Assume $ \Xv[l] = \mathcal{F}\{\xv\}$ is precomputed at positions $l \in \mathcal{S}$.
%	 			
%	 			\item[\color{red} 2.] \textit{\color{blue} Sketch of $\yv$:}  Compute $ \Yv'[l] = \mathcal{F}\{\yv'\}$ for $l \in \mathcal{S}$.
%	 			\begin{itemize}
%	 				\item[-] Only $M$ non-zero values in $\yv'$ - Efficient computation (folding and adding)
%	 			\end{itemize}
%	 			
%	 			\item[\color{red} 3.] \textit{\color{blue} Sparse $\mathcal{F}^{-1}$}:
%	 			\begin{itemize}
%	 				\item[-] Robust Sparse Inverse Fourier Transform (RSIDFT)
%	 				\item[-] Efficient Implementation- {\color{blue} sublinear} time and sampling complexity
%	 			\end{itemize}
%	 		\end{itemize}
%	 	\end{block}
%\end{frame}
%

%%------------------------------------------------------------------
%\begin{frame}{Robust Sparse Inverse Fourier Transform(RSIDFT)}
%\begin{block}{Main Idea}
%	\begin{itemize}
%		\item \alert{Sub-sampling} in frequency corresponds to \alert{aliasing} in time
%		\item Aliased coefficients $\Leftrightarrow$ parity check constraints of \alert{GLDPC codes}
%		\item \alert{CRT} guided sub-sampling induces a code good for \alert{Peeling decoder}
%		\item R-FFAST- proposed by Pawar and Ramchandran 2014
%	\end{itemize}
%\end{block}
%\begin{block}{Key modifications}
%   \begin{itemize}
%   	\item Optimized for the induced noise model
%   	\item Correlation peak is always {\color{blue} positive}
%   	\item Take advantage in decoding algorithm - {\color{blue}sub-linear} time complexity
%   \end{itemize}
%\end{block}
%\end{frame}

%--------------------------------------------------------------------------------------
	\begin{frame}{DSP - A Quick Review}
			\begin{columns}
				
				\column{0.35\textwidth}
				\begin{figure}
					\centering
					\scalebox{0.60}{\input{multitone_time_real.tex}}
				\end{figure}
				
				\column{0.45\textwidth}
				\begin{figure}
					\centering
					\scalebox{0.60}{\input{multitone_freq.tex}}
				\end{figure}
			\end{columns}
		
		\begin{block}{Discrete Fourier Transform}
		    \[
		        x[n]\autorightleftharpoons{DFT}{IDFT}X[k]
		    \]
		$$X[k] = \sum_{n=0}^{N-1} x[n] e^{\frac{-j 2 \pi k n}{N}} $$
		$$x[n] = \frac{1}{N}\sum_{k=0}^{N-1} X[k] e^{\frac{+j 2 \pi k n}{N}} $$
		\end{block}
	\end{frame}


	\begin{frame}{DSP - A Quick Review}
\begin{block}{Subsampling results in aliasing}
\begin{itemize}
  \item Let $x[n] \xrightarrow{N-DFT} X[k] , \ \ k,n = 0,1, \ldots,N-1$
  \item Let $x_{s}[m]  = x[mL] , \ \ m = 0,1, \ldots, N/L=M$ be a sub-sampled signal
  \item Let $x_s[m] \xrightarrow{M-DFT} X_s[l]$ be the DFT of the sub-sampled signal
  \item $\boxed{X_s[l] = M\sum\limits_{p=0}^{L-1}X[l+pM]}$
\end{itemize}
\end{block}
\pause
\begin{block}{Shift in time}
\begin{itemize}
  \item Let $x[n] \xrightarrow{N-DFT} X[k] , \ \ k,n = 0,1, \ldots,N-1$
  \item $x[n-n_0] \xrightarrow{N-DFT} \omega^{n_0k} X[k]$
  \item $x[n-1] \xrightarrow{N-DFT} \omega^k X[k]$
\end{itemize}
\end{block}
\end{frame}
	%--------------------------------------------------------------------------------------
	\begin{frame}{Aliasing and Sparse Graph Codes}
	%\begin{itemize}
	%
	%\item Give a simple example that explains how aliasing can induce a Sparse Graph Code.\\ \item Introduce the Tanner Graph for the induced code here (No background required since it is covered in Part I).	
	%\end{itemize}
	
		\begin{block}{$N = P1 \times P_2 = 2 \times 3$}
			\begin{figure}[t]
				\centering
				\includegraphics[width=3.1in]{X_DFT}
			\end{figure}
		\end{block}
		
		\begin{columns}
			
			\column{.47\textwidth}
			\begin{block}{{\small $\color{red}x_s$:\ Sub-sampled by $f_1=P_1=2$}}
				\begin{figure}[t]
					\centering
					\includegraphics[width=2.3in]{Xs}
				\end{figure}
			\end{block}
			
			\begin{block}{{\small$\color{red}z_s$:\ Sub-sampled by $f_2=P_2=3$}}
				\begin{figure}[t]
					\centering
					\includegraphics[width=2.3in]{Zs}
				\end{figure}
			\end{block}
			
			\column{.47\textwidth}
			\begin{block}{\small Factor graph}
				\begin{figure}[t]
					\begin{center}
						\resizebox{1.0\textwidth}{!}{\input{Factorgraph_example.tex}}
					\end{center}
				\end{figure}
%				\begin{figure}[t]
%					\centering
%					\includegraphics[width=2.3in]{Factorgraph_example}
%				\end{figure}
			\end{block}
		\end{columns}
	\end{frame}

	\begin{frame}{Aliasing and Sparse Graph Codes}
	%\begin{itemize}
	%
	%\item Give a simple example that explains how aliasing can induce a Sparse Graph Code.\\ \item Introduce the Tanner Graph for the induced code here (No background required since it is covered in Part I).	
	%\end{itemize}
	
	\begin{block}{$N = P_1 \times P_2 = 2 \times 3$}
		\begin{figure}[t]
			\centering
			\includegraphics[width=3.1in]{X_DFT}
		\end{figure}
	\end{block}
	
	\begin{columns}
		
		\column{.47\textwidth}
		\begin{block}{{\small $\color{red}x_s$:\ Sub-sampled by $f_1=P_1=2$}}
			\begin{figure}[t]
				\centering
				\includegraphics[width=2.3in]{Xs_shift}
			\end{figure}
		\end{block}
		
		\begin{block}{{\small$\color{red}z_s$:\ Sub-sampled by $f_2=P_2=3$}}
			\begin{figure}[t]
				\centering
				\includegraphics[width=2.3in]{Zs_shift}
			\end{figure}
		\end{block}
		
		\column{.47\textwidth}
		\begin{block}{\small Factor graph}
			
			\begin{figure}[t]
				\begin{center}
					\resizebox{1.0\textwidth}{!}{\input{Factorgraph_example_tilde.tex}}
					\end{center}
				\end{figure}
%			\begin{figure}[t]
%				\centering
%				\includegraphics[width=2.3in]{Factorgraph_example_tilde}
%			\end{figure}
		\end{block}
	\end{columns}
\end{frame}
	
	
	%--------------------------------------------------------------------------------------
	\begin{frame}{FFAST Algorithm Example}
	%Slide-1:
	%\begin{itemize}
	%	\item Block Diagram of a simple 2-stage FFAST setup.
	%	\item Tanner Graph of the induced code with the parity equations displayed.
	%\end{itemize}

\begin{figure}[t]
	\begin{center}
		\resizebox{0.75\textwidth}{!}{\input{FFAST_2stages_example.tex}}
	\end{center}
\end{figure}
	\vspace*{-4mm}
\begin{figure}[t]
	\begin{center}
		\resizebox{0.52\textwidth}{!}{\input{Factorgraph_example_tilde.tex}}
	\end{center}
\end{figure}
	
	\end{frame}
	%------------------------------------------------------------------------------------
	\begin{frame}{Singleton Detection}
			
		%Slide-2: Singleton Detection
		%\begin{itemize}
		%	\item Tanner Graph of the induced code with the equations displayed
		%	\item Simple example explaining singleton detection (ratio test)
		%	\item Summarize the singleton detection condition. {\bf(put in slide-3 if needed)}
		%\end{itemize}
		
			\vspace{-5pt}
			\begin{columns}
				\column{0.55\columnwidth}
			\begin{figure}[t]
				\begin{center}
					\resizebox{0.8\textwidth}{!}{\input{Factorgraph_example_tilde.tex}}
				\end{center}
			\end{figure}
				\column{0.45\columnwidth}
				\begin{block}{Bin classification}
					\begin{itemize}
						\item \alert{Zeroton}: 
						\vspace{3pt}
						$\begin{bmatrix}
							0 \\
							0 
						\end{bmatrix}$
						\item \alert{Singleton:}
						\vspace{3pt}
%					\begin{align}
						$
						\begin{bmatrix}
						r[p_1] \\
						r[p_1]w^{p_1}
						\end{bmatrix}$
%					\end{align}
						
						\item \alert{Multiton:}\\
						\vspace{7pt}
%						\begin{align}
						$
						\begin{bmatrix}
						r[p_1]&+ \cdots &+&r[p_d] \\
						r[p_1]w^{p_1}&+\cdots &+&r[p_2]w^{p_d}
						\end{bmatrix}$
%						\end{align}
					\end{itemize}
				\end{block}
				
			\end{columns}
			\begin{block}{Singleton condition for a checknode}
			\begin{itemize}
				\item Let $i=\frac{N}{j2\pi} \log(\frac{\tilde{x}_s[l]}{x_s[l]})$. If {\color{blue} $0 \leq i \leq N-1$}, then checknode $l$ is a \alert{Singleton}.\\
				\item $Pos(l) = i$ is the only variable node participating and $X_s[l]$ is its value.
			\end{itemize}
				
			\end{block}
			
	\end{frame}	

%%------------------ Peeling Decoder ------
%
%\input{peeling.tex}

	%-------------------------------------------------------------------------------------
%	\begin{frame}{FFAST Decoder}
%		
%		%Slide-3: Decoder
%		%\begin{itemize}
%		%	\item Tanner Graph of the induced code with the equations displayed
%		%	\item Explain in few lines how the peeling decoder works for this setup
%		%	\item Include a complete example with graphics that explains the complete FFAST decoder {\bf(if needed)}
%		%\end{itemize}
%		
%%	\begin{columns}
%%			\column{0.50\textwidth}
%%			\begin{figure}[t]
%%				\centering
%%				\includegraphics[width=2.5in]{C:/Users/nrkri/Dropbox/Work/RESEARCH/Presentations/NASIT2016/Figures/FFAST_2stages}
%%			\end{figure}
%%			\vspace{-6mm}
%%			\hspace{-1.5in}
%%			\column{0.50\textwidth}
%%			
%%			\begin{figure}[t]
%%				
%%				\includegraphics[width=2.45in]{C:/Users/nrkri/Dropbox/Work/RESEARCH/Presentations/NASIT2016/Figures/Factorgraph_example_tilde}
%%			\end{figure}
%%			
%%		\end{columns}
%			\vspace{-5pt}
%			\begin{figure}[t]				
%%\includegraphics[width=3.0in]{C:/Users/nrkri/Dropbox/Work/RESEARCH/Presentations/NASIT2016/Figures/Factorgraph_example_tilde}
%\resizebox{3.0in}{!}
%{
%\input{Factorgraph_example_tilde.tex}
%}
%			\end{figure}
%
%		\begin{block}{Peeling decoder}
%			\begin{itemize}
%				\item 1 non-zero value among the neighbors of any right node can be recovered
%								
%				\item Iteratively errors can be corrected and analyzed for random non-zero coeffs
%			\end{itemize}
%		\end{block}
%	\end{frame}	
	%-----------------------------------------------------------------------------------------
	\begin{frame}{FFAST Decoder Example}
	\only<1-2>{\begin{block}{Example 1}
		Let $N=6$, and the non-zero coefficients be r[0]=5, r[3]=4, r[4]=7
	    \end{block}	}
	    \vspace{-7pt}
		\begin{columns}
			\only<1-2>{
				\column{0.43\textwidth}
                \resizebox{2.0in}{!}
                {
                \input{Factorgraph_example_tilde_abbrv.tex}
                }

				\column{0.55\textwidth}

				\resizebox{2.3in}{!}
				{
					\input{Factorgraph_example1_tilde.tex}
				}
			}
				%\column{0.25\textwidth}
				%\only<3>{\large \color{green} Yes, recoverable!}			
		\end{columns}
		
		\vspace{-7pt}
		
	\only<3-4>{
		
		 \begin{block}{Example 2}
		Let $N=6$, and the non-zero coefficients be r[0] = 5, r[1] = 3 , r[2]=1, r[3] = 4, r[4] = 7.
	     \end{block}}
     	\begin{columns}
     	\only<3->{
     		\column{0.43\textwidth}
     		\resizebox{1.8in}{!}
     		{
     			\input{Factorgraph_example_tilde_abbrv.tex}
     		}
     		
     		\column{0.55\textwidth}
			\resizebox{2.3in}{!}
			{
				\input{Factorgraph_example2_tilde.tex}
			}
		}
     	%\column{0.25\textwidth}
     	%\only<3>{\large \color{green} Yes, recoverable!}		
     	\only<4>{\Large \alert{ Not recoverable!}}		
     \end{columns}
  	
	\end{frame}
%	
	%-----------------------------------------------------------------------------------------
%	\begin{frame}{FFAST Construction For Different Regimes}
%%	\begin{itemize}
%%		\item Figure: Block diagram of the a d-staged FFAST setup with generalized down-sampling parameter
%%		\item Give the values of the down-sampling parameters for less-sparse and very-sparse regime. (Also, give the aliasing equations for each case {\bf(if needed)}).
%%		\item Include the generalized FFAST construction for each $\delta$ ranges {\bf(if needed)}
%%	\end{itemize}	
%
%	\begin{figure}[t]
%		\includegraphics[width=3.0in]{C:/Users/nrkri/Dropbox/Work/RESEARCH/Presentations/NASIT2016/Figures/FFAST_2stages}
%	\end{figure}
%	
%	
%
%			Let, $N=P_1 \times P_2 \times \ldots P_d$
%	\begin{columns}
%		\column{0.4\textwidth}
%		
%		
%		\begin{block}{\alert{Very-sparse regime}}
%			\begin{itemize}
%				\item $K = O(N^\delta), 0<\delta \leq 1/3$
%				\item \textcolor{blue}{$f_i= N/P_i,\ i=1,2, \ldots,d$}
%				\end{itemize}
%				\end{block}
%				
%				\column{0.4\textwidth}
%				\begin{block}{\alert{Less-sparse regime}}
%					\begin{itemize}
%						\item $K = O(N^\delta), 1/3<\delta \leq 1$
%						\item \textcolor{blue}{$f_i= P_i,\ i=1,2, \ldots,d$}
%						\end{itemize}
%						\end{block}
%						
%						\end{columns}
%							
%
%			\end{frame}
	%-------------------------------------------------------------------------------------
%	\begin{frame}{Generalization}
%		
%		%Slide-3: Decoder
%		%\begin{itemize}
%		%	\item Tanner Graph of the induced code with the equations displayed
%		%	\item Explain in few lines how the peeling decoder works for this setup
%		%	\item Include a complete example with graphics that explains the complete FFAST decoder {\bf(if needed)}
%		%\end{itemize}
%		
%	\begin{columns}
%			\column{0.50\textwidth}
%			\begin{figure}[t]
%				\centering
%				\includegraphics[width=2.5in]{FFAST_2stages}
%			\end{figure}
%			\vspace{-6mm}
%			\hspace{-1.5in}
%			\column{0.50\textwidth}
%			
%			\begin{figure}[t]
%				
%				\includegraphics[width=2.45in]{Factorgraph_example_tilde}
%			\end{figure}
%			
%		\end{columns}
%%			\vspace{-5pt}
%%			\begin{figure}[t]
%%				
%%				\includegraphics[width=2.75in]{C:/Users/nrkri/Dropbox/Work/RESEARCH/Presentations/NASIT2016/Figures/Factorgraph_example_tilde}
%%			\end{figure}
%		\begin{block}{Reed Solomon component codes}
%			\begin{itemize}
%				\item $(X_s[l_1],\tilde{X}_s[l_1])$ correspond to 2 syndromes of a 1-error correcting RS code
%                \item RS is over the complex field, no miscorrection
%			\end{itemize}
%		\end{block}
%	\end{frame}	



%---------------------------------------------------------
%\begin{frame}\frametitle{Sparse Fourier Transform Approach}
%	\begin{figure}[t]
%		\centering
%		\scalebox{0.28}{\input{notional_diag.tex}}
%	\end{figure}
%\begin{itemize}
%\item In the pattern matching problem we want to recover a sparse IDFT
%\item FFT $\Leftrightarrow$ IFFT duality can be used
%\end{itemize}
%\end{frame}
%--------------------------------------------------------------------------------------------------------
\begin{frame}\frametitle{RSIDFT Framework}
	\begin{itemize}
    \item $N = P_1 \ P_2 \ldots \ P_d$,  $f_i = \begin{cases}
    N/P_i  ~~~~~~~ \mu< 0.5\\
    P_i  ~~~~~~~  \mu >= 0.5  \\
    \end{cases} $.\\
    Choose $d$ s.t. $f_i \approx N^\alpha$, $\alpha > 1-\mu$
    \item Subsample by $N^{1-\alpha} \Rightarrow$ aliasing from $N^{1-\alpha}$ coeffs
    \end{itemize}
		\begin{figure}[t!]
			\begin{center}
				\resizebox{0.7\textwidth}{!}{\input{FFAST_Robust_PM.tex}}
				%	 		\includegraphics[height=7cm]{Figures/FFAST_Robust}
			\end{center}	
			\label{fig:rsidft}
			\vspace{5 pt}
		\end{figure}
\end{frame}
%--------------------------------------------------------------------------------------------------------
\begin{frame}\frametitle{RSIDFT-Decoding (Peeling Decoder)}
\begin{columns}
	\column[]{0.65\columnwidth}
		\begin{figure}[h!]
			\begin{center}
				%		\includegraphics[height=7cm]{Figures/Factorgraph}
				\resizebox{1.0\textwidth}{!}{\input{Factorgraph_PM.tex}}	
			\end{center}	
			%\caption{Example of a Tanner graph formed in a RSIDFT framework with system parameters being $d=2$, $B=2$, $N=6$, $f_1 = 2$ and $f_2=3$. The variable nodes (colored gray circles) represent the cross-correlation vector $\rv$ and the bin nodes (uncolored white boxes) represent the binned observation vector $\zv_{i,k}$. The figure also illustrates the relationship between $\zv_{i,k}$ and $\rv$.}\label{fig:factorgraph}
		\end{figure}
	\column[]{0.33\columnwidth}
	    \begin{block}{Observations:}
	          $
	    		\zv_{i,k} = \begin{bmatrix}
	    		r_{i,1}[k]\\
	    		r_{i,2}[k]\\
	    		\vdots\\
	    		r_{i,B}[k]
	    		\end{bmatrix}
	    		$
	    	\end{block}
	    	\begin{block}{Decoding- 3 steps}	
	    		\begin{enumerate}
	    			\item Bin Classification
	    			\item Position Identification
	    			\item Peeling Process
	    		\end{enumerate}
	
	    \end{block}
	
		
\end{columns}	
\end{frame}

\begin{frame}\frametitle{Observations}
\begin{align} \nonumber
	    		\zv_{i,k} = \begin{bmatrix}
	    		r_{i,1}[k]\\
	    		r_{i,2}[k]\\
	    		\vdots\\
	    		r_{i,B}[k]
	    		\end{bmatrix}			
            = \begin{bmatrix}
			1 & 1 & \ldots & 1 \\
			\omega^{k}_{2} & \omega^{(k+f_i)}_{2} & \ldots & \omega^{(k+(g_i-1)f_i)}_{2}   \\
			\vdots & \vdots & \ddots & \vdots\\
			\omega^{k}_{B} & \omega^{(k+f_i)}_{B} & \ldots & \omega^{(k+(g_i-1)f_i)}_{B} \\
			\end{bmatrix} \times
			\begin{bmatrix}
			r[k+(0)f_i] \\
			r[k+(1)f_i] \\
			\vdots\\
			r[k+(g_i-1)f_i]
			\end{bmatrix}
			\end{align}

			\begin{figure}[t]
			\begin{center}
				\includegraphics[width=3.5in]{bin_statistics.pdf}
			\end{center}
		\end{figure}

\end{frame}

\begin{frame}\frametitle{Decoder}

\only<1>{\begin{block}{Bin Classification}
		\begin{itemize}
			\item  Classify each check-node -  Zero-ton / Single-ton / Multi-ton
			\item  {\color{blue} Threshold constraints} on first observation $z_{i,k}[1] = z$
			\item  Threshold varies with $\eta$
			\begin{itemize}
				\item[-] different for exact($\eta=0$) and approximate matching
			\end{itemize}
		\end{itemize}
		\begin{align}
		\label{Eqn:BinClassifApprox} \nonumber
		\widehat{\mc{H}}_{i,j}=
		\begin{cases}
		\mc{H}_z &  	 z/M < \gamma_1\\
		\mc{H}_s &	  \gamma_1 < z/M < \gamma_2  \\
		\mc{H}_d  &    \gamma_2  < z/M <  \gamma_3\\
		\mc{H}_m &      z/M > \gamma_3\\
		\end{cases}
		\end{align}
		where $(\gamma_1,\gamma_2,\gamma_3)=(\frac{1-2\eta}{2},\frac{3-4\eta}{2},\frac{5-6\eta}{2})$
	\end{block}}
	
\only<2>{\vspace{-5pt} \begin{block}{Position Identification}
		\begin{itemize}
			\item 	{\color{blue}Observation:} 	\begin{align} \nonumber
			\zv_{i,k}= \begin{bmatrix}
			1 & 1 & \ldots & 1 \\
			\omega^{k}_{2} & \omega^{(k+f_i)}_{2} & \ldots & \omega^{(k+(g_i-1)f_i)}_{2}   \\
			\vdots & \vdots & \ddots & \vdots\\
			\omega^{k}_{B} & \omega^{(k+f_i)}_{B} & \ldots & \omega^{(k+(g_i-1)f_i)}_{B} \\
			\end{bmatrix} 
			\begin{bmatrix}
			r[k+(0)f_i] \\
			r[k+(1)f_i] \\
			\vdots\\
			r[k+(g_i-1)f_i]
			\end{bmatrix}
			\end{align}
			
			\item Column that gives {\color {blue} maximum correlation} with the observation\\
			{ \[\boxed{\hat{k} = \underset{k\in\{j+l f_i\}}{\arg \max}~~ \zv^{\dagger}_{i,j} \mathbf{W}[:,l]}\]}
		\end{itemize}
	
		\end{block}}

\only<3>{\begin{block}{Peeling}
\begin{itemize}
  \item Peel the known variables
  \item Repeat the process for doubleton and multiton nodes
\end{itemize}
\end{block}
}
%\only<3>{\begin{block}{Error Probability}
%			\begin{align*}
%			\mbb{P}(\mathcal{E}_{\text{total}}) & \leq &  \mbb{P}(\mathcal{E}_1)~~~~~ & + & \mbb{P}(\mc{E}_2)~~~~~~~~ & + & \mbb{P}(\mc{E}_3)~~~~~~~~\\
%			&\leq & 6e^{-\frac{N^{\mu+\alpha-1}(1-6\eta)^2}{16}}~ & + & 2e^{-N^{\mu+\alpha-1} ~ c_1(\eta)}~ & + &  e^{-c_3 N^{c_4\alpha}}
%			\end{align*}
%			\[\boxed{	\mbb{P}(\mathcal{E}_{\text{total}}) \rightarrow 0  ~~\text{if}~~  \alpha >1-\mu}\]
%		\end{block}
%	     }	
\end{frame}
%-------------------------------------------------------------------------------------------------
\begin{frame}{Error Analysis}

\begin{block}{Error Events}
	\begin{enumerate}\small
		\item {\color{blue}$\mathcal{E}_1${-\it Bin Classification}}: Bin is wrongly classified
		\item {\color{blue}$\mathcal{E}_2${-\it Pos. Identification}}: Position of singleton is identified wrongly, given a singleton
		\item {\color{blue}$\mathcal{E}_3${-\it Peeling Process}}: Peeling process fails to recover the $L$ significant correlation coefficients, given $\mbb{P}(\mc{E}_1)= \mbb{P}(\mc{E}_2)=0$
	\end{enumerate}
\end{block}
\end{frame}

\begin{frame}{Error Analysis}

\begin{block}{Error Events}
\begin{enumerate}\small
	\item {\color{red}$\mathcal{E}_1${-\it Bin Classification}}: Bin is wrongly classified
	\item {\color{blue}$\mathcal{E}_2${-\it Pos. Identification}}: Position of singleton is identified wrongly, given a singleton
	\item {\color{blue}$\mathcal{E}_3${-\it Peeling Process}}: Peeling process fails to recover the $L$ significant correlation coefficients, given $\mbb{P}(\mc{E}_1)= \mbb{P}(\mc{E}_2)=0$
\end{enumerate}
\end{block}
\end{frame}

\begin{frame}{Error Analysis}

\begin{block}{Error Events}
	\begin{itemize}\small
		\item {\color{blue}$\mathcal{E}_1${-\it Bin Classification}}: Bin is wrongly classified
		\item {\color{red}$\mathcal{E}_2${-\it Pos. Identification}}: Position of singleton is identified wrongly, given a singleton
		\item {\color{blue}$\mathcal{E}_3${-\it Peeling Process}}: Peeling process fails to recover the $L$ significant correlation coefficients, given $\mbb{P}(\mc{E}_1)= \mbb{P}(\mc{E}_2)=0$
	\end{itemize}
\end{block}
\end{frame}



%----------------------------------------------------------------------------------------------------------------



\begin{frame}{Bin Classification Error}
\vspace{-10pt}
{ \small
	\begin{align*} \nonumber
	Z[1]=\begin{cases}
	\sum\limits_{\ell=0}^{g_{i}-1}\sum\limits_{k=0}^{M-1} n_{l,k}  & ~~\text{ if } ~~ \mc{H}=\mc{H}_z\label{Eqn:BinCombination}\\
	M_1+\sum\limits_{\ell=0}^{g_{i}-2}\sum\limits_{k=0}^{M-1} n_{l,k}  & ~~\text{ if } ~~ \mc{H}=\mc{H}_s\\
	M_1+M_2+\sum\limits_{\ell=0}^{g_{i}-3}\sum\limits_{k=0}^{M-1} n_{l,k}  & ~~\text{ if } ~~ \mc{H}=\mc{H}_d\\
	\end{cases}
	\end{align*}
	
	where $n_{l,k}=x[\theta_{\ell}+k]y[k]$,  $\theta_{\ell}\notin\{\tau_1,\tau_2,\ldots,\tau_L\}$, and  $M_1,M_2\in[M(1-2\eta):M]$.
}


\begin{block}{$\mathcal{E}_1${-\it Bin Classification}}
	\begin{figure}[t]
		\begin{center}
			\includegraphics[width=2.5in]{bin_statistics.pdf}
		\end{center}
	\end{figure}
	\vspace{-30pt}
	
	{\small \begin{align*}
		\mbb{P}[\mathcal{E}_1] & \leq & \mbb{P}[\mathcal{E}_1|\widehat{\mathcal{H}}_{i,j}=\mc{H}_z]~& + &
		\quad \mbb{P}[\mathcal{E}_1|\widehat{\mathcal{H}}_{i,j}=\mathcal{H}_s]~~~~~& + &
		\quad \mbb{P}[\mc{E}_1|\widehat{\mathcal{H}}_{i,j}=\mathcal{H}_d \cup \mathcal{H}_m]\\
		~& = & \mbb{P}[z[1]>\gamma_1] ~& + & (1- \mbb{P}[\gamma_1<z[1]<\gamma_2])  ~& + & \mbb{P}[z[1]<\gamma_2]~~~~~~\\
		\end{align*}		
	}
	
	\vspace{-15pt}
\end{block}
\end{frame}



\begin{frame}{Bin Classification Error}

\begin{lemma}[Hoeffding tail bound]% for bounded random variables]
	\label{Lem:Chernoff}
	Let $X_1, X_2,\ldots, X_n$ be a sequence of independent random variables such that $X_i$ has mean $\mu_i$ and sub-Gaussian parameter $\sigma_i$. Then for any $\delta>0$:
	\begin{align*}
	\text{\textbf{Upper Tail}}: ~&\mbb{P}\left[\sum \left(X_i-\mu_i\right)\geq \delta\right]\leq \exp\left\lbrace-\frac{\delta^2}{2\sum \sigma_i^2}\right\rbrace\\
	\textbf{Lower Tail}: ~&\mbb{P}\left[\sum \left(X_i-\mu_i\right)\leq -\delta\right]\leq \exp\left\lbrace-\frac{\delta^2}{2\sum \sigma_i^2}\right\rbrace
	\end{align*}
	Note that for bounded random variables $X_i\in [a,b]$ the sub-Gaussian parameter is $\sigma_i=\frac{b-a}{2}$ whereupon the upper tail Hoeffding bound can be simplified to
	\begin{align*}
	%\text{\textbf{Upper Tail}}: ~&
	\mbb{P}\left[\sum_{i=1}^{n} \left(X_i-\mu_i\right)\geq \delta\right]\leq \exp\left\lbrace-\frac{2\delta^2}{n(b-a)^2}\right\rbrace.
	%\textbf{Lower Tail}: ~&\mbb{P}\left[\sum_{i=1}^{n} \left(X_i-\mu_i\right)\leq -\delta\right]\leq \exp\left\lbrace-\frac{2\delta^2}{n\sum(b-a)^2}\right\rbrace
	\end{align*}
	Similarly the lower tail bound can be simplified.
\end{lemma}
\end{frame}




\begin{frame}{Bin Classification Error}
\begin{lemma}
	The probability of bin classification error at any bin $(i,j)$ can be upper bounded by
	\begin{align*}
	\mbb{P}[\mc{E}_1]\leq 6e^{-\frac{N^{\mu+\alpha-1}(1-6\eta)^2}{16}}
	\end{align*} 
\end{lemma}
\end{frame}




%----------------------------------------------------------------------------------------------------------------


\begin{frame}{Position Identification Error}

\begin{block}{$\mathcal{E}_2${-\it Pos. Identification}}
	\begin{itemize}
		\item $\underline{z} =  r[j_p] ~ \wv_{j_p}+ \sum_{k \neq p}n_k \wv_{j_k}$
		\begin{align*}
		\mbb{P}[\mc{E}_2] &= & \mbb{P}[\wv_{j_p}^{\dagger}\underline{z}< \wv_{j_{p'}}^{\dagger}\underline{z}]
		&= & \mbb{P}[\sum_{k \neq p,p'}\alpha_k n_k+\beta n_{p'}\geq  r[j_p]\left(1-\frac{\wv_{j_{p'}}^{\dagger}\wv_{j_p}}{B}\right)]
		\end{align*}
		
		\item Mutual Incoherence property to bound the cross-correlation(noise) term
		\begin{itemize}
			\item [-] $\log N$ measurements (shifts) suffices [Pawar'14]
		\end{itemize}
	\end{itemize}
\end{block}
\begin{align} \nonumber
\zv_{i,k}= \begin{bmatrix}
1 & 1 & \ldots & 1 \\
\omega^{k}_{2} & \omega^{(k+f_i)}_{2} & \ldots & \omega^{(k+(g_i-1)f_i)}_{2}   \\
\vdots & \vdots & \ddots & \vdots\\
\omega^{k}_{B} & \omega^{(k+f_i)}_{B} & \ldots & \omega^{(k+(g_i-1)f_i)}_{B} \\
\end{bmatrix} 
\begin{bmatrix}
r[k+(0)f_i] \\
r[k+(1)f_i] \\
\vdots\\
r[k+(g_i-1)f_i]
\end{bmatrix}
\end{align}	
\end{frame}



\begin{frame}{Position Identification Error}
\begin{definition}[Mutual Incoherence]
	%	{}[]
	The mutual incoherence $\mu_{\text{max}}( \mathbf{W})$ of a matrix $\mathbf{W} = [\wv_1 ~ \wv_2 ~ \cdots \wv_i \cdots \wv_N ]$ is defined as 
	
	\[\mu_{\text{max}}(\mathbf{W}) \defeq \max \limits_{\forall i \neq j} \frac{|\wv_i^{\dagger} \wv_j |}{||\wv_i || . ||\wv_j ||} \]
\end{definition}

\begin{lemma}[Mutual Incoherence Bound for sub-sampled IDFT matrix (Pawar'14)]
	%	[Mutual Incoherence Bound for sub-sampled IDFT matrix (Pawar'14)]
	The mutual incoherence $\mu_{\text{max}}$ $(\mathbf{W_{i,k}})$ of the sensing matrix $\mathbf{W}_{i,k}$, with columns and rows sampled from IDFT matrix, and  $B$ shifts, is upper bounded by
	
	\[ \mu_{\text{max}} < 2\sqrt{\frac{\log(5N)}{B}} \] 
\end{lemma}
\end{frame}




\begin{frame}{Position Identification Error}

\begin{lemma}
	For some constant $c_1 \in \mathbb{R}$ and the choice of $B=4c_1^2\log 5N$,  the probability of error in identifying the position of a singleton at any bin $(i,j)$ can be upper bounded by
	\begin{align*}
	\mbb{P}[\mc{E}_{21}]\leq \exp\left\lbrace-\frac{N^{\mu+\alpha-1}(1-2\eta)^2(c_1^2-1)}{8(c_1^2+1)}\right\rbrace
	\end{align*}
\end{lemma}

\end{frame}


%----------------------------------------------------------------------------------------------------------------
\begin{frame}{Error Analysis}

\begin{block}{Error Events}
\begin{itemize}\small
	\item {\color{blue}$\mathcal{E}_1${-\it Bin Classification}}: Bin is wrongly classified
	\item {\color{blue}$\mathcal{E}_2${-\it Pos. Identification}}: Position of singleton is identified wrongly, given a singleton
	\item {\color{red}$\mathcal{E}_3${-\it Peeling Process}}: Peeling process fails to recover the $L$ significant correlation coefficients, given $\mbb{P}(\mc{E}_1)= \mbb{P}(\mc{E}_2)=0$
\end{itemize}
\end{block}
\end{frame}

\begin{frame}{Error in Peeling Process}

\begin{lemma}[Exact Matching]
For the exact matching case, choose $F^{d-1}=\delta N^\alpha$ where $\delta$ are thresholds from DE. Then the oracle based peeling decoder:
\begin{itemize}
\item successfully uncovers all the $L$ matching positions if $L=\Omega(N^{\alpha})$ and $L\leq N^{\alpha}$, with probability at least $1-O(1/N^{\frac{1}{d}})$
\item successfully uncovers all the $L$ matching positions, if $L=o(N^{\alpha})$, with probability at least $1-e^{-\beta \varepsilon_1^2N^{\alpha/(4l+1)}}$ for some constants $\beta,\varepsilon_1>0$ and $l>0.$
\end{itemize}\label{Lem:peeling_exact}
\end{lemma}

\begin{block}{$\mathcal{E}_3${-\it Peeling Process}}
\begin{itemize}
\item Tools from Coding Theory to analyze Sparse Graph Codes
\item Density Evolution to quantify Error Probability
\item \# of check-nodes is a function of sparsity (query length)
\item Error probability decays with $N$ - {\small R-FFAST and SAFFRON [Pawar'14, Lee'15]}
\end{itemize}
\end{block}
\end{frame}




%----------------------------------------------------------------------------------------------------------------



\begin{frame}{Error Analysis}

\begin{block}{Error Events}
		\begin{itemize}\small
			\item {\color{blue}$\mathcal{E}_1${-\it Bin Classification}}: Bin is wrongly classified
			\item {\color{blue}$\mathcal{E}_2${-\it Pos. Identification}}: Position of singleton is identified wrongly, given a singleton
			\item {\color{red}$\mathcal{E}_3${-\it Peeling Process}}: Peeling process fails to recover the $L$ significant correlation coefficients, given $\mbb{P}(\mc{E}_1)= \mbb{P}(\mc{E}_2)=0$
		\end{itemize}
\end{block}

\begin{block}{Error Probability}
		\begin{align*}
		\mbb{P}(\mc{E}_{\text{total}}) & \leq &  \mbb{P}(\mc{E}_1)~~~~~ & + & \mbb{P}(\mc{E}_2)~~~~~~~~ & + & \mbb{P}(\mc{E}_3)~~~~~~~~\\
		&\leq & 6e^{-\frac{N^{\mu+\alpha-1}(1-6\eta)^2}{16}}~ & + & 2e^{-N^{\mu+\alpha-1} ~ c_1(\eta)}~ & + &  e^{-c_3 N^{c_4\alpha}}
		\end{align*}
		\[\boxed{	\mbb{P}(\mc{E}_{\text{total}}) \rightarrow 0  ~~\text{if}~~  \alpha >1-\mu}\]
\end{block}
	
\end{frame}


%----------------------------------------------------------------------------------------------------------------
\begin{frame}{Complexity Analysis}
	\begin{block}{Sample Complexity}
		\vspace{-10pt}
		\begin{align*}
		\text{Total \# of samples required (S)} &= O \left(dBN^{\alpha}\right) =   {\color{blue}O(N^{1-\mu}\log N)}
		\end{align*}
	\end{block}
	\begin{block}{Computational Complexity}
		\begin{equation*}\label{eqn:Rxy_fourier}
		\rv = \underset{\color{red}  \RNum{2} } {\mathcal{F}_{N}^{-1}} \ \{   \mathcal{F}_{N}\{\xv\}  \odot \ \underset{\color{red}  \RNum{1}  }{ \mathcal{F}_{N}\{\yv'\}}  \}
		\end{equation*}
		\vspace{-10pt}

	\begin{itemize}
		\item {\color{blue}Sketch of Query:}\\ \vspace{5pt}
	 {\small $C_{\color{red}\RNum{1}} \ = \  dB ~
	 ( \underset{\text{Folding} }{\underbrace{N^{\mu}}} + \ \
	 \underset{\text{Shorter FFTs} }{\underbrace{N^{\alpha} \ \log N^{\alpha}}} \ )
	 =  {\color{blue} O(\max(N^{\mu}\log N,N^{1-\mu}\log^2 N)) }$}

		\item {\color{blue}RSIDFT:} \\	{\small$C_{\color{red} \RNum{2}} =  {d B}  \left (
			\underset{\text{Shorter IFFTs /block/stage} }{\underbrace{ O(N^{\alpha}  \log N^{\alpha})}} \hspace{-3pt}+ \underset{\text{Correlations} }{\underbrace{ L~N^{1-\alpha}}} \right ) = {\color{blue} O(\max(N^{1-\mu}\log^2 N ,N^{\mu+\lambda}\log N)) }$}
	\end{itemize}
	\vspace{10pt}	
	\[\boxed{C_{\text{total}} = \max(C_{\color{red} \text{ \RNum{1}}},C_{\color{red} \text{ \RNum{2}}}) = {\color{blue} O(\max(N^{1-\mu}\log^2 N ,N^{\mu+\lambda}\log N)) }}\]
		
	\end{block}
\end{frame}
%----------------------------------------------------------------------------------------------------------------
\begin{frame}\frametitle{Simulation Results}
\only<1>{\begin{figure}[h!]
		\begin{center}
			
			\resizebox{0.7\textwidth}{!}{\input{sim_results_M_10_7.tex}}	
				\end{center}
			\caption{Plot of Probability of Missing a Match vs. Sample Gain for Exact Matching of a substring of length $M=10^5$($\mu=0.41$) from a equiprobable  binary \{+1,-1\} sequence of length $N= 10^{12}$, divided into $G=10^{5}$ blocks each of length $\tilde{N}=10^7$. The substring was simulated to repeat in $L=10^6$($\lambda=0.5$) locations uniformly at random.}
	
\end{figure}}

\only<2>{\begin{figure}[h!]
	\begin{center}
		\resizebox{0.7\textwidth}{!}{\input{sim_results_M_10_3.tex}}	
	\end{center}

	\caption{Plot of Probability of Missing a Match vs. Sample Gain for Exact Matching of a substring of length $M=10^3$($\mu=0.25$) from a equiprobable  binary \{+1,-1\} sequence of length $N= 10^{12}$, divided into $G=10^{6}$ blocks each of length $\tilde{N}=10^6$. The substring was simulated to repeat in $L=10^6$($\lambda=0.5$) locations uniformly at random.}
	
\end{figure}}
\end{frame}

\section{Ongoing/Future work}
\begin{frame}{Extension to non-i.i.d. case}


\begin{figure}[t]
	\begin{center}
		\includegraphics[width=2.0in]{audio_signal.png}
	\end{center}
\end{figure}

\begin{block}{}
	\begin{itemize}
		\item {\color{blue}i.i.d assumption} does not hold for {\color{blue}natural signals} - audio, genome sequences
		\item Samples are {\color{blue} correlated} - Autocorrelation is not an impluse (less-sparse)
		\item Induces lot of false positives \& the complexity of RSIDFT decoder increases 		
	\end{itemize}	
\end{block}
\end{frame}

\begin{frame}{Extension to non-i.i.d. case - Whitening Filter}

\begin{columns}
	\column[]{0.50\columnwidth}
	
\begin{figure}[t]
	\begin{center}
		\includegraphics[width=2.0in]{whitening_block_diag.png}
	\end{center}
\end{figure}

	\column[]{0.50\columnwidth}

\begin{figure}[t]
	\begin{center}
	\includegraphics[width=2.0in]{autocorr_whiten.png}
	\end{center}
\end{figure}

\end{columns}


\begin{block}{}
	\begin{itemize}
	\item Model the auto-correlation $R_{XX}(\tau)$ in terms of an Auto Regressive (AR) process
		
		\[ \tilde{X}_t = \phi_1 \tilde{X}_{t-1} + \cdots + \phi_p \tilde{X}_{t-p} + Z_t \]\\
		where, $Z_t \sim \mathcal{N}(0,\sigma^2 I)$ is a white noise process. 

	\item Whiten $X$ to $Z$ using a FIR filter with coefficients $\Phi = \{\phi_1, \phi_2, \cdots, \phi_p\}$ from the AR process that models $X$
	\end{itemize}
\end{block}
\end{frame}

\begin{frame}{Extension to non-i.i.d. case}


	\begin{figure}[t]
		\begin{center}
			\includegraphics[width=4.5in]{nonIID.png}
		\end{center}
	\end{figure}
	

\begin{block}{}
	\begin{enumerate}
		\item Fit a AR($p$) process for the Database $\xv$ to obtain $\Phi$
		\item Whiten the database $\xv$ and query $\yv$ before passing it to RSIDFT framework 
		\begin{itemize}\normalsize
			\item[-] Linear Time Invariant (LTI) filter - preserves the substrings of the database after whitening
			\item[-] One time operation on the database - included in sketching
			\item[-] No additional increase in order complexity during the query phase
		\end{itemize} 
\end{enumerate}
\end{block}
\end{frame}


%-----------------------------------------------
%\begin{frame}{Secure Pattern Matching}
%\begin{figure}
%  \centering
%  \includegraphics[width=4.0in]{airline}
%\end{figure}
%\end{frame}

\begin{frame}{Secure Pattern Matching}
\begin{figure}
  \centering
  \includegraphics[width=4.0in]{doctorpatient}
\end{figure}
\end{frame}

\begin{frame}{Secure Pattern Matching - i.i.d. case}
\begin{figure}
	\centering
	\includegraphics[width=4.0in]{securePM_iid.png}
\end{figure}
\begin{block}{Secure PM - i.i.d. signal}
	\begin{itemize}
		\item Cloud Server - outsourced computation
		\begin{itemize}
			\item[-] untrusted party (given only encrypted forms)
			\item[-] compute matches only without knowing about $\xv$ or $\yv$
		\end{itemize}
		\item Encryption:
			$$\rv = \underset{\text{\color{red} 3 } } {\mathcal{F}_{N}^{-1}} \ \{ \underset{\text{ \color{red} 1 } }{  \mathcal{F}_{N}\{\xv\}}  \odot \ \underset{\text{ \color{red} 2 } }{ \mathcal{F}_{N}\{\yv'\}}  \}$$
		\vspace{-5pt}
		\begin{itemize}
			\item[-] $key$ = $\mathcal{A}^{|\mathcal{S}|}$, typically $\mathcal{A} = \{\pm 1\}$ (common for both parties)
			\item[-] $f_e(X,key) = X[\mathcal{S}] \odot key
			$
			\item[-] Multiplying by random vector in freq domain, garbles the signal - provides security 
		\end{itemize}
	\end{itemize}
\end{block}
\end{frame}

%\begin{frame}{Secure Pattern Matching - non i.i.d. case}
%\begin{figure}
%	\centering
%	\includegraphics[width=4.0in]{securePM_noniid.png}
%\end{figure}
%
%\begin{block}{Secure PM - non i.i.d. signal}
%	\begin{itemize}
%		\item Cloud Server - outsourced computation
%		\begin{itemize}
%			\item[-] untrusted party (given only encrypted forms)
%			\item[-] compute matches only without revealing any information about $\xv$ or $\yv$
%		\end{itemize}
%		\item Encryption:
%		\begin{itemize}
%			\item[-] Whitening operation is encryption: $key = \Phi$
%			\item[-] converts the data and query to a white noise process 
%			\item[-] $key$ can be distilled from the query (for a longer query length) 
%		\end{itemize}
%	\end{itemize}
%\end{block}
%\end{frame}

%--------------------------------------------------------------------------------------
\begin{frame}
\frametitle{Other Extensions to Sub-string Matching}

\begin{block}{Extensions}
	\begin{enumerate}
			\item Text Matching - Word embedding like Word2Vec
			\item Images - can be easily extended to 2D-data (images)
			\item Graph Pattern Matching  - define similarities based on graph spectrum (Graph Signal Processing)
	\end{enumerate}
\end{block}


\end{frame}


\begin{frame}%\frametitle{Publications}
Preprints:
{\footnotesize 
	\begin{itemize}
		\item N. T. Janakiraman, A. Vem, K. R. Narayanan, J.-F. Chamberland, {\blue``Sub-string/Pattern Matching in Sub-linear Time Using a Sparse Fourier Transform Approach",} arXiv:1704.07852 (presented in KDD'17 workshop on time series)
		\item N.T. Janakiraman, K. R. Narayanan, R. Calderbank {\blue``Improved Sub-linear Time Sparse Walsh-Hadamard Transform Using Sparse Graph Codes"}.
		\item A. Vem, N. T. Janakiraman, K. R. Narayanan,{\blue ``Group Testing using left-and-right-regular sparse-graph codes",} arXiv:1701.07477
		
	\end{itemize}
}

Conference/Journals:
{\footnotesize 
	\begin{itemize}
		\item N. T. Janakiraman, S. Emmadi, K. R. Narayanan, K. Ramchandran{\blue ``Exploring connections between Sparse Fourier Transform computation and decoding of product codes",} in Proc. of Allerton Conference on Communication, Control, and Computing, pp. 1366-1373, 2015
		\item A. Vem, N. T. Janakiraman, K. R. Narayanan, {\blue ``Sub-linear time compressed sensing for support recovery using left and right regular sparse-graph codes",} in Proc. of Information Theory Workshop, pp. 429-433, 2016
		\item P. Eedara, H. Li, N. T. Janakiraman, A. Tungala, J.F. Chamberland, G. Huff, {\blue ``Occupancy estimation with wireless monitoring devices and application-specific antennas",} IEEE Transactions on Signal Processing, 65(8), pp.2123-2135, 2017
		
		
	\end{itemize}
}
\end{frame}


\begin{frame}\frametitle{Next steps}
\begin{itemize}\itemsep10pt
\item Submit the preprints to Journals:
\begin{itemize}
	\item[-]  Pattern Matching
	\item[-]  Hadamard Transform
\end{itemize}
\item Conduct simulations on real datasets for non i.i.d. case
\item In-depth analysis of Secure Pattern Matching
\item Write the
journal paper on sparse graph signal estimation and its application to MAC problems
\end{itemize}
\end{frame}


\section{Other Sparse Estimation Works}

\subsection{Sparse Fourier Transform}


%-------------------------------------------------------------------------------------------------------			
\begin{frame} \frametitle{Fast Fourier Aliasing-based Sparse Transform (FFAST) }

\begin{block}{FFAST for Computing the FFT - Pawar and Ramchandran 2013}
	\begin{itemize}
		\item FFT of an $N$-length vector can be computed with complexity $N \log N$
		\item $K$-sparse spectrum - \alert{{Sampling complexity:}} $M = O(K)$ time domain samples
		\item $K$-sparse spectrum - \alert{{Computational complexity:}} $O(K \log{K})$
	\end{itemize}
\end{block}


\begin{block}{Main Idea}
	\begin{itemize}
		\item \alert{Sub-sampling} in time corresponds to \alert{aliasing} in frequency
		%\item Spectrum can be recovered from multiple versions of aliased spectrum if a  \alert{one-to-one} mapping exists
		\item Aliased coefficients $\Leftrightarrow$ parity check constraints of LDGM codes
		%\item \alert{Chinese Reminder Theorem (CRT)} guided sub-sampling
		\item Leverage connection to LDGM - Density Evolution to derive thresholds
		\item LDGM code is structured, but randomness in the non-zero coeffs suffices
	\end{itemize}
\end{block}


\begin{block}{Main Contributions}
	\begin{itemize}
		\item We show \alert{\textbf{FFAST}} is equivalent to \alert{\textbf{iterative decoding of product codes}}
		\begin{itemize}
			\item Provides an alternate way to derive and characterize thresholds for FFAST
			\item Proposed a generalized FFAST for better short length performance
			\item Use the connection to analyze FFAST under burst sparsity model
		\end{itemize}
	\end{itemize}
\end{block}
\end{frame}

%\pause
%
%\begin{block}{Main Idea}
%\begin{itemize}
%\item \alert{Sub-sampling} in time corresponds to \alert{aliasing} in frequency
%%\item Spectrum can be recovered from multiple versions of aliased spectrum if a  \alert{one-to-one} mapping exists
%\item Aliased coefficients $\Leftrightarrow$ parity check constraints of LDPC codes
%%\item \alert{Chinese Reminder Theorem (CRT)} guided sub-sampling
%\item Leverage connection to LDPC - Density Evolution to derive thresholds
%\item Randomness in the non-zero coeffs suffices for analysis
%\end{itemize}
%\end{block}

%\pause
%
%\begin{block}{In this talk}
%\begin{itemize}
%\item We show \alert{\textbf{FFAST}} is equivalent to \alert{\textbf{iterative decoding of product codes}}
%    \begin{itemize}
%    \item Provides an alternate way to derive and characterize thresholds for FFAST
%    \item Use the connection to analyze FFAST under burst sparsity model
%    \end{itemize}
%\end{itemize}
%\end{block}


\begin{comment}

\begin{frame} \frametitle{Product codes and FFAST ($d=2$)}
\begin{itemize}
\item \alert{$X$}: \ $K$-sparse spectrum of length $N = P_1 P_2$  ($P_1$ and $P_2$ are co-prime)
\item \alert{$X'$}: \  $P_1 \times P_2$ matrix formed by rearranging $X$ according to mapping $\mathcal{M}$

\end{itemize}

\begin{columns}

\column{.6\textwidth}
\begin{block}{}
\scriptsize{
\begin{eqnarray}\nonumber
{\color{blue}X_s[l_1]} & = & \sum_{i=0}^{P_2-1} X[l_1+iP_1], \ \ \ 0 \leq l_1 \leq P_2-1  \label{eqn:Xs_equation} \\ \nonumber
{\color{blue}Z_s[l_2]} & = & \sum_{i=0}^{P_1-1} X[l_2+iP_2], \ \ \  0 \leq l_2 \leq P_1-1  \label{eqn:Zs} \nonumber
\end{eqnarray}
}

\end{block}

\begin{block}{Mapping}
% There exists a mapping from $X$ to $X'$, $\mathcal{M}:\{0,1,\dots,P_1-1\}\times\{0,1,\dots,P_2-1\} \xrightarrow{} \{0,1,\dots,P-1\}$, which relates $X'(i,j)$ with $X(r+1)$, where $r=\mathcal{M}(i,j)$ is given by,
%%\begin{equation*}
%\begin{align*}
%\color{blue} \mathcal{M}(i,j) &\color{blue} \equiv (j-i)bP_2 + i \quad(\text{mod $N$})\\
%         & \color{blue} \equiv (i-j)aP_1 + j \quad(\text{mod $N$})
%\end{align*}
%%\end{equation*}
%where $a$ and $b$ are integers such that $aP_1+bP_2=1$.

The mapping from $X(r)$ to $X'(i,j)$ is given by
\[\color{blue}
(i,j) = \mathcal{M}(r) \equiv (r \mod P_2, \ r \mod P_1).
\]
\alert{Note:} CRT ensures that $\mathcal{M}$ is \textcolor{blue}{bijective}
\end{block}


\column{.4\textwidth}

\begin{figure}[t]
\centering
\includegraphics[width=1.8in]{ProductCodesMatrix}

\end{figure}


\end{columns}
\end{frame}

\begin{frame}\frametitle{Product codes and FFAST ($d \geq 3$)}
\color{blue}$N= P_1 \times P_2 \times \ldots \times P_d$

\[\color{blue} (i_1,i_2,\ldots, i_d) = \mathcal{M}(r) \equiv (r \mod f_1, r \mod f_2, \ldots, r \mod f_d ).
\]

\begin{columns}
\column{.45\textwidth}
\begin{block}{Less-sparse regime}
\color{red} $f_i= N/P_i,\ i=1,2, \ldots,d$\\ \vspace{0.2in}
\vspace{-2mm}
\color{blue} $\underline{d=3}$
\vspace{-3mm}
\begin{figure}[t]
\centering
\includegraphics[width=2.2in]{less-sparse}
\end{figure}

\end{block}
\column{.45\textwidth}
\begin{block}{Very-sparse regime}
\color{red} $f_i= P_i,\ i=1,2, \ldots,d$\\ \vspace{0.2in}
\vspace{-2mm}
\color{blue} $\underline{d=3}$
\vspace{-3mm}
\begin{figure}[t]
\centering
\includegraphics[width=2.2in]{very-sparse}
\end{figure}

\end{block}
\end{columns}

\end{frame}

\end{comment}

\begin{frame}\frametitle{Connections between FFAST and Product Codes}
\begin{columns}
\column{0.50\textwidth}
\begin{figure}[t]
\centering
\includegraphics[width=2.2in]{FFAST_2stages_generalized}
\end{figure}
\vspace{-6mm}
\hspace{-1.5in}
\column{0.50\textwidth}

\begin{figure}[t]
\includegraphics[width=2.4in]{less-sparse}
\end{figure}
\end{columns}

\begin{tabular}{ccc}
FFAST & $\Leftrightarrow$ & Product codes \\
$d$ stages & $\Leftrightarrow$ & $d$-dimensional product code \\
$2t$ branches & $\Leftrightarrow$ & $t$-error correcting RS component codes \\
Non-zero coefficients & $\Leftrightarrow$ & Error locations \\
Recovery of coefficients & $\Leftrightarrow$ & Iterative decoding
\end{tabular}

%\begin{figure}[t]
%    \centering
%    \includegraphics[width=3.5in]{ FFAST_2stages_generalized}
%\end{figure}

\end{frame}
%---------------------------------------------------------------------------------------
\begin{frame}\frametitle{Connection between FFAST and Product Codes}
\begin{itemize}
\item Alternative characterization of thresholds
\item Insight into improving finite length performance
\item Bursty non-zero coefficients model can be analyzed using stopping sets
\item Applications in designing interference tolerant A/D converters
%\item New designs for very high rate product codes
\end{itemize}
\end{frame}
%-----------------------------------------------------------------------------------------
% DE Proof

\begin{comment}

\begin{frame}\frametitle{Density Evolution(DE) for Product codes - Justesen et al}

\begin{columns}
\column{0.72\textwidth}

\begin{block}{Assumptions}

\begin{itemize}
\item $P_1=P_2= \ldots =P_d = P$
\item Errors are \alert{randomly distributed} in rows and columns
\item If $P \gg t$, \alert{\# errors} in each row/col $\sim$ \alert{Poisson}($M$))
\end{itemize}
\end{block}
\pause
\begin{block}{Main Idea}
\begin{itemize}
%\item Random \alert{bipartite graph} - row and column codes
\item Removal of \alert{corrected vertices} (degree$\leq t$) from row codes $\Leftrightarrow$ removal of random edges from column codes
\item \# of errors in row/column changes after each iter
\begin{itemize}
\item Track the distribution
%\item Changes the Poisson parameter ($m(j)$)
%\item \alert{Threshold} - max. $M$ such that $m(j) \rightarrow 0$ as $j \rightarrow \infty$
\end{itemize}
%\item Generalize for $d \geq 2$
\end{itemize}
\end{block}
\column{0.25\textwidth}


\begin{figure}[t]
\centering
\includegraphics[width=1.3in]{Bipartite_graph}
\end{figure}

\end{columns}
\end{frame}

\begin{frame} \frametitle{DE continued}
\begin{block}{Tail of the Poisson distribution}
\begin{equation}\nonumber
\pi_t(m) = \sum_{j \geq t} \mathrm{e}^{-m}m^j/j!
\label{eqn:defpi}
\end{equation}
\end{block}

\begin{block}{Effect of first step of decoding}
If the \# errors is poisson with mean $M$, Mean \# of errors after decoding is
\begin{equation}\nonumber
\textcolor{blue}{m(1)} = \sum_{j \geq t+1} j\mathrm{e}^{-M}M^j/j! = M\pi_t(M)
\label{eqn:defpi}
\end{equation}
\end{block}

\end{frame}
%\begin{frame} \frametitle{DE cont.}
%\begin{block}{Evolution of degree distribution ($d=2$)}
%
%
%\begin{equation}\nonumber
%\pi(m) = \sum_{j \geq t} \mathrm{e}^{-m}m^j/j!
%\label{eqn:defpi}
%\end{equation}
%\alert{Initial}: Number of errors ($W$) $\rightarrow$ Poisson with mean $M$ ; \textcolor{blue}{$W = P M$} \\
%\alert{Iter.'1'}: \textcolor{blue}{Remove light vertices from rows}
%
%\begin{itemize}
%\item Columns follow Poisson distribution with mean $m(1)$
%\item Mean \# of errors after decoding $=$
%\vspace{-8mm}
%\begin{equation}\nonumber\cite{\cite{}}
%\hspace{48mm}
%\sum_{j \geq t+1} j\mathrm{e}^{-M}M^j/j! = M\pi(M)= \textcolor{blue}{m(1)}
%\label{eqn:defpi}
%\end{equation}
%\end{itemize}
%\vspace{-2mm}
%\alert{Iter.'j'}: \textcolor{blue}{Remove random edges and light vertices from rows}
%\begin{itemize}
%\item Follows Truncated Poisson distribution with parameter $m(j)$
%\item \textcolor{blue}{ $m(j)= M \ \pi(m(j-1))$} (Proof by induction)
%\end{itemize}
%
%\end{block}
%\begin{block}{d-stages}
%\vspace{-2mm}
%{\color{blue}
%\begin{center}
%$m(j) = M\prod\limits_{i=1}^{d-1} \pi(m(j-i))\quad\text{if $j\geq d$}$
%\end{center}
%}
%
%\end{block}
%\end{frame}

\begin{frame} \frametitle{Evolution of degree distribution($d=2$) - first iteration}
\begin{columns}

\column{0.5\textwidth}
{\vspace{-6mm}
\hspace{6mm}
\begin{block}{Before row decoding}
{\color{blue}Distribution}: Poisson($M$) \\
{\color{blue}Mean}: $M$
\end{block}}

\begin{block}{After row decoding}
{\color{blue}Distribution}: Truncated Poisson($M$) \\
{\color{blue}Mean}: $M \pi_t(M) = m(1)$
\end{block}

\begin{block}{Before column decoding}
{\color{blue}Distribution}: Poisson($m(1)$) \\
{\color{blue}Mean}: $m(1)$
\end{block}

\begin{block}{After column decoding}
{\color{blue}Distribution}: Truncated Poisson($m(1)$) \\
%{\color{blue}Mean}: $m(2) = M \pi_t(m(1))$
\end{block}


\column{0.5\textwidth}
\begin{center}
\vspace{-3mm}
\input{poisson_row_before_1.tex}
\input{poisson_row_after_1.tex}
\input{poisson_col_before_1.tex}
\input{poisson_col_after_1.tex}
\end{center}

\end{columns}
\end{frame}

\begin{frame} \frametitle{Evolution of degree distribution - $j$th iteration}
\begin{columns}

\column{0.55\textwidth}
{\vspace{-6mm}
\hspace{6mm}
\begin{block}{Before row decoding}
{\color{blue}Distribution}: Poisson($m(j)$) \\
\end{block}}
\vspace{6mm}
\begin{block}{After row decoding}
{\color{blue}Distribution}: Truncated Poisson($m(j)$) \\
{\color{blue}Mean}: $m(j) \pi_t(m(j))$ \\
{\color{blue}Reduction by a factor}: $\frac{m(j) \pi_t(m(j))}{m(j-1) \pi_t(m(j-1))}$ \\
\end{block}
\column{0.45\textwidth}
\begin{center}
\vspace{-3mm}
\input{poisson_row_before_1.tex}
\input{poisson_row_after_1.tex}
\end{center}
\end{columns}


\begin{block}{d-stages}

\begin{center}
\begin{itemize}
\item  $m(j)= M \ \prod \limits_{i=1}^{d-1}{\pi_t(m(j-i))}$
\item $\frac{m(j)}{m(j-d)}=\frac{M\prod\limits_{i=1}^{d-1} \pi_t(m(j-i))}{m(j-d)} \leq M \frac{\pi_t^{d-1}(m(j-d))}{m(j-d)}$
\end{itemize}
\end{center}

\end{block}
\end{frame}


\end{comment}

\begin{frame}\frametitle{Thresholds}
%------------------------------------------------------------
%\begin{block} {$k$-core}
%\color{blue} A $k$-core in a graph is a sub-graph with all the vertices having degree at least $k$.
%
%\begin{itemize}
%\item Iterative decoder fails if the error graph contains a $t+1$-core
%\begin{itemize}
%\item $\alert{G}$ : Random graph with $n$ vertices and $w$ edges
%\item With high probability, a $k-core$ exists in $G$ when \color{blue}$w > c_k n/2$.
%\small
%\begin{equation}\nonumber
%\centering
%\sigma(j)= e^{-\lambda}{\lambda^{j}/j!} \ \ \ \ \
%\pi_{k}(\lambda)= \sum_{j\geq k-1}{\sigma(j)} \ \ \ \ \
%c_k = min_{\lambda}[\lambda/\pi_{k}(\lambda)], \lambda > 0
%\end{equation}
%\end{itemize}
%\end{itemize}
%\end{block}
%-------------------------------------------------------------
\begin{theorem}\label{thm:thresh}
Less sparse case: In the limit of large $P$, the FFAST algorithm with $d$ branches and $2t$ stages can recover the FFT coefficients w.h.p if $K < \frac{2dt}{c_{d,t}}$. \\
\vspace{2mm}
\centering
\color{blue} $c_{d,t} = \min_m \{ m / \pi^{d-1}(m)\} $
\end{theorem}
%\begin{itemize}
%\item  $m(j)= M \ \prod \limits_{i=1}^{d-1}{\pi(m(j-i))}$
%\item $\frac{m(j)}{m(j-d)}=\frac{M\prod\limits_{i=1}^{d-1} \pi(m(j-i))}{m(j-d)} \leq M \frac{\pi^{d-1}(m(j-d))}{m(j-d)}$
%\end{itemize}
\begin{block}{}

\begin{center}
\alert{Threshold} = $ \frac{\# \ of \ measurements}{recoverable \ sparsity}={\color{blue} \frac{2dt}{c_{d,t}}}$
\end{center}

\vspace{-6mm}
\color{black}
\begin{table}[ht]
\centering
\begin{tabular}{c|ccccccc}
\hline
& $d=2$ & $d=3$ & $d=4$ & $d=5$ & $d=6$ & $d=7$ & $d=8$ \\
\hline
\rowcolor{lightgray}
$t=1$& \ & 2.4436 & 2.5897 & 2.8499 & 3.1393 & 3.4378 & 3.7383 \\
$t=2$& 2.3874 & 2.5759 & 2.9993 & 3.4549 & 3.9153 & 4.3736 & 4.8278 \\
\rowcolor{lightgray}
$t=3$& 2.3304 & 2.7593 & 3.3133 & 3.8817 & 4.4483 & 5.0094 & 5.5641 \\
$t=4$& 2.3532 & 2.9125 & 3.5556& 4.2043 & 4.8468 & 5.4802 & 6.1033 \\
%\rowcolor{lightgray}
%$t=5$& 2.3908 & 3.0394 & 3.7471 & 4.4500 & 5.1362 & 5.8018 & 6.4451 \\

\hline
\end{tabular}
\end{table}
\vspace{-3mm}

Notice that $L,K = O \left( N^{\frac{1-d}{d}}\right)$
\end{block}

\end{frame}


%Bursty Signals

\begin{frame}\frametitle{Bursty signals}
\begin{block}{Bursty Signals}
Signals that have non-zero Fourier coefficients occurring in bursts. Let $b$ be the number of bursts.
\end{block}

\begin{figure}[t]
\centering
\includegraphics[width=3.0in]{burst_spacing}
\end{figure}

\end{frame}

\begin{comment}

\begin{frame}\frametitle{One burst case $b=1$}

\begin{theorem}
\color{blue} For any finite $N$ and $d=2$, $b=1$, any burst of length $K \leq P_1+P_2-1$ is guaranteed to be recoverable

\begin{columns}
\column{.35\textwidth}

\begin{figure}[t]
\centering
\includegraphics[width=1.9in]{1burstmatrix}
\end{figure}

\column{.57\textwidth}
\begin{proof}
\begin{itemize}


\item WLOG, assume the burst starts at $(0,0)$ (due to cyclic shift property of DFT)
\item Paths \alert{1} and \alert{2} cover all columns once.
\item Paths \alert{3} and \alert{4} cover all rows once
\item So minimum burst length leading to stopping set (\alert{1 + 2 + 3 + 4})= $P_1 + P_2$
\end{itemize}

\end{proof}
\end{columns}
\end{theorem}
\end{frame}

\end{comment}

%\begin{frame}\frametitle{Bursty signals ($b=2$) cont.}
%
%\begin{block}{Notations}
%\begin{itemize}
%\item $\alert{X}$: Sparse bursty signal with two bursts($\mathcal{B}_1$ and $\mathcal{B}_2$)  of lengths $k_1$ and $k_2$
%\item $\alert{X'}$: Product code structure of $X$, as given by the inverse mapping $\mathcal{M}^{-1}$.
%\item $\alert{s}$: Spacing between the bursts.
%\item \alert{$\mathcal{S}_{2n}$} : Stopping set of size $2n$ observed when decoding using the peeling decoder.
%\item $\alert{K_n}$: The smallest value of $k_1$ or $k_2$ such that a stopping set of size $2n$ occurs (i.e., for $k_1,k_2 < K_n$, $\mathcal{S}_{2n}$ never occurs)
%
%%\end{itemize}
%
%%\end{block}
%
%%\begin{block}{Stopping set analysis}
%%\begin{itemize}
%%\item WLOG, consider $\mathcal{B}_1$ starting at (0,0) in $X'$ (Cyclic property of DFT)
%%\item For $k_1, k_2$ $\leq$ min($P_1$,$P_2$), the points in $\mathcal{B}_1$ would be $(a_1,a_1), (a_2,a_2),\dots, (a_n,a_n)$.
%%\item Since $\mathcal{B}_1$ and $\mathcal{B}_2$ form a stopping set, the sequential points in $\mathcal{B}_2$ will be $(a_1,b_1), (a_2,b_2),\dots, (a_n,b_n)$, where $(b_1,b_2,\dots,b_n) = \phi_j(a_1,a_2,\dots,a_n)$ $\forall j < n!$ such that $b_i \neq a_i \forall i \in \{1,2,\dots,n\}$ ($\phi_j$ denotes the $j$th permutation).
%
%\end{itemize}
%\end{block}
%
%
%\end{frame}

%\begin{frame} \frametitle{Bursty signals (b=2) cont.}
%
% \begin{theorem}\label{thm2}
% \color{blue} An $\mathcal{S}_{2n}$ occurs first time in $X'$ for $k_1=k_2=K_n$ when $k_1$ and $k_2$ are increased, individually, from 1 to $P_2$ (for $P_2<P_1$), if there exists a solution for the set of equations(\ref{eqn:congruence}) in $\{a_1,a_2,\dots,a_n\}<P_2$ for some permutation $\phi_j$.
%
%\color{black}
% \begin{align}
% b_2 - b_1 \text{  (mod $P_1$)} &\equiv a_2 - a_1 \text{  (mod $P_2$)} \nonumber \\
% b_3 - b_2 \text{  (mod $P_1$)} &\equiv a_3 - a_2 \text{  (mod $P_2$)} \nonumber \\
%  								&\vdots  \label{eqn:congruence}\\
% b_n - b_{n-1} \text{  (mod $P_1$)} &\equiv a_n - a_{n-1} \text{  (mod $P_2$)} \nonumber
% \end{align}
% \end{theorem}
%
% \begin{corollary}\label{coro1}
%\color{blue} An $\mathcal{S}_{2n}$ occurs in $X'$ for $k_1,k_2\geq K_n$ if there exists a permutation $\phi_j$ such that the set of equations (\ref{eqn:congruence}) have solutions for $(a_1,a_2,\dots,a_n)$ for some $K_n$.
%\end{corollary}
%\end{frame}
%
%\begin{frame} \frametitle{Bursty signals (b=2) cont.}
%\begin{figure}[t]
%    \centering
%    \includegraphics[width=2.8in]{ 2burstmatrix}
%\end{figure}
%
%
% \begin{lemma}\label{lemma1}
%\color{blue} For any finite $N = P_1 P_2 \ (P_1 > P_2)$, $d=2$ and $b=2$, there exist no stopping set of size $2n$ for those $s$ values that create only one wraparound in bursts, if the length of the bursts is $l \leq  \frac{(n-1)}{n}P_2$, where $n$ is a factor of $P_2$.
%\end{lemma}
%
%\end{frame}

\begin{comment}
\begin{frame} \frametitle{Bursty signals (b=2) cont.}

\begin{theorem}
\color{blue} For any finite $N = P_1 P_2 \ (P_1 > P_2)$, $d=2$ and $b=2$, all bursts of length $l \leq P_2/2$ are recoverable.
\end{theorem}

\begin{block}{Conjecture}
\color{blue} As $N \rightarrow \infty$, for $d=2$, $b=2$, the fraction of bursts of length $K \leq P_1+P_2-1$ that are not recoverable vanishes as $N \rightarrow \infty$.
\end{block}

\begin{block}{}
This means that a threshold exists even for $d=2$ and $t=1$ in the bursty case
\end{block}
\end{frame}
%-------------------------------------------------------------------------------------------------
\begin{frame}
\frametitle{Interference-tolerant A/D Converter}
\begin{figure}[t]
\centering
\includegraphics[width=3.5 in]{spectrumsensing}
\end{figure}
\begin{figure}[t]
\centering
\includegraphics[width=3.5 in]{systemdiagram}
\end{figure}
\end{frame}
\end{comment}

%--------------------------------------------------------------------------------------------------
\begin{frame}
\frametitle{Simulation results}
\begin{figure}[t]
\centering
\includegraphics[width=3.5 in]{Average_Perror_K_250_251_semilog.pdf}
\vspace{-4mm}
\caption{Plot of the average probability of error , $P_e$, as a function of the sparsity value, $K$, for bursty and random selection of non-zero coefficients; $N=250\times251$, 1002 samples}
\label{fig:probofsuccess250_251}
\end{figure}

\end{frame}
%--------------------------------------------------------------------------------------------------
\begin{frame}
\frametitle{Simulation results for $N \approx 10000$}
\begin{figure}[t]
\centering
\includegraphics[width=3.5 in]{Finite_N_analysis_block}
\end{figure}

\end{frame}
%-----------------References --------------------
%\begin{frame}[allowframebreaks]
%\frametitle{References}%in case more than 1 slide needed
%	{\footnotesize
%		\bibliographystyle{ieeetr}
%		\bibliography{sparseestimation}	}
%\end{frame}

\subsection{Hadamard Transform}


%\begin{frame}{Sub-linear Time Sparse Walsh Hadamard Transform}
%\begin{block}{Problem Statement}
%	
%
%	
%\end{block}
%\end{frame}

\begin{frame}{Sparse Walsh Hadamard Transform (SWHT) Computation}
\vspace{-5pt}
\begin{block}{Problem Statement}
	\vspace{4pt} 	
	\centering	
	
	{$x[\mv]  \xrightarrow{\text{WHT}}  \underset{\color{blue}(K-\text{sparse})}{X[\kv]}$ \\
	
	\[X[\kv] = \frac{1}{\sqrt{N}}\sum_{\mv \in \F_2^n} (-1)^{<\mv,\kv>} x[\mv], ~~ \kv \in \F_2^n \]	
}
	\vspace{10pt}
	Compute the {\alert{locations}} and \alert{values} of the $K$ non-zero coefficients w.h.p
\end{block}

\pause

 \begin{block}{Main contribution}
		\begin{itemize} \itemsep1pt \parskip0pt \parsep0pt
			\item Scheibler'15 proposed a fast WHT framework
				\begin{itemize}
					\item[-] Sample complexity: $O(K \log_2 (\frac{N}{K}))$
					\item[-] Time complexity:  $O(K \log_2(K) \log_2(\frac{N}{K}))$
				\end{itemize}
			\item {\color{blue} 3x improvement} in sample and computational complexity.
				\begin{itemize}
					\item[-] Better sensing matrix design
					\item[-] New decoding algorithm that can determine {\color{blue} 2-sparse vectors} in each bin
				\end{itemize}
		\end{itemize} 	          			
\end{block}
\end{frame}

\begin{frame}{Results}

 \begin{table}
	\centering
	\caption{Comparison of thresholds for the scheme in Scheibler'15 and the proposed scheme.}
	\label{table:thresholds}
	\begin{tabular}{|c|c|c|c|c|c|}
		\hline
		$d$ & $2$ & $3$ & $4$ & $5$ & $6$ \\
		\hline
		% after \\: \hline or \cline{col1-col2} \cline{col3-col4} ...
		$\eta_1$ & 2.0000 &  1.2218 &   1.2949 &  1.4249  &  1.5697 \\
		\hline
		$\eta_2$ & 0.5968 &  0.6440 &  0.7498  &  0.8637  &  0.9788 \\
		\hline
		$\eta_2/\eta_1$ &    0.2984 &   0.5271  &  0.5791  &  0.6061  &  0.6236 \\
		\hline
	\end{tabular}
\end{table}

\begin{block}{Thresholds}
	\begin{equation*}
	\eta_t(d) = \frac{\text{No. of measurements}}{\text{Recoverable sparsity} (K)}  \frac{M_b(d)}{K} = \frac{d}{c_{d,t}}
	\end{equation*}
	
\end{block}

\end{frame}

\begin{frame}{Results}

\begin{center}
	\begin{figure}[h!]
		\resizebox{0.60\textwidth}{!}{\input{noiseless_sims_d_2.tex}}
		\caption{Plots of probability of error in recovering the non-zero coefficients vs. Number of non-zero coefficients $K$.} \label{Fig:Simulation Results}
	\end{figure}	
\end{center}

\end{frame}

\begin{frame}{Results}

		\begin{figure}[h!]
	
	\resizebox{0.60\textwidth}{!}{\input{noiseless_sims_d_3.tex}}
	
	\caption{Plots of probability of error in recovering the non-zero coefficients vs. Number of non-zero coefficients $K$. The chosen parameters are $N = 2^{15}, d=3, b=10,$ $M_1 = M_2 \approx 46000$.} \label{Fig:Simulation Results}.
\end{figure}

\end{frame}


%---------------------------------------------------------------------------------------
\begin{frame}\frametitle{Questions?}
	\begin{figure}[t]
		\centering
		\includegraphics[width=2.8in]{questions}
	\end{figure}
	\centering
	\color{blue}
	\Huge{Thank you!}
\end{frame}

\end{document} 