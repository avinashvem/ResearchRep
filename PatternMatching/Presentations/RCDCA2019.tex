\documentclass[10pt,xcolor=table]{beamer}
%\usetheme{warsaw}
\usetheme{CambridgeUS}
\usepackage{etex}
\usepackage[english]{babel}
\usepackage{amstext,amsthm,amsmath,amsfonts,latexsym,graphicx,amssymb,epsfig,epsf,psfrag,mathtools}
\usepackage{pgfpages,xcolor,subfigure,pstricks}
\usepackage{pgfplots}
\usepackage{url}
\usepackage{tikz,tikz-cd}
\pgfplotsset{compat=newest}
%% the following commands are sometimes needed
%\usetikzlibrary{plotmarks}
\usetikzlibrary{arrows,backgrounds,plotmarks,decorations.pathmorphing,decorations.footprints,fadings,calc,trees,mindmap,shadows,decorations.text,patterns,positioning,shapes,matrix,fit}
\input{graphical_settings}		
\usepackage{grffile}
\usepackage{fancyhdr}
\usepackage{pst-all}

\graphicspath{{./../figures/}}
\makeatletter
\def\input@path{{./../figures/}}
\makeatother

\usepackage{tikz,pgfplots}
\pgfplotsset{compat=newest}
\pgfplotsset{plot coordinates/math parser=false}
\usetikzlibrary{shapes.geometric, arrows}
\usetikzlibrary{decorations.markings}
\tikzstyle{startstop} = [rectangle, rounded corners, minimum width=1.5cm, minimum height=0.5cm,text centered, draw=black, fill=red!30]
\tikzstyle{io} = [trapezium, trapezium left angle=70, trapezium right angle=110, minimum width=1.5cm, minimum height=0.5cm, text centered, draw=black, fill=blue!30]
\tikzstyle{process} = [rectangle, minimum width=1.5cm, minimum height=0.5cm, text centered, draw=black, fill=orange!30]
\tikzstyle{decision} = [diamond, minimum width=1.5cm, minimum height=0.5cm, text centered, draw=black, fill=green!30]
\tikzstyle{arrow} = [thick,->,>=stealth]


%\hypersetup{pdfpagemode=FullScreen}
\usefonttheme{professionalfonts}
\setbeamertemplate{footline}[frame number]
\setbeamertemplate{theorem}[ams style]
\setbeamertemplate{theorems}[numbered]
\definecolor{purple}{RGB}{255,0,204}

\newcommand{\defeq}{\triangleq}
\newcommand{\Pp}{\mathbb{P}}
\newcommand{\Gm}{\mathbf{G}}
\newcommand{\Hm}{\mathbf{H}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\Zp}{\mathbb{Z}_{+}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\Rp}{\R_{+}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\F}{\mathbb{F}}
\newcommand{\Zw}{\mathbb{Z}[\omega]}
\newcommand{\Zi}{\mathbb{Z}[i]}
\newcommand{\mc}{\mathcal}
\newcommand{\mbb}{\mathbb}

\newcommand{\wv}{\underline{w}}
\newcommand{\cv}{\underline{c}}
\newcommand{\ev}{\underline{e}}
\newcommand{\sv}{\underline{s}}
\newcommand{\xv}{\underline{x}}
\newcommand{\bv}{\underline{b}}
\newcommand{\kv}{\underline{k}}
\newcommand{\zv}{\underline{z}}
\newcommand{\yv}{\underline{y}}
\newcommand{\hv}{\underline{h}}
\newcommand{\rv}{\underline{r}}

\newcommand{\Yv}{\underline{Y}}
\newcommand{\Xv}{\underline{X}}
\newcommand{\Rv}{\underline{R}}
\newcommand{\mfk}[1]{\mathfrak{#1}}


\newcommand{\RNum}[1]{\uppercase\expandafter{\romannumeral #1\relax}}

\usecolortheme[RGB={0,100,0}]{structure}
\setbeamertemplate{itemize item}[circle]
\setbeamertemplate{itemize subitem}[rectangle]
\setbeamertemplate{itemize subsubitem}{$-$}
\setbeamertemplate{blocks}[rounded][shadow=true]
\setbeamertemplate{navigation symbols}{} % get rid of navigation symbols
%\setbeamertemplate{footline}[page number]
%\logo{\includegraphics[height=1cm,keepaspectratio]{greentouch.eps}}

\definecolor{DarkFern}{HTML}{407428}
\definecolor{DarkCharcoal}{HTML}{4D4944}
\colorlet{Fern}{DarkFern!85!white}
\colorlet{Charcoal}{DarkCharcoal!85!white}
\colorlet{LightCharcoal}{Charcoal!50!white}
\colorlet{DarkRed}{red!70!black}
\colorlet{AlertColor}{DarkRed!80!black}
\colorlet{DarkBlue}{blue!70!black}
\colorlet{DarkGreen}{green!70!black}
% Use the colors:
\setbeamercolor{title}{fg=DarkRed}
\setbeamercolor{frametitle}{fg=DarkRed}
\setbeamercolor{normal text}{fg=black}
\setbeamercolor{block title}{fg=black,bg=DarkBlue!35!white}
\setbeamercolor{block body}{fg=black,bg=DarkBlue!15!white}
%\setbeamercolor{block title}{fg=black,bg=Fern!25!white}
%\setbeamercolor{block body}{fg=black,bg=Fern!25!white}
\setbeamercolor{alerted text}{fg=AlertColor}
\setbeamercolor{itemize item}{fg=Charcoal}

\usepackage[backend=bibtex,style=authoryear]{biblatex}

\usepackage{chemarrow}	

\addbibresource{IEEEabrv.bib}
\addbibresource{sparseestimation.bib}




\def\fig_path{../Figures}
\begin{document}
\title{Sub-string/Pattern Matching in Sub-linear Time Using a Sparse Fourier Transform Approach}
\author{ Krishna R. Narayanan \\
Joint work with Nagaraj T. Janakiraman, Avinash Vem, J.-F. Chamberland \\
}
\titlegraphic{
\includegraphics[width=0.75in]{TAMULogoBox}
}
\institute{Department of Electrical and Computer Engineering \\ Texas A\&M University}
\date{}
\frame{\titlepage}
%-----------------------------------------------------------------------------------------------
\begin{frame}\frametitle{Problem statement}
 	\vspace{-0.4cm}
	\begin{figure}[t]
		\centering
		\includegraphics[width=2.8in]{Pattern_matching_ex.pdf}
	\end{figure}
	\vspace{-10pt}
	\begin{block}{}
\begin{itemize}\itemsep5pt
	\item {\color{blue} Database/String}: $\xv = [x[0], x[1], \cdots, x[N-1]]$ \ (length $N$)
	\item { \color{blue} Query/Substring}: $\yv = [y[0], y[1], \cdots, y[M-1]]$ \ (length $M$)
    \item Determine ({\color{blue}with high probability}) the locations where $\yv$ matches with $\xv$ 
\end{itemize}
\end{block}

\vspace{-3pt}
\only<2>{\begin{block}{}
	\begin{enumerate}
		\item \alert{Exact Matching}:~  $\yv$ appears {\color{blue}exactly} in $\xv$
		\begin{itemize}
				\item [-]  $\yv := \xv[\tau:\tau+M-1]$
		\end{itemize}
        \pause
		\item \alert{Approximate Matching:} ~ $\yv$ is a {\color{blue}noisy substring} of $\xv$
		\begin{itemize}\itemsep3pt
				\item [-] $\yv := \xv[\tau:\tau+M-1] \odot \bv$
				\item [-] $\bv$ is a noise sequence with $d_H(\yv,\xv[\tau:\tau+M-1]) \leq K$
		\end{itemize}
	\end{enumerate}
	\end{block}
}
\only<3>{
	\begin{block}{Performance measures}
		\begin{itemize}
			\item \alert{Storage/Sketching complexity} - space in bits to store the sketch of database
            \item \alert{Computational complexity} - operations to find the matching locations
		\end{itemize}
	\end{block}

}
\only<4>{
\begin{block}{Signal model}
		\begin{itemize}
			\item When $\yv$ does not match with $\xv$, $\xv[\tau:\tau+M-1]$ and $\yv$ are uncorrelated
            \item To keep things simple assume $\xv$ is a i.i.d. sequence of $\pm 1$s
		\end{itemize}
\end{block}
}
\end{frame}
%%-------------------------------------------------------------------------------------------------------------

%\begin{frame} \frametitle{Sketching Model}
%		\vspace{-10pt}
%	\begin{figure}
%		\includegraphics[width=2.25in]{SketchingModel.pdf}
%	\end{figure}
%	\vspace{-8pt}
%	\begin{block}{Sketch}
%		\begin{itemize}
%			\item Succinct fingerprint (compressed form) of the data - min communication cost
%			\item Allows Pattern Matching without decompression
%			\item Could be both lossless or lossy (probabilistic recovery)
%			\item \alert{Sketching Complexity} - space in bits to store the sketch of database
%            \item \alert{Computational Complexity} - operations to find the matching locations
%		\end{itemize}
%	\end{block}
%\end{frame}
% ----------------------------------------Notations---------------------------------------

\begin{frame}\frametitle{Notation}

	\begin{figure}[t]
		\centering
		\includegraphics[width=2.5in]{Pattern_matching_ex.pdf}
	\end{figure}
	\vspace{-8pt}
	{\small
	\begin{table}[h!]
		\label{Table:Notations3}
		\begin{center}
			\begin{tabular}{|c|c|} 	
				\hline		
				\textit{Symbol}		&  \textit{Meaning} \\		
				\hline
				$N$           		& Size of the string or database in symbols \\
				\hline
				$M = N^{\mu}$       & Length of the query in symbols \\
				\hline
				$L = N^\lambda$    &   Number of matches \\
				\hline
				$K$             &$\max_{\tau}d_{H}(\xv[\tau:\tau+M-1],\yv)$\\
				\hline
				$\eta$             &$\frac{K}{M}$\\
				\hline
				%$G = N^\gamma$    & Number of blocks \\
				%\hline
				%$\tilde{N} = N^{1-\gamma}$   & Length of one block \\
				%\hline
				%$f_i = N^\alpha$     & Length of smaller point IDFT at each branch\\
				%\hline
				%$g_i = N/f_i$     	   &  Sub-sampling parameter \\
				%\hline
				%$B$   					    & Number of shifts also referred to as branches  \\
				%\hline
				%$d$           				& Number of stages in the FFAST algorithm \\
				%\hline
			\end{tabular}
		\end{center}
	\end{table}
    }
    \begin{block}{Probabilistic recovery}
    $\mbb{P}(\hat{\underline{\tau}} \neq \underline{\tau}) \rightarrow 0$ as $N \rightarrow \infty$
    \end{block}
	\end{frame}	
%%-------------------------------------------------------------------------------------------------------------
\begin{frame} \frametitle{Some prior work}
	\vspace{-0.2cm}
	 \begin{block}{Exact matching - Non-sketching versions}	 	
	 	\begin{itemize}
            \item  {Rabin-Karp}: Match a set of strings - $O(N)$ complexity
	 		\item  {Boyer'77}: First occurrence of the match (only $\tau_1$)	 		
	 		\begin{itemize}
	 			\item[-] Average complexity - $O(N^{1-\mu} \log N)$ (sub-linear)
	 			\item[-] Worst case complexity - $O(N \log N)$
	 		\end{itemize}	
	 	\end{itemize}
	 \end{block}
	
	 \begin{block}{Exact matching - Sketching versions - Bio informatics} 	
	 	\begin{itemize}
	 		\item  {Goodrich'05}: BWT, suffix-arrays based indexing
	 		\begin{itemize}
	 			\item[-] Time complexity - $O(M + L)$ (sub-linear)
	 			\item[-] Storage Complexity - $O(N~H_k(X) \log^\epsilon N) + o(N)$ bits  (linear)
	 			\item[-] Read alignment in Bio-informatics community[ {\color{blue}Li'09,Li'10}]
	 		\end{itemize}	 		
	 	\end{itemize}
	 \end{block}
\end{frame}
%-------------------------------------------------------------------------------------------------------------
\begin{frame} \frametitle{Some prior work}
	\vspace{-0.2cm}
	\begin{block}{\alert{Approximate Matching}}
		
	 	\begin{itemize}
	 		\item {Chang'94}: Generalization of Boyer'77
	 		\begin{itemize}
	 			\item[-] Average time complexity - $O(NK/M \log N)$ (sub-linear only when $K \ll M$ )
	 		\end{itemize}
	 		
	 		\item {Zhang'03}: Approximate Matching using BWT
	 			\begin{itemize}
	 				\item[-] \textcolor{blue}{Worst case time complexity: $O(\min\{M(M-K){|\mc{A}|}^K\log \frac{N}{|\mc{A}|} , NM \log \frac{N}{|\mc{A}|}\})$}
	 				\item[-] \textcolor{blue}{Complexity grows with $|\mc{A}|$ and $K$}
	 			\end{itemize}
	 		\pause	
	 		\item {Andoni, Hassanieh, Indyk and Katabi'13}: Sub-linear for $K = O(M)$
	 		\begin{itemize}
                \item[-] \textcolor[rgb]{0.00,0.00,1.00}{$O\left(N^{1-0.359\mu}\right)$ (sub-linear even when $K = O(M)$)}
	 			\item[-] Combinatorial in nature
	 		\end{itemize}
	 	\end{itemize}
	 	
	 \end{block}
\end{frame}
	

\begin{frame} \frametitle{Main result}
	
	{\small
		\begin{table}[h!]
			\begin{center}
				\begin{tabular}{|c|c|} 	
					\hline		
					\textit{Symbol}		&  \textit{Meaning} \\		
					\hline
					$N$           		& Size of the string or database in symbols \\
					\hline
					$M = N^{\mu}$       & Length of the query in symbols \\
					\hline
					$L = N^\lambda$    &   Number of matches \\
					\hline
					$K$             &$\max_{\tau}d_{H}(\xv[\tau:\tau+M-1],\yv)$\\
					\hline
					$\eta$             &$\frac{K}{M}$\\
					\hline
				\end{tabular}
			\end{center}
		\end{table}
	}	
	
	\vspace{-2.5mm}
	\begin{theorem}
		Assume that a sketch of $\xv$ can be precomputed and stored. Then for the {\it exact pattern matching} and {\it approximate pattern matching} (with $K = \eta M,~ 0 \leq \eta \leq 1/6$) problems, our algorithm has
		\begin{itemize}
		 \item	For $\mu \leq 0.5:$
		\begin{itemize}
			\item \alert{Sketching complexity:}
			{\color{blue} $O(\frac{N}{M}\log N)=O(N^{1-\mu}\log N)$} \alert{samples}
			\item \alert{Computational complexity:}
			{\color{blue}$O(\max(N^{1-\mu}\log^2 N, N^{\mu+\lambda}\log N ))$}
		\end{itemize}	
        \pause
		\item	For $\mu > 0.5$ and $\lambda<\mu-0.5$ :
		\begin{itemize}
			\item \alert{Sketching complexity:}
			{\color{blue} $O(\sqrt{N}\log N)$} samples
			\item \alert{Computational complexity:}
			{\color{blue}$O(\sqrt{N}\log^2 N)$}
		\end{itemize}	
        \pause				 		
		\item a decoder for which $\mbb{P}(\hat{{\mathcal{T}}} \neq \mathcal{T}) \rightarrow 0$ as $N \rightarrow \infty$
		\end{itemize}
	\end{theorem}
\end{frame}

%-------------------------------------------------------------------------------------------------------------
\begin{frame}\frametitle{Signal processing view}

	\begin{figure}[t]
		\centering
		\includegraphics[width=3.5in]{Pattern_matching_ex.pdf}
	\end{figure}

			\begin{block}{}			
				\begin{itemize}
					\item {Cross-correlation} ($\rv$): $\displaystyle{r[m]=(\xv*\yv)[m] \defeq \sum_{i=0}^{M-1} x[m+i] y[i] }$
			        \pause
					\item {Naive implementation}: $O(MN) = O(N^{1+\mu})$ ({\color{blue} super-linear} complexity)
                    \pause
					\item {Fourier Transform Approach}: 1970's - $O(N \log N)$ complexity
					\begin{equation}\nonumber
					\rv = \mathcal{F}_{N}^{-1} \{~  \mathcal{F}_{N}\{\xv\} ~ \odot ~  \mathcal{F}_{N}\{\yv'\} ~ \}, \ \ \yv' = \yv^{*}[-n]
					\end{equation}
					
				\end{itemize}
			\end{block}												
\end{frame}
%-------------------------------------------------------------------------------------------------------------
\begin{frame}\frametitle{Main idea}
	\vspace{-0.4cm}
			\begin{block}{}
			
				\begin{itemize}
					\item {Cross-correlation} ($\rv$):
					
                    \begin{eqnarray}
                    \nonumber
					r[m] & = & (\xv*\yv)[m] \defeq \sum_{i=0}^{M-1} x[m+i] y[i], ~ ~ \ 0 \leq m \leq N-1\\
                    \nonumber
					\rv  & = & \mathcal{F}_{N}^{-1} \{~  \mathcal{F}_{N}\{\xv\} ~ \odot ~  \mathcal{F}_{N}\{\yv'\} ~ \}, \ \ \yv' = \yv[-n]
					\end{eqnarray}					
				\end{itemize}
			\end{block}
			
				\begin{columns}
					\column{0.45\columnwidth}
			\begin{block}{\alert{ \bf Key Observation}}
			\vspace{0.2cm}
				\begin{itemize}
					\item $\rv$ is {\color{blue}Sparse} with some noise.
				\end{itemize}
				 \begin{equation} \label{eqn:RXY_sparse}\nonumber
				 r[m] \ = \left\{
				 \begin{array}{ll}
				 &M,~~  \text{if} \ m \in \mathcal{T} \\
				 & n_m,~~ m \in [N]-\mathcal{T}
				 \end{array}
				 \right.  			
				 \end{equation}
			\end{block}
						
					\column[]{0.45\columnwidth}
					\begin{figure}
						\centering
						\scalebox{0.35}{\input{cross_corr.tex}}
					\end{figure}
					
				\end{columns}
				
					
\end{frame}
%--------------------------------------------------------------------------------------
\def\fracty{0.45}
\def\fractx{0.8}
\begin{frame}{Main idea - $\rv = \underset{\text{\color{red} 3 } } {\mathcal{F}_{N}^{-1}} \ \{ \underset{\text{ \color{red} 1 } }{  \mathcal{F}_{N}\{\xv\}}  \odot \ \underset{\text{ \color{red} 2 } }{ \mathcal{F}_{N}\{\yv'\}}  \}$}

	\begin{figure}[t]
		\centering
		\includegraphics[width=4.8in]{Example_full_framework.pdf}
	\end{figure}
\end{frame}
%-----------------------------------------
\begin{frame}\frametitle{Sparse Fourier transform approach}
        \begin{block}{Robust Sparse Fourier Transform - Pawar and Ramchandran'14}
	 		\begin{itemize}
	 			\item[-] Sparse graph code approach
	 			\item[-] Computational complexity : $O(N \log N)$
	 		\end{itemize}
	 	\end{block}
\pause
\begin{block}{Key modifications}
   \begin{itemize}
   	\item Optimized for the induced noise model
   	\item Correlation peak is always {\color{blue} positive}
   	\item Take advantage in decoding algorithm - {\color{blue}sub-linear} time complexity
   \end{itemize}
\end{block}
\pause
        \begin{block}{Faster GPS receiver - Hassanieh '12}
	 		\begin{itemize}
	 			\item[-] Exploited sparsity in Correlation function $R_{XY}$
                \item[-] Algorithm based on hashing
                \item[-] In this application, complexity is still $O(N \log N)$
	 		\end{itemize}		
	 	\end{block}	 	


\end{frame}
%%---------------------------------------------------------
\begin{frame}{DSP - a quick review}
\begin{block}{Subsampling $\Leftrightarrow$ aliasing - Poisson summation formula}
\begin{itemize}
  \item Let $x[n] \xrightarrow{N-DFT} X[k] , \ \ k,n = 0,1, \ldots,N-1$
  \item Let $x_{s}[m]  = x[mL] , \ \ m = 0,1, \ldots, N/L=M$ be a sub-sampled signal
  \item Let $x_s[m] \xrightarrow{M-DFT} X_s[l]$ be the DFT of the sub-sampled signal
  \item $\boxed{X_s[l] = M\sum\limits_{p=0}^{L-1}X[l+pM]}$
\end{itemize}
\end{block}
\pause
\begin{block}{Shift in time $\Leftrightarrow$ multiplying by a complex exponential}
\begin{itemize}
  \item Let $x[n] \xrightarrow{N-DFT} X[k] , \ \ k,n = 0,1, \ldots,N-1$
  \item $x[n-n_0] \xrightarrow{N-DFT} e^{-\frac{j 2 \pi n_0 k}{N}} X[k]$
  %\item $x[n-1] \xrightarrow{N-DFT} \omega^k X[k]$
\end{itemize}
\end{block}
\pause
Shifting, sub-sampling $\xrightarrow{M-DFT}$ weighted linear combinations of $X[l+pM]$
\end{frame}

%--------------------------------------------------------------------------------------
	\begin{frame}{Aliasing \& graph codes (Pawar \& Ramchandran'13)}
	%\begin{itemize}
	%
	%\item Give a simple example that explains how aliasing can induce a Sparse Graph Code.\\ \item Introduce the Tanner Graph for the induced code here (No background required since it is covered in Part I).	
	%\end{itemize}
	
		\begin{block}{}
			\begin{figure}[t]
				\begin{center}
					\resizebox{1.0\textwidth}{!}{\input{X_DFT_bold.tex}}
				\end{center}
			\end{figure}
%			\begin{figure}[t]
%				\centering
%				%\includegraphics[width=3.1in]{C:/Users/nrkri/Dropbox/Work/RESEARCH/Presentations/NASIT2016/Figures/X_DFT}
%                \includegraphics[width=3.1in]{X_DFT}
%			\end{figure}
		\end{block}
		\pause
		\begin{columns}
			
			\column{.47\textwidth}
			\begin{block}{{\small $\color{red}x_s$:\ Sub-sampled by $f_1=P_1=2$}}
					\begin{figure}[t]
					\begin{center}
						\resizebox{1.0\textwidth}{!}{\input{Xs_bold.tex}}
					\end{center}
				\end{figure}
%				\begin{figure}[t]
%					\centering
%					%\includegraphics[width=2.3in]{C:/Users/nrkri/Dropbox/Work/RESEARCH/Presentations/NASIT2016/Figures/Xs}
%					\includegraphics[width=2.3in]{Xs}
%				\end{figure}
			\end{block}
			\pause
			\begin{block}{{\small$\color{red}z_s$:\ Sub-sampled by $f_2=P_2=3$}}
				
				\begin{figure}[t]
					\begin{center}
						\resizebox{1.0\textwidth}{!}{\input{Zs_bold.tex}}
					\end{center}
				\end{figure}
%				\begin{figure}[t]
%					\centering
%					%\includegraphics[width=2.3in]{C:/Users/nrkri/Dropbox/Work/RESEARCH/Presentations/NASIT2016/Figures/Zs}
%					\includegraphics[width=2.3in]{Zs}
%				\end{figure}
			\end{block}
			\pause
%			\column{.47\textwidth}
%			\begin{block}{\small Factor graph}
%				\begin{figure}[t]
%					\centering
%					%\includegraphics[width=2.3in]{C:/Users/nrkri/Dropbox/Work/RESEARCH/Presentations/NASIT2016/Figures/Factorgraph_example}
%					\includegraphics[width=2.3in]{Factorgraph_example}
%				\end{figure}

			\column{.47\textwidth}
			\begin{block}{\small Factor graph}
				\begin{figure}[t]
					\begin{center}
						\resizebox{1.0\textwidth}{!}{\input{Factorgraph_example_bold.tex}}
					\end{center}
				\end{figure}

			\end{block}
		\end{columns}
	\end{frame}
%%---------------------------------------------------------
	\begin{frame}{Aliasing and sparse graph codes}
	%\begin{itemize}
	%
	%\item Give a simple example that explains how aliasing can induce a Sparse Graph Code.\\ \item Introduce the Tanner Graph for the induced code here (No background required since it is covered in Part I).	
	%\end{itemize}
	
	\begin{block}{$N = P_1 \times P_2 = 2 \times 3$}
		\begin{figure}[t]
			\begin{center}
				\resizebox{1.0\textwidth}{!}{\input{X_DFT_bold.tex}}
			\end{center}
		\end{figure}
%		\begin{figure}[t]
%			\centering
%			\includegraphics[width=3.1in]{X_DFT}
%		\end{figure}
	\end{block}
	
	\begin{columns}
		
		\column{.47\textwidth}
		\begin{block}{{\small $\color{red}x_s$:\ Sub-sampled by $f_1=P_1=2$}}
				\begin{figure}[t]
				\begin{center}
					\resizebox{1.0\textwidth}{!}{\input{Xs_shift_bold.tex}}
				\end{center}
			\end{figure}
%			\begin{figure}[t]
%				\centering
%				\includegraphics[width=2.3in]{Xs_shift}
%			\end{figure}
		\end{block}
		
		\begin{block}{{\small$\color{red}z_s$:\ Sub-sampled by $f_2=P_2=3$}}
				\begin{figure}[t]
				\begin{center}
					\resizebox{1.0\textwidth}{!}{\input{Zs_shift_bold.tex}}
				\end{center}
			\end{figure}
%			\begin{figure}[t]
%				\centering
%				\includegraphics[width=2.3in]{Zs_shift}
%			\end{figure}
		\end{block}
		
		\column{.47\textwidth}
		\begin{block}{\small Factor graph}
			
			\begin{figure}[t]
				\begin{center}
					\resizebox{1.0\textwidth}{!}{\input{Factorgraph_example_tilde_bold.tex}}
					\end{center}
				\end{figure}
%			\begin{figure}[t]
%				\centering
%				\includegraphics[width=2.3in]{Factorgraph_example_tilde}
%			\end{figure}
		\end{block}
	\end{columns}
\end{frame}
	
	
	%--------------------------------------------------------------------------------------
	\begin{frame}{FFAST algorithm - Example}
	%Slide-1:
	%\begin{itemize}
	%	\item Block Diagram of a simple 2-stage FFAST setup.
	%	\item Tanner Graph of the induced code with the parity equations displayed.
	%\end{itemize}

\begin{figure}[t]
	\begin{center}
		\resizebox{0.75\textwidth}{!}{\input{FFAST_2stages_example.tex}}
	\end{center}
\end{figure}
	\vspace*{-4mm}
\begin{figure}[t]
	\begin{center}
		\resizebox{0.52\textwidth}{!}{\input{Factorgraph_example_tilde.tex}}
	\end{center}
\end{figure}
	
	\end{frame}
	%------------------------------------------------------------------------------------
	\begin{frame}{Singleton detection}
			
		%Slide-2: Singleton Detection
		%\begin{itemize}
		%	\item Tanner Graph of the induced code with the equations displayed
		%	\item Simple example explaining singleton detection (ratio test)
		%	\item Summarize the singleton detection condition. {\bf(put in slide-3 if needed)}
		%\end{itemize}
		
			\vspace{-5pt}
			\begin{columns}
				\column{0.55\columnwidth}
			\begin{figure}[t]
				\begin{center}
					\resizebox{0.8\textwidth}{!}{\input{Factorgraph_example_tilde.tex}}
				\end{center}
			\end{figure}
				\column{0.45\columnwidth}
				\begin{block}{Bin classification}
					\begin{itemize}
						\item \alert{Zeroton}:
						\vspace{3pt}
						$\begin{bmatrix}
							0 \\
							0
						\end{bmatrix}$
						\item \alert{Singleton:}
						\vspace{3pt}
%					\begin{align}
						$
						\begin{bmatrix}
						r[p_1] \\
						r[p_1]w^{p_1}
						\end{bmatrix}$
%					\end{align}
						
						\item \alert{Multiton:}\\
						\vspace{7pt}
%						\begin{align}
						$
						\begin{bmatrix}
						r[p_1]&+ \cdots &+&r[p_d] \\
						r[p_1]w^{p_1}&+\cdots &+&r[p_2]w^{p_d}
						\end{bmatrix}$
%						\end{align}
					\end{itemize}
				\end{block}
				
			\end{columns}
			\begin{block}{Singleton condition for a checknode}
			\begin{itemize}
				\item Let $i=\frac{N}{j2\pi} \log(\frac{\tilde{x}_s[l]}{x_s[l]})$. If {\color{blue} $0 \leq i \leq N-1$}, then checknode $l$ is a \alert{Singleton}.\\
				\item $Pos(l) = i$ is the only variable node participating and $X_s[l]$ is its value.
			\end{itemize}
				
			\end{block}
			
	\end{frame}	


	%-----------------------------------------------------------------------------------------
	\begin{frame}{FFAST decoder and peeling Process - Example}
	\only<1-2>{\begin{block}{Example 1}
		Let $N=6$, and the non-zero coefficients be r[0]=5, r[3]=4, r[4]=7
	    \end{block}	}
	    \vspace{-7pt}
		\begin{columns}
			\only<1-2>{
				\column{0.43\textwidth}
                \resizebox{2.0in}{!}
                {
                \input{Factorgraph_example_tilde_abbrv.tex}
                }

				\column{0.55\textwidth}

				\resizebox{2.3in}{!}
				{
					\input{Factorgraph_example1_tilde.tex}
				}
			}
				%\column{0.25\textwidth}
				%\only<3>{\large \color{green} Yes, recoverable!}			
		\end{columns}  	
	\end{frame}
%--------------------------------------------------------------------------------------------------------
\begin{frame}\frametitle{RSIDFT framework}
	\begin{itemize}
    \item Correlation function is not exactly sparse
    \item Small entries are inner products between two random vectors of length $M$
    \item $N = f_1 \ f_2 \ldots \ f_d$ where $f_i \approx N^\alpha$, $\alpha$ depends on $M$
    \item Subsample by $N^{1-\alpha} \Rightarrow$ aliasing from $N^{1-\alpha}$ coeffs
    \end{itemize}
		\begin{figure}[t!]
			\begin{center}
				\resizebox{0.6\textwidth}{!}{\input{FFAST_Robust_PM.tex}}
				%	 		\includegraphics[height=7cm]{Figures/FFAST_Robust}
			\end{center}	
			\label{fig:rsidft}
			\vspace{5 pt}
		\end{figure}
\end{frame}
%--------------------------------------------------------------------------------------------------------
\begin{frame}\frametitle{RSIDFT-Decoding (Peeling decoder)}
\begin{columns}
	\column[]{0.65\columnwidth}
		\begin{figure}[h!]
			\begin{center}
				%		\includegraphics[height=7cm]{Figures/Factorgraph}
				\resizebox{1.0\textwidth}{!}{\input{Factorgraph_PM.tex}}	
			\end{center}	
			%\caption{Example of a Tanner graph formed in a RSIDFT framework with system parameters being $d=2$, $B=2$, $N=6$, $f_1 = 2$ and $f_2=3$. The variable nodes (colored gray circles) represent the cross-correlation vector $\rv$ and the bin nodes (uncolored white boxes) represent the binned observation vector $\zv_{i,k}$. The figure also illustrates the relationship between $\zv_{i,k}$ and $\rv$.}\label{fig:factorgraph}
		\end{figure}
	\column[]{0.33\columnwidth}
	    \begin{block}{Observations:}
	          $
	    		\zv_{i,k} = \begin{bmatrix}
	    		r_{i,1}[k]\\
	    		r_{i,2}[k]\\
	    		\vdots\\
	    		r_{i,B}[k]
	    		\end{bmatrix}
	    		$
	    	\end{block}
	    	\begin{block}{Decoding- 3 steps}	
	    		\begin{enumerate}
	    			\item Bin Classification
	    			\item Position Identification
	    			\item Peeling Process
	    		\end{enumerate}
	
	    \end{block}
	
		
\end{columns}	
\end{frame}

\begin{frame}\frametitle{Observations}
\begin{align} \nonumber
	    		\zv_{i,k} = \begin{bmatrix}
	    		r_{i,1}[k]\\
	    		r_{i,2}[k]\\
	    		\vdots\\
	    		r_{i,B}[k]
	    		\end{bmatrix}			
            = \begin{bmatrix}
			1 & 1 & \ldots & 1 \\
			\omega^{k}_{2} & \omega^{(k+f_i)}_{2} & \ldots & \omega^{(k+(g_i-1)f_i)}_{2}   \\
			\vdots & \vdots & \ddots & \vdots\\
			\omega^{k}_{B} & \omega^{(k+f_i)}_{B} & \ldots & \omega^{(k+(g_i-1)f_i)}_{B} \\
			\end{bmatrix}
			\begin{bmatrix}
			r[k+(0)f_i] \\
			r[k+(1)f_i] \\
			\vdots\\
			r[k+(g_i-1)f_i]
			\end{bmatrix}
			\end{align}

			\begin{figure}[t]
			\begin{center}
				\includegraphics[width=3.0in]{bin_statistics.pdf}
			\end{center}
		\end{figure}
\begin{itemize}
  \item Classification - Set a threshold for $|r_{i,1}[k]|$
  \item 1-sparse recovery - Column that gives maximum |correlation with $\zv_{i,k}$|
  \item Peeling - standard message passing
\end{itemize}
\end{frame}
%----------------------------------------------------------------------------------------------------------------
\begin{frame}{Error analysis}
\begin{block}{Error probability}
			\begin{align*}
			\mbb{P}(\mathcal{E}_{\text{total}}) & \leq &  \mbb{P}(\mathcal{E}_1)~~~~~ & + & \mbb{P}(\mc{E}_2)~~~~~~~~ & + & \mbb{P}(\mc{E}_3)~~~~~~~~\\
			&\leq & 6e^{-\frac{N^{\mu+\alpha-1}(1-6\eta)^2}{16}}~ & + & 2e^{-N^{\mu+\alpha-1} ~ c_1(\eta)}~ & + &  e^{-c_3 N^{c_4\alpha}}
			\end{align*}
			\[\boxed{	\mbb{P}(\mathcal{E}_{\text{total}}) \rightarrow 0  ~~\text{if}~~  \alpha >1-\mu}\]
		\end{block}
	

\begin{block}{Error events}
		\begin{itemize}\small
				\item {\color{blue}$\mathcal{E}_1${-\it Bin Classification}}: Bin is wrongly classified
                    \begin{itemize}
                      \item Hoeffding tail bound
                    \end{itemize}
                    \pause
				\item {\color{blue}$\mathcal{E}_2${-\it Pos. Identification}}: Position of singleton is identified wrongly, given a singleton
                    \begin{itemize}
                      \item Mutual incoherence of sub-sampled DFT matrices
                    \end{itemize}
                    \pause
				\item {\color{blue}$\mathcal{E}_3${-\it Peeling Process}}: Peeling process fails to recover the $L$ significant correlation coefficients, given $\mbb{P}(\mc{E}_1)= \mbb{P}(\mc{E}_2)=0$
                    \begin{itemize}
                      \item Bounds from iterative decoding literature - density evolution
                    \end{itemize}
		\end{itemize}
	\end{block}
\end{frame}
%------------------------------------------------------------------------------------------------------------
\begin{frame}{Bin classification error}
\vspace{-10pt}
{ \small
	\begin{align*} \nonumber
	Z[1]=\begin{cases}
	\sum\limits_{\ell=0}^{g_{i}-1}\sum\limits_{k=0}^{M-1} n_{l,k}  & ~~\text{ if } ~~ \mc{H}=\mc{H}_z\label{Eqn:BinCombination}\\
	M_1+\sum\limits_{\ell=0}^{g_{i}-2}\sum\limits_{k=0}^{M-1} n_{l,k}  & ~~\text{ if } ~~ \mc{H}=\mc{H}_s\\
	M_1+M_2+\sum\limits_{\ell=0}^{g_{i}-3}\sum\limits_{k=0}^{M-1} n_{l,k}  & ~~\text{ if } ~~ \mc{H}=\mc{H}_d\\
	\end{cases}
	\end{align*}
	
	where $n_{l,k}=x[\theta_{\ell}+k]y[k]$,  $\theta_{\ell}\notin\{\tau_1,\tau_2,\ldots,\tau_L\}$, and  $M_1,M_2\in[M(1-2\eta):M]$.
}


\begin{block}{$\mathcal{E}_1${-\it Bin Classification}}
	\begin{figure}[t]
		\begin{center}
			\includegraphics[width=2.5in]{bin_statistics.pdf}
		\end{center}
	\end{figure}
	\vspace{-30pt}
	
	{\small \begin{align*}
		\mbb{P}[\mathcal{E}_1] & \leq & \mbb{P}[\mathcal{E}_1|\widehat{\mathcal{H}}_{i,j}=\mc{H}_z]~& + &
		\quad \mbb{P}[\mathcal{E}_1|\widehat{\mathcal{H}}_{i,j}=\mathcal{H}_s]~~~~~& + &
		\quad \mbb{P}[\mc{E}_1|\widehat{\mathcal{H}}_{i,j}=\mathcal{H}_d \cup \mathcal{H}_m]\\
		~& = & \mbb{P}[z[1]>\gamma_1] ~& + & (1- \mbb{P}[\gamma_1<z[1]<\gamma_2])  ~& + & \mbb{P}[z[1]<\gamma_2]~~~~~~\\
        & \leq & 6e^{-\frac{N^{\mu+\alpha-1}(1-6\eta)^2}{16}} \\
		\end{align*}		
	}
	\vspace{-15pt}
\end{block}
\end{frame}

%------------------------------------------------------------------------------------------------------------
\begin{frame}{Position identification error}

\begin{lemma}[Mutual Incoherence Bound for sub-sampled IDFT matrix (Pawar'14)]
	%	[Mutual Incoherence Bound for sub-sampled IDFT matrix (Pawar'14)]
	The mutual incoherence $\mu_{\text{max}}$ $(\mathbf{W_{i,k}})$ of the sensing matrix $\mathbf{W}_{i,k}$, with columns and rows sampled from IDFT matrix, and  $B$ shifts, is upper bounded by
	
	\[ \mu_{\text{max}} < 2\sqrt{\frac{\log(5N)}{B}} \]
\end{lemma}

\begin{lemma}
	For some constant $c_1 \in \mathbb{R}$ and the choice of $B=4c_1^2\log 5N$,  the probability of error in identifying the position of a singleton at any bin $(i,j)$ can be upper bounded by
	\begin{align*}
	\mbb{P}[\mc{E}_{21}]\leq \exp\left\lbrace-\frac{N^{\mu+\alpha-1}(1-2\eta)^2(c_1^2-1)}{8(c_1^2+1)}\right\rbrace
	\end{align*}
\end{lemma}

\end{frame}

%------------------------------------------------------------------------------------------------------------

\begin{frame}{Error in peeling process}

\begin{lemma}[Exact Matching]
For the exact matching case, choose $F^{d-1}=\delta N^\alpha$ where $\delta$ are thresholds from DE. Then the oracle based peeling decoder:
\begin{itemize}
\item successfully uncovers all the $L$ matching positions if $L=\Omega(N^{\alpha})$ and $L\leq N^{\alpha}$, with probability at least $1-O(1/N^{\frac{1}{d}})$
\item successfully uncovers all the $L$ matching positions, if $L=o(N^{\alpha})$, with probability at least $1-e^{-\beta \varepsilon_1^2N^{\alpha/(4l+1)}}$ for some constants $\beta,\varepsilon_1>0$ and $l>0.$
\end{itemize}\label{Lem:peeling_exact}
\end{lemma}

\begin{block}{$\mathcal{E}_3${-\it Peeling Process}}
\begin{itemize}
\item Tools from coding theory to analyze sparse graph codes
\item Density Evolution to quantify error probability
\item \# of check-nodes is a function of sparsity (query length)
\item Error probability decays with $N$ - {\small R-FFAST and SAFFRON [Pawar'14, Lee'15]}
\end{itemize}
\end{block}
\end{frame}
%----------------------------------------------------------------------------------------------------------------
\begin{frame}\frametitle{Simulation results}
\only<1>{\begin{figure}[h!]
		\begin{center}
			
			\resizebox{0.7\textwidth}{!}{\input{sim_results_M_10_7.tex}}	
				\end{center}
			\caption{Plot of Probability of missing a match vs. Sample gain for exact matching of a substring of length $M=10^5$($\mu=0.41$) from a equiprobable  binary \{+1,-1\} sequence of length $N= 10^{12}$, divided into $G=10^{5}$ blocks each of length $\tilde{N}=10^7$. The substring was simulated to repeat in $L=10^6$($\lambda=0.5$) locations uniformly at random.}
	
\end{figure}}

\only<2>{\begin{figure}[h!]
	\begin{center}
		\resizebox{0.7\textwidth}{!}{\input{sim_results_M_10_3.tex}}	
	\end{center}
	\caption{Plot of Probability of missing a match vs. Sample gain for exact matching of a substring of length $M=10^3$($\mu=0.25$) from a equiprobable  binary \{+1,-1\} sequence of length $N= 10^{12}$, divided into $G=10^{6}$ blocks each of length $\tilde{N}=10^6$. The substring was simulated to repeat in $L=10^6$($\lambda=0.5$) locations uniformly at random.}
	
\end{figure}}
\end{frame}
%------------------------------------------------------------------------------------------------------------
\begin{frame}\frametitle{Big data applications - word embeddings}
\begin{block}{word2vec}
\begin{itemize}
\item word2vec is a word embedding: words to vectors in a Euclidean space
\item For e.g., each word is converted to a 300 length vector
\item A text can be converted to a sequence of vectors
\end{itemize}
\end{block}
\begin{block}{Querying and text matching}
\begin{itemize}
  \item 5 chapters of Tale of two cities
  \item Query - With drooping heads and {\color{red}{trembling}} tails, they mashed their way
  \pause
  \item Answer - With drooping heads and {\color{blue}{tremulous}} tails, they mashed their way
\end{itemize}
\end{block}
\end{frame}


\begin{frame}\frametitle{Conclusions}
\begin{itemize}
\item We considered the approximate sub-string matching problem
\item Proposed a sparse-Fourier transform based algorithm
\item To the best of our knowledge, this has the best complexity orderwise
\item Several possible extensions
    \begin{itemize}
    \item Secure pattern matching
    \item What if data is not i.i.d?
    \item What if there are insertion/deletions?
    \item Can we do graph matching?
    \end{itemize}
\end{itemize}
\end{frame}

%------------------------------------------------------------------------------------------------------------
\begin{frame}\frametitle{Big data applications}
\begin{block}{Metabolomics}
\begin{itemize}
  \item Database of genomes contains a sequence $\vec{x}$ of length $N=10^{6}-10^{12}$
\end{itemize}

\end{block}
  \begin{figure}[h]
  \includegraphics[width=4.0in]{genomesizes.pdf}
  \end{figure}

\url{http://www.biology-pages.info/G/GenomeSizes.html}
\end{frame}
%------------------------------------------------------------------------------------------------------------
\begin{frame}\frametitle{Big data applications}
\begin{block}{Metabolomics}
\begin{itemize}
  \item Database of genomes contains a sequence $\vec{x}$ of length $N=10^{6}-10^{12}$
  \item $M=1000$ to $M = 100000$ gives $\mu = 0.25$ to $\mu = 0.42$
\end{itemize}

  \begin{figure}[h]
  \includegraphics[width=4.0in]{genomebiology.pdf}
  \end{figure}


\end{block}

\end{frame}

%------------------------------------------------------------------------------------------------------------
\begin{frame}\frametitle{Big data applications}
\begin{block}{Audio}
Let us consider the case of an audio signal which is sampled at a moderate sampling rate of $10$~KHz and let the signal $\xv$ correspond to 1 hour of audio and hence, $N = 3.6 \times 10^7$. Let the query $\yv$ be a 10 second clip which corresponds to $M = 10^4$ resulting in a $\mu = 0.529$.
\end{block}
\begin{block}{}
\begin{figure}
\includegraphics[width=3.5in]{ScaledVoice.pdf}
\end{figure}
\end{block}
\end{frame}
%-----------------References --------------------
%\begin{frame}[allowframebreaks]
%\frametitle{References}%in case more than 1 slide needed
%	{\footnotesize
%		\bibliographystyle{ieeetr}
%		\bibliography{sparseestimation}	}
%\end{frame}

\begin{frame}{Secure Pattern Matching}
v\begin{figure}
  \centering
  \includegraphics[width=4.0in]{airline}
\end{figure}
\end{frame}
%----------------------------------------------------------------------------------------------------------------
\begin{frame}{Complexity analysis}
	\begin{block}{Sample complexity}
		\vspace{-10pt}
		\begin{align*}
		\text{Total \# of samples required (S)} &= O \left(dBN^{\alpha}\right) =   {\color{blue}O(N^{1-\mu}\log N)}
		\end{align*}
	\end{block}
\pause
	\begin{block}{Computational complexity}
		\begin{equation*}\label{eqn:Rxy_fourier}
		\rv = \underset{\color{red}  \RNum{2} } {\mathcal{F}_{N}^{-1}} \ \{   \mathcal{F}_{N}\{\xv\}  \odot \ \underset{\color{red}  \RNum{1}  }{ \mathcal{F}_{N}\{\yv'\}}  \}
		\end{equation*}
		\vspace{-10pt}
\pause
	\begin{itemize}
		\item {\color{blue}Sketch of Query:}\\ \vspace{5pt}
	 {\small $C_{\color{red}\RNum{1}} \ = \  dB ~
	 ( \underset{\text{Folding} }{\underbrace{N^{\mu}}} + \ \
	 \underset{\text{Shorter FFTs} }{\underbrace{N^{\alpha} \ \log N^{\alpha}}} \ )
	 =  {\color{blue} O(\max(N^{\mu}\log N,N^{1-\mu}\log^2 N)) }$}
\pause
		\item {\color{blue}RSIDFT:} \\	{\small$C_{\color{red} \RNum{2}} =  {d B}  \left (
			\underset{\text{Shorter IFFTs /block/stage} }{\underbrace{ O(N^{\alpha}  \log N^{\alpha})}} \hspace{-3pt}+ \underset{\text{Correlations} }{\underbrace{ L~N^{1-\alpha}}} \right ) = {\color{blue} O(\max(N^{1-\mu}\log^2 N ,N^{\mu+\lambda}\log N)) }$}
	\end{itemize}
	\vspace{10pt}	
	\[\boxed{C_{\text{total}} = \max(C_{\color{red} \text{ \RNum{1}}},C_{\color{red} \text{ \RNum{2}}}) = {\color{blue} O(\max(N^{1-\mu}\log^2 N ,N^{\mu+\lambda}\log N)) }}\]
		
	\end{block}
\end{frame}
%--------------------------------------------------------------------------------------
\begin{frame}{Secure pattern matching}
\begin{figure}
  \centering
  \includegraphics[width=4.0in]{doctorpatient}
\end{figure}
\end{frame}

%---------------------------------------------------------------------------------------
\begin{frame}\frametitle{Questions?}
	\begin{figure}[t]
		\centering
		\includegraphics[width=2.8in]{questions}
	\end{figure}
	\centering
	\color{blue}
	\Huge{Thank you!}
\end{frame}

\end{document} 