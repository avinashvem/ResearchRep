\section{Description Of The Algorithm}
\label{sec:Algo_desc}
In this section, given the input string $\xv$ and the query string $\yv$, we describe our algorithm that finds the matching positions $\mathcal{T}\coleq \{\tau_1, \tau_2, \cdots \tau_L\}$ with sample and time complexities that are sub-linear in $N$. The main idea here is that the correlation vector $\rv$ is sparse (upto some noise) with dominant peaks at $L$ matching positions denoted by $\mc{T}$ and noise components at $N-L$ positions where the strings do not match. 

\begin{figure}[h!]
	\begin{center}
	 	\resizebox{0.75\textwidth}{!}{\input{./Figures/notional_diag.tex}}	
%		\includegraphics[height=3cm]{Figures/notional_diag} 
	\end{center}	   
	\caption{}\label{fig:notional}
	\vspace{5 pt}
\end{figure}

Consider the correlation signal $\rv$ in the case of exact matching:
\begin{equation} \label{eqn:RXY_sparse}
r[m] \ = \left\{
\begin{array}{ll}
  &M,~~  \text{if} \ m \in \mathcal{T} \\
  & n_m,~~ m \in [N]-\mathcal{T}
\end{array} 
\right.  
\end{equation}
where $n_m$ is the noise component that is induced due to correlation of two i.i.d. sequence of random variables each taking values from $\mathcal{A} := \{+1,-1\}$. The sparse vector $\rv$ can be computed indirectly using Fourier transform approach as shown below:
\begin{equation}\label{eqn:Rxy_fourier}
  \rv = \underset{\text{ \RNum{1} } } {\mathcal{F}_{N}^{-1}} \ \{ \underset{\text{ \RNum{2} } }{  \mathcal{F}_{N}\{\xv\}}  \odot \ \underset{\text{ \RNum{3} } }{ \mathcal{F}_{N}\{\yv'\}}  \} 
\end{equation} 
where $\mathcal{F}_{N}\{ \cdot \}$ and $\mathcal{F}_{N}^{-1}\{ \cdot \}$ refer to $N$-point discrete Fourier transform and its inverse respectively, $\odot$ is the point-wise multiplication operation and ${ y'[n]} = { y^{*}[-n]}$. Fig.~\ref{fig:notional} presents a notional schematic of our Algorithm. As evident from Eq.~\eqref{eqn:Rxy_fourier}, our algorithm for computing $\rv$ consists of three stages:
\begin{itemize}
\item Computing the sketch(FFT) $\Xv$ of $\xv$
\item Computing the sketch(FFT) $\Yv$ of $\yv$
\item Computing the IDFT of $\Rv$ given $\Xv$ and $\Yv$
\end{itemize}


	 \begin{figure}[t!]
	 	\begin{center}
	 	\resizebox{0.6\textwidth}{!}{\input{./Figures/FFAST_Robust.tex}}
%	 		\includegraphics[height=7cm]{Figures/FFAST_Robust} 
	 	\end{center}	   
	 	\caption{ RSIDFT Framework to compute inverse Fourier Transform of a signal $\Rv$ that is sparse in time domain. }\label{fig:rsidft}
	\vspace{5 pt}
	 \end{figure}

\subsection{Sparse Inverse Discrete Fourier Transform}
\label{subsec:RSIDFT}	
 In this section we present Robust Sparse Inverse Discrete Fourier Transform(RSIDFT) scheme that exploits sparsity in the cross-correlation signal $\rv$ and efficiently recovers it's $L$ dominant coefficients. The architecture of RSIDFT is similar to that of the FFAST scheme proposed in \cite{pawar2014robust}, but the decoding algorithm has some key modifications to handle the noise model induced in this problem. We will see in Sec.~\ref{subsec:skteches} how the sketches $\Xv$ and $\Yv$ are computed efficiently but for this section we will focus only on the recovery of the sparse coefficients in $\rv$ given $\Xv$ and $\Yv$.
	  	  
Consider the RSIDFT framework shown in Fig.~\ref{fig:rsidft}. Let $ \Rv =\Xv \odot \Yv'$ be the DFT of the cross-correlation signal of $\xv$ and $\yv$. The framework consists of {\it $d$-stages} with each stage correpsonding to a sub-sampling factor of $\frac{N}{f_i}$. The design scheme for choosing parameters $\{f_1,f_2,\ldots,f_d\}$ for various values of $\mu$ such that $f_i | N$ and $f_i=N^{\alpha}+O(1) \forall ~i\in[d]$ are given in \ref{subsec:DesignParameters}. In each stage, there are {\it $B$ branches} with shifts from the set $ \{s_1, s_2, \cdots s_B\} $, where $s_1 =0$ in the first branch and the rest are chosen randomly from $[N]$.
	   	 	
	 Given the input $\Rv$, in branch $j$ of $i^{\text{th}}$ stage of RSDIFT, referred to as \textit{branch $(i,j)$} for simplicity, RSIDFT sub-samples the signal $\Rv$ at
\begin{align}
\label{Eqn:SamplingSets}
	 \mathcal{S}_{i,j} \coleq \{s_j,\ s_j + n_i,\ s_j + 2n_i,\ \cdots s_j + (f_i-1) n_i\}, \quad i\in[d], j\in[B]
\end{align}
where $n_i\coleq \frac{N}{f_i}$ to obtain $\Rv_{i,j}\coleq\Rv[\mc{S}_{i,j}]$. The sub-sampling operation is followed by a $f_i$-point IDFT in each branch of stage-$i$ to obtain $ \rv_{i,j}$. Notice that $ \rv_{i,j}$ is an aliased version of $\rv$ due to the property that sub-sampling in Fourier domain is equivalent to aliasing in time domain.

	 Let $\zv_{i,k}\in\mbb{R}^{B}$, for $k\in [f_i]$, be the $k$th \textit{binned} observation vector of stage-$i$ formed by stacking $\rv_{i,j}[k], j\in[B]$, together as a vector i.e.
\[
	  \zv_{i,k} = \begin{bmatrix}
	 r_{i,1}[k] \\
	 r_{i,2}[k] \\
	 \vdots\\
	 r_{i,B}[k]
	 \end{bmatrix}  
\]
Using the properties of Fourier transfrom, we can write the relationship between the observation vectors $\zv_{i,j}$ at bin $(i,j)$ and sparse vector to be estimated $\rv$ as:
\begin{align}
	\label{Eqn:Generator Equation}
	\zv_{i,k}= \mb{W}_{i,k} \times
	\begin{bmatrix}
		r[k+(0)f_i] \\
		r[k+(1)f_i] \\
		\vdots\\
		r[k+(n_i-1)f_i]
	\end{bmatrix}
\end{align}

where we refer to $\mb{W}_{i,k}$ as the sensing matrix at bin $(i,k)$ and is defined as  
\begin{align}\label{Eqn:Sensing Matrix}
	\mb{W}_{i,k} = \left[\wv^{k},\wv^{k+f_i},\ldots, \wv^{k+(n_i-1)f_i}\right] \text{where} ~\wv^{k}=
	\begin{bmatrix}
		e^{\frac{j2\pi ks_1}{N}}\\
		e^{\frac{j2\pi ks_2}{N}}\\
		\vdots\\
		e^{\frac{j2\pi ks_B}{N}}
	\end{bmatrix}
\end{align}

\begin{figure}[h!]
	\begin{center}
%		\includegraphics[height=7cm]{Figures/Factorgraph} 
	 	\resizebox{0.75\textwidth}{!}{\input{./Figures/Factorgraph.tex}}	
	\end{center}	   
	\caption{Example of a Tanner graph formed in a RSIDFT framework with system parameters being $d=2$, $B=2$, $N=6$, $f_1 = 2$ and $f_2=3$. The variable nodes (colored gray circles) represent the cross-correlation vector $\rv$ and the bin nodes (uncolored white boxes) represent the binned observation vector $\zv_{i,k}$. The figure also illustrates the relationship between $\zv_{i,k}$ and $\rv$.}\label{fig:factorgraph}
	\vspace{5 pt}
\end{figure}

In Fig.~\ref{fig:factorgraph} we represent the relation between the set of observation vectors $\{\zv_{i,k},i\in[1:d],k\in[f_i]\}$ and $\rv$ via an example using Tanner graph. The nodes on the left, which we refer to as {\it variable nodes}, represent the $N$ elements of vector $\rv$. Similarly the nodes on the right, which we refer to as {\it bin nodes}, represent the $\sum_{i\leq d} f_i$ sub-sensing signals. We will now describe the decoding algorithm which takes the set of observation vectors $\{\zv_{i,k},i\in[1:d],k\in[f_i]\}$ at $df_i$ bins as input and estimates the $L$-sparse $\rv$.	 

\subsubsection{Decoder}		
	Observe from the Tanner graph that the degree of each variable node is $d$ and that of each bin node at stage $i$ is $n_i$. A variable node is referred to as non-zero if it corresponds to a matching position and as zero if it corresponds to a non-matching position. Note that even though the cross-correlation vector value corresponding to a non-matching position is not exactly zero but some negligible noise value we refer to them as zero variable nodes for simplicity. We refer to a bin node as {\it zero-ton} (or $\msc{H}_z$) if the number of non-zero variable nodes connected to the bin node is zero. The {\it singleton ($\msc{H}_s$), double-ton ($\msc{H}_d$) and multi-ton ($\msc{H}_m$)} bin nodes are defined similarly where the number of non-zero variable nodes connected are one, two and greater than two, respectively. The peeling decoder has the following three steps in the decoding process.

\paragraph*{Bin Classification} In this step a bin node is classified as a zero-ton or a singleton or a multi-ton. At bin $(i,j)$ the classification is done based on  comparing the first observation $z_{i,j}[1]$, which corresponds to zero shift, with a predefined threshold. For $z_{i,j}[1]=z$, the classification at bin $(i,j)$, $\widehat{\msc{H}}_{i,j}$, can be written as follows:
\begin{align}
\label{Eqn:BinClassifApprox}
\widehat{\msc{H}}_{i,j}=
\begin{cases}
\msc{H}_z &  	 z/M < \gamma_1\\
\msc{H}_s &	  \gamma_1 < z/M < \gamma_2  \\
\msc{H}_d  &    \gamma_2  < z/M <  \gamma_3\\ 
\msc{H}_m &      z/M > \gamma_3\\
\end{cases}
\end{align}
where $(\gamma_1,\gamma_2,\gamma_3)=(\frac{1-2\eta}{2},\frac{3-4\eta}{2},\frac{5-6\eta}{2})$. Note that for the case of exact matching $\eta=0$.
\paragraph*{Singleton decoding}
 If a bin node $(i,j)$ is classified as a singleton, we need to identify the position of the non-zero variable node connected to it. This is done by correlating the observation vector $\zv_{i,j}$ with each column of the sensing matrix  $\mb{W}_{i,j} \coleq [\wv^{j},\wv^{j+n_i},\ldots,   \wv^{j+(f_i-1)n_i}]$ and choose the index that maximizes the correlation value.
\begin{align*}
 \hat{k} = \underset{k\in\{j+l n_i\}}{\argmax}~~ \zv^{\dagger}_{i,j} \wv^{k}
\end{align*}
where $\dagger$ denotes the conjugate transpose. The value of the variable node connected to the singleton bin is estimated as follows:
 $$
 \hat{r}[\hat{k}]=M(1-\eta).
 $$
 Note that for the case of exact matching we know the value to be exactly equal to $M$ which is our estimate in the case of exact matching ($\eta=0)$. But in the case of approximate matching the actual value of $r[k]\in\{M(1-2\eta),\ldots,M-1,M\}$ and our estimate $ \hat{r}[\hat{k}]=M(1-\eta)$, the mid-point of the range, although is only a loose approximate it would suffice for our purposes as we are mainly interested in recovering only the positions of matches i.e. the indices of the sparse coefficients in $\rv$.			 
\paragraph*{Peeling Process} The peeling based decoder we employ consists of finding a singleton bin, then identify the single non-zero variable node connected to the bin, decoding it's value and removing ({\it peeling off}) it's contribution from all the bin nodes connected to that variable node. The main idea behind this decoding scheme is that hopefully the peeling off operation induces at least one more singleton bin and thus the process of peeling off can be repeated iteratively. Although the main idea is similar for exact matching and the approximate matching scenarios, there are some subtle differences in their implementation.\\
{\it Exact Matching}: In the case of exact matching, we remove the decoded variable node's contribution from all the bin nodes it is connected to.\\
{\it Approximate Matching}: In this case we remove the decoded variable node's contribution only from bins that are originally a singleton or a double-ton. We do not alter the bins which are classified to be multi-tons with degree more than two.   

{\it Note:} The differences in peeling implementation for exact and approximate matching cases is because unlike exact matching the approximate matching may result in non-zero error ($e_1$) between the actual correlation value $r[k]$ and the estimate $\hat{r}[\hat{k}]$, i.e. $e_1 = |r[k]- \hat{r}[\hat{k}]|$ with $0 \leq e_1 \leq \eta M$. This may lead to error propagation if we use the same decoding scheme as in exact matching. Hence to overcome this problem we impose a constraint on the type of bin nodes participating in the peeling process. 
	
We present the overall recovery algorithm, which comprises of {\it bin classification, singleton decoding} and {\it peeling process}, in the Algorithm. \ref{Algo:decoder} pseudo-code. Note that $\msc{N}(k)$ for variable node $k$ denote it's neighborhood i.e. the set of bins connected to $k^{\text{th}}$ variable node. 

\def\gap{4pt}
\begin{algorithm}[h!]
\caption{Peeling based recovery algorithm}
\label{Algo:decoder}
\begin{algorithmic}
\State Compute $\widehat{\msc{H}}_{i,j}$ for $i\in[d], j\in[f_i]$. See Eqn. \eqref{Eqn:BinClassifApprox} \hspace{26.5ex} {\it Bin Classification}
\vspace{\gap}
\While {$\exists~ i,j:\widehat{\msc{H}}_{i,j}= \msc{H}_s$,}
\vspace{\gap}
  \State $(\hat{k},\hat{r}[\hat{k}])=${\bf Singleton-Decoder}$(\zv_{i,j})$
\vspace{\gap}
  \State Assign $\hat{r}[\hat{k}]$ to $\hat{k}^{\text{th}}$ variable node
\vspace{1.5\gap}
  \For{$(i_0,j_0)\in\msc{N}(\hat{k})$}
\vspace{\gap}
	   \State $\zv_{i_0,j_0}\gets\zv_{i_0,j_0}-\hat{r}[\hat{k}]\wv^{\hat{k}}$   \hspace{45.5ex} {\it Exact Matching}
	   \vspace{\gap}
	   \State $\zv_{i_0,j_0}\gets\zv_{i_0,j_0}-\hat{r}[\hat{k}]\wv^{\hat{k}}$   \hspace{6ex} only if $\widehat{\msc{H}}_{i_0,j_0}=\msc{H}_s$ or $\msc{H}_d$ \hspace{14ex} {\it Approximate Matching}
	   \vspace{\gap}
	   \State Re-do the bin classification for $(i_0,j_0)$ and compute $\widehat{\msc{H}}_{i_0,j_0}$
      \EndFor
\EndWhile
\end{algorithmic}
\end{algorithm}

\begin{algorithm}[h!]
\caption{Singleton-Decoder}
\label{Algo:SingletonDecoder}
\begin{algorithmic}
\State{\bf Input:} $\zv_{i,j}$
\vspace{\gap}
\State{\bf Output:} $(\hat{k},\hat{r}[\hat{k}])$
\vspace{\gap}
\State Estimate singleton index to be $ \hat{k} = \underset{k\in\{j+l n_i\}}{\argmax}\  \zv^{\dagger}_{i,j} \wv^{k}$ 
\vspace{\gap}
  \State Estimate the value to be:$$ \hat{r}[\hat{k}]=
   \begin{cases}
   M & \text{ Exact Matching case}\\
  M-K & \text{ Approximate Matching case}
  \end{cases}
  $$
%  \vspace{\gap}
%  \State\hspace{23ex}           $\hat{r}[\hat{k}]=$ \hspace{10ex} Approximate Matching case
\end{algorithmic}
\end{algorithm}

\subsubsection{Choosing $f_i$ and $\alpha$ for various $\mu$}
\label{subsec:DesignParameters}
For a given value of $\mu$, we will describe how to choose the parameters $d$ and $f_i$.\\
 {\it Exact Matching} Find $d\in \mbb{N}\backslash \{1,2\}$ such that $\mu\in(\frac{1}{d},\frac{1}{d-1}]\cup[1-\frac{1}{d-1},1-\frac{1}{d})$\\
{\it Approximate Matching}: If $\mu\in(\frac{1}{8},\frac{7}{8})$, choose $d=8$. Else find $d\geq 8$ such that $\mu\in(\frac{1}{d},\frac{1}{d-1})\cup(1-\frac{1}{d-1},1-\frac{1}{d})$
\begin{itemize}
\item Find a factorization for signal length $N=\prod_{i=0}^{d-1} \mcl{P}_i$ such that the set of integers $\{\mcl{P}_0,\mcl{P}_1,\ldots,\mcl{P}_{d-1}\}$ are pairwise co-prime and all the $\mcl{P}_i$ are approximately equal
\item More precisely, let $\mcl{P}_i=\mathbf{F}+O(1) ~\forall i$ for some value $\mathbf{F}$and choose $f_i=N/\mcl{P}_i$
\item Note that our choice of design parameters results in $\alpha=1-\frac{1}{d}$ (recall $f_i\approx N^{\alpha}$)
\end{itemize} 
To summarize our choice of design parameters: In both the exact and approximate matching cases, for any $0<\mu<1$, we choose the down-sampling factors $f_i$ to be approximately equal to $N^{\alpha}$ in all the $d$-stages where $\alpha=\frac{d-1}{d}$ satisfies:
\begin{equation}
\label{Eqn:IneqMuAlpha}
\alpha>\max(\mu,1-\mu).
\end{equation}

\subsection{Sketches of $\Xv$ and $\Yv$} 
\label{subsec:skteches}		 
 As we have already seen in Sec.~\ref{subsec:RSIDFT} the RSDIFT framework requires the values of $\Rv(=\Xv\odot \Yv)$ only at indices $\mc{S}$ or in other words we need $\Xv$ and $\Yv$ only at the indices $\mc{S}$. We assume that the sketch of $\xv$, $ \Xv[\mc{S}]= \{X[i],i\in \mc{S}\}$ is pre-computed and stored in a database.

\subsubsection*{Computing the sketch of $\yv$}: For every new query $\yv$, only $\{\Yv'[\mc{S}_{i,j}],i\in[d],j\in[B]\}$ needs to be computed where the subsets $\mc{S}_{i,j}$ are defined in Eq. \eqref{Eqn:SamplingSets}. Naively, the FFT algorithm can be used to compute $\Yv'$ and the required subset of coefficients can be taken but this is inefficient and would be of $O(N \log N)$ complexity. Instead, we observe that $\Yv'[\mc{S}_{i,j}]$ is $\Yv'$ shifted by $s_j$ and sub-sampled by a factor of $n_i$. Thus for a given $(i,j)$ this corresponds to, in time domain, a point-wise multiplication by $[1,w_{s_j},w_{s_j}^2,\ldots,w_{s_j}^{N-1}]$ followed by \textit{folding} the signal into $f_i$ number of length $\frac{N}{f_i}$ signals and adding them up resulting in a single length-$n_i$ signal denoted by $\yv'_{i,j}$. Formally the \textit{folding} operation can be described as follows:
	  \begin{equation}
	  	\yv'_{i,j} = \sum \limits_{m = 0}^{n_i-1} \yv'[mf_i:(m+1)f_i-1] \odot [w_{s_j}^{mf_i},w_{s_j}^{mf_i+1},\ldots, w_{s_j}^{(m+1)f_i-1}],\qquad w_{s_j}=e^{-\frac{j2\pi s_j}{N}}.
	  \end{equation}
 Taking $n_i$-point DFT of $\yv'_{i,j}$ produces $\Yv'[\mc{S}_{i,j}]$ i.e. $\Yv'$ sampled only at $\mathcal{S}_{i,j}$ indices which is what we need in branch $(i,j)$. To obtain all the samples in $\mathcal{S}$ required for the RSDIFT framework, the \textit{folding} technique followed by a IDFT needs to be carried out for each $(i,j)$, for $i\in[d],j\in[B]$, a total of $dB$ times $N^{1-\alpha}$-point DFT.