\begin{frame}{Introduction}
\begin{block}{Message passing algorithms}
\begin{itemize}
  \item Remarkably successful in coding theory
  \item Used to design capacity-achieving codes/decoders for a variety of channels
  \item Tools have been developed to analyze their performance
\end{itemize}
\end{block}

\end{frame}
%-------------------------------------------------------
\begin{frame}{Two main goals}
\begin{block}{Goal 1}
 Review some developments in modern coding theory and show how to analyze the performance of a simple peeling decoder for the BEC and $p$-ary symmetric channels.
\end{block}
\pause
\begin{block}{Goal 2}
Show that the following problems have the same structure as channel coding problems and show how to use the \alert{peeling decoder} to solve them.
\end{block}

\begin{block}{Problems}
\begin{itemize}
%\item Rateless codes
\item \alert{Uncoordinated massive multiple access}
\item \alert{Sparse Fourier transform (SFT) computation}
\item Sparse Walsh-Hadamard transform computation	
\item \alert{Compressed sensing}
    \begin{itemize}
    \item Data stream computing
    \item Group testing
    \item Compressive phase retrieval
    \end{itemize}
%\item Sparse covariance estimation
\end{itemize}
\end{block}

%\begin{block}{Outline}
%\begin{itemize}
%  \item Tanner graph representations of codes, LDPC and LDGM ensembles
%  \item Introduce the peeling decoder for BEC
%  \item Analyze the performance of the decoder via density evolution
%  \item Provide a brief overview of how to design optimal code ensembles
%  \item Peeling decoder for the error channel via generalized LDPC/LDGM codes
%  \item Syndrome source coding is the same as error correction
%\end{itemize}
%\end{block}

%\begin{block}{Literature}
%Presented in detail in ``Modern Coding Theory", Richardson and Urbanke
%\end{block}
\end{frame}

%--------------------------------------------------------------------------------------
\begin{frame}{\only<2->{Remembering Sir David MacKay}}
\only<2->{David Mackay's rediscovery of LDPC codes and his very interesting book on Information Theory has undoubtedly had a big influence on the field.}
\begin{columns}
\column{0.5\textwidth}
\includegraphics[width=2.3in]{David_MacKay}
\column{0.5\textwidth}
\only<2->{\includegraphics[width=2.0in]{Mackaybook}}
\end{columns}
\end{frame}
%%--------------------------------------------------------------------------------------
%\begin{frame}{Part II}
%\begin{block}{Goal for Part II}
%Show that the following problems have the same structure as the erasure correction, error correction or syndrome source coding problem and show how to use the \alert{peeling decoder} can be used to solve them
%\end{block}
%
%\begin{block}{Problems}
%\begin{itemize}
%%\item Rateless codes
%\item \alert{Uncoordinated multiple access}
%\item \alert{Sparse Fourier transform (SFT) computation}
%\item Sparse Walsh-Hadamard transform computation	
%\item \alert{Compressed sensing}
%\item Data stream computing
%\item Group testing
%\item Compressive phase retrieval
%\item Sparse covariance estimation
%\end{itemize}
%\end{block}
%
%\end{frame}
%-------------------------------------------------------------
\begin{frame}{Binary erasure channel (BEC) and erasure correction}

\begin{figure}[t]
\centering
\scalebox{0.55}{\input{./Figures/BECsystemmodel.tex}}
\end{figure}

\begin{block}{Channel coding problem}
\begin{itemize}
\item Transmit a message $\underline{m} = [m_1, \ldots, m_k]^T$ through a binary erasure channel
\item Encode the $k$-bit message $\underline{m}$ into a $n$-bit codeword $\underline{x}$
\item Redundancy is measured in terms of rate of the code $R = k/n$
%\item For $p=2$, capacity $C = 1-H_2(\epsilon)$
%\item A sequence of codes $\{\mathcal{C^n}\}$ is capacity achieving if $P_e^n \rightarrow 0$ while the rate $R^n \rightarrow C$
\end{itemize}
\end{block}

\end{frame}

%--------------------------------------------------------------------------------------
\begin{frame}{Binary erasure channel (BEC) and erasure correction}
%\begin{figure}[t]
%\centering
%\includegraphics[width=3.0in]{ParySystemModel}
%\end{figure}
%\vspace*{-5mm}
%\begin{figure}[t]
%\centering
%\includegraphics[width=1.0in,angle=-90]{BECchannelmodel}
%\end{figure}
\vspace*{-3mm}
\begin{figure}[t]
\centering
\scalebox{0.55}{\input{./Figures/BECsystemmodel.tex}}
\end{figure}
\vspace*{-3mm}
\begin{block}{Capacity achieving sequence of codes}
\begin{itemize}
\pause \item Capacity $C(\epsilon) = 1-\epsilon$
\pause  \item A sequence of codes $\{\mathcal{C}^n\}$
  \item Probability of erasure $P_e^n$
  \item Rate $R^n$
  \item Capacity achieving if $P_e^n \rightarrow 0$ as $n \rightarrow \infty$ while $R^n \rightarrow C$
\pause  \item \alert{Find efficient encoders/decoders in terms encoding and decoding complexities}
\end{itemize}
\end{block}
\pause
\begin{block}{Significance of the erasure channel}
\begin{itemize}
  \item Introduced by Elias in 1954 as a toy example
%  \item Good model for transmission of packets over the Internet
  \item Has become the canonical model for coding theorists to gain insight
\end{itemize}
\end{block}


\end{frame}



%%-----------------------------------------------------		
%\begin{frame}{$(n,k)$ Binary linear block codes - basics}
%\begin{block}{Generator matrix - $\mathbf{G}$ is a $n \times k$ encoding matrix}	
%\[
%\begin{bmatrix}
%  g_{1,1} & \cdots & g_{k,l} \\
%  \vdots & \ddots & \vdots \\
%  \vdots & \ddots & \vdots \\
%  g_{n,1} &  & g_{k,l} \\
%\end{bmatrix}
%\begin{bmatrix}
%  m_1 \\
%  \vdots \\
%  m_k \\
%\end{bmatrix}
%=
%\begin{bmatrix}
%  x_1 \\
%  \vdots \\
%  \vdots \\
%  x_n \\
%\end{bmatrix}
%\pause
%\bigoplus
%\begin{bmatrix}
%  e_1 \\
%  \vdots \\
%  \vdots \\
%  e_n \\
%\end{bmatrix}
%=
%\begin{bmatrix}
%  r_1 \\
%  \vdots \\
%  \vdots \\
%  r_n \\
%\end{bmatrix}
%\]
%%\[ \mathbf{G}_{n \times k} \underline{m}_{k \times 1} = \underline{c}_{n \times 1} \oplus \underline{e}_{n \times 1} = \underline{r}_{n \times 1} \]
%\pause
%\begin{eqnarray}
%\nonumber \underline{x} & = & \mathbf{G} \underline{m} \\
%\nonumber \underline{r} & = & \underline{x} \oplus \underline{e}
%\end{eqnarray}
%\end{block}
%\pause
%\begin{block}{Parity check matrix - $\mathbf{H}$ is a $(n-k) \times n$ matrix such that $\mathbf{H} \mathbf{G} = \mathbf{0}$}
%\[
%\underline{y} = \mathbf{H} \underline{r} = \mathbf{H} \underline{x} \oplus \mathbf{H} \underline{e} = \underline{0} \oplus \mathbf{H} \underline{e}
%\]
%\end{block}
%\end{frame}

%-----------------------------------------------------		
\begin{frame}{$(n,k)$ Binary linear block codes - basics}
\begin{columns}[t]
\column{0.47\textwidth}
\begin{block}{$\mathbf{G}$ is a $n \times k$ generator matrix}	
\[
\begin{bmatrix}
  g_{1,1} & \cdots & g_{k,l} \\
  \vdots & \ddots & \vdots \\
  \vdots & \ddots & \vdots \\
  g_{n,1} &  & g_{k,l} \\
\end{bmatrix}
\begin{bmatrix}
  m_1 \\
  \vdots \\
  m_k \\
\end{bmatrix}
=
\begin{bmatrix}
  x_1 \\
  \vdots \\
  \vdots \\
  x_n \\
\end{bmatrix}
\]
\end{block}

\column{0.5\textwidth}
\begin{block}{Example - (6,3) code}
\[
\ \
\begin{bmatrix}
  1 & 0 & 0 \\
  0 & 1 & 0 \\
  0 & 0 & 1 \\
  1 & 0 & 1 \\
  1 & 1 & 0 \\
  0 & 1 & 1 \\
\end{bmatrix}
\begin{bmatrix}
  1 \\
  1 \\
  0 \\
\end{bmatrix}
=
\begin{bmatrix}
1\\
1\\
0\\
1\\
0\\
1\\
\end{bmatrix}
\]
\end{block}
\end{columns}
\pause
\begin{block}{Parity check matrix - $\mathbf{H}$ is a $(n-k) \times n$ matrix s.t. $\Hm \Gm = \mathbf{0} \Rightarrow \Hm \xv = 0$}
\[
\Hm = \begin{bmatrix}
  1 & 0 & 1 & 1 & 0 & 0 \\
  1 & 1 & 0 & 0 & 1 & 0 \\
  0 & 0 & 1 & 0 & 0 & 1 \\
\end{bmatrix}
\begin{bmatrix}
1\\
1\\
0\\
1\\
0\\
1\\
\end{bmatrix}
=
\begin{bmatrix}
0\\
0\\
0\\
\end{bmatrix}
\]
\end{block}
\end{frame}

%%--------------------------------------------------------------------------------------
%\begin{frame}{Brief survey of the literature}
%\begin{itemize}
%\item The binary erasure channel (BEC) introduced by Elias in 1954
%\pause
%\item Regular LDPC codes introduced by Gallager in 1962, 1963
%\pause
%\item Zyablov and Pinsker considered peeling decoder in 1974
%\pause
%\item Papers by Luby, Mitzenmacher, Shokrollahi, Spielman, Stemann 1997-2001
%\pause
%\item Rateless codes introduced by Luby in 2002
%\pause
%\item EXIT chart analysis by ten Brink'99 and Ashikmin, Kramer and ten Brink'04
%\pause
%\item Summarized in Modern coding theory book by Richardson and Urbanke
%\end{itemize}
%\end{frame}
%--------------------------------------------------------------------------------------
\begin{frame}{Tanner graph representation of codes}
\begin{columns}
\column{0.5\textwidth}
\includegraphics[width=2.3in,angle=-90]{paritycheckmatrix63code}
\column{0.5\textwidth}
\includegraphics[width=2.25in,angle=-90]{Tannergraph63code}
\end{columns}
\begin{block}{}
\begin{itemize}
    \item Gallager'63, Tanner'81
    \item Parity check matrix implies that $\Hm \xv = 0$
    \item Code constraints can be specified in terms of a bipartite (Tanner) graph
  %\item A code can be specified by giving the Tanner graph
\end{itemize}
\end{block}
\end{frame}
%--------------------------------------------------------------------------------------
\begin{frame}{Peeling decoder for the BEC}
\vspace{-5mm}
\begin{columns}
\column{0.5\textwidth}
\includegraphics[width=2.3in,angle=-90]{paritycheckmatrix63code}
\column{0.5\textwidth}
%\includegraphics[width=2.25in,angle=-90]{Tannergraph63codewitherasures}
\scalebox{1}{\input{./Figures/peelingdecoder_BEC.tex}}
\end{columns}
\begin{block}{}
\begin{itemize}
  \item Zyablov and Pinsker'74, Luby et al '95
  \item Remove edges incident on known variable nodes and adjust check node values
  \item If there is a check node with a \alert{single edge}, it can be recovered
\end{itemize}
\end{block}
\end{frame}
%--------------------------------------------------------------------------------------
\begin{frame}{Message passing decoder for the BEC}
\vspace{-5mm}
\begin{columns}
\column{0.5\textwidth}
%\includegraphics[width=2.3in,angle=-90]{paritycheckmatrix63code}
%\includegraphics[width=2.25in,angle=-90]{Tannergraph63codewitherasures}
\scalebox{1}{\input{./Figures/peelingdecoder_BEC.tex}}
\column{0.5\textwidth}
\includegraphics[width=2.25in,angle=-90]{Tannergraph63codewitherasures}
\end{columns}
\vspace{-5mm}
\begin{block}{}
\begin{itemize}
  \item Pass messages between variable nodes and check nodes along the edges
  \item Messages $\in \{\text{value of var node (NE), erasure (E)}\}$
  \item Var-to-check node message is NE if \alert{at least one incoming message is NE}
  \item Check-to-var node message is NE if \alert{all other incoming messages are NE}
\end{itemize}
\end{block}
\end{frame}
%--------------------------------------------------------------------------------------
\begin{frame}{Peeling decoder is a greedy decoder}
\vspace{-3mm}
\begin{columns}
\column{0.5\textwidth}
\[
\Hm = \begin{bmatrix}
      x_1 & x_2 & x_3 & x_4 & x_5 & x_6 \\
      1 & 1 & 1 & 1 & 0 & 0 \\
      1 & 1 & 0 & 0 & 1 & 0 \\
      0 & 1 & 1 & 0 & 0 & 1 \\
    \end{bmatrix}
\]
\begin{eqnarray*}
% \nonumber % Remove numbering (before each equation)
  x_1 \oplus x_2 \oplus x_3 \oplus x_4 &=& 0 \\
  x_1 \oplus x_2 \oplus x_5 &=& 0 \\
  x_2 \oplus x_3 \oplus x_6 &=& 0
\end{eqnarray*}
\column{0.5\textwidth}
\includegraphics[width=2.25in,angle=-90]{Tannergraph63codestoppingset}
\end{columns}
\pause
\vspace{-3mm}
\begin{columns}
\column{0.5\textwidth}
\begin{block}{Linearly independent set of equations}
\begin{eqnarray*}
% \nonumber % Remove numbering (before each equation)
  x_1 \oplus x_2 \oplus x_3 & = & x_4 \\
  x_1 \oplus x_2  &=& x_5 \\
  x_2 \oplus x_3  &=& x_6
\end{eqnarray*}
\end{block}
\column{0.5\textwidth}
\end{columns}
\end{frame}
%--------------------------------------------------------------------------------------
\begin{frame}{Degree distributions}
\begin{columns}
\begin{column}{0.43\textwidth}
\begin{center}
\includegraphics[width=2.0in,angle=-90]{Tannergraph63codestoppingset}
\end{center}
\end{column}
\begin{column}{0.57\textwidth}
\only<5>{
\begin{itemize}
%\item $L(x) = \frac{3}{6} x + \frac26 x^2 + \frac16 x^3$
%\vspace{3mm}
%\item $\lambda(x) = \frac{3}{10} + \frac{4}{10} x + \frac {3}{10} x^2$
%\vspace{3mm}
%\item $R(x) = \frac{2}{3}x^3 + \frac13 x^4$
%\vspace{3mm}
%\item $\rho(x) = \frac{6}{10} x^2+ \frac{4}{10} x^3$
%\item $l_{\text{avg}} = L'(1) = \frac{1}{\int_{0}^{1} \lambda(x) \ dx}$
%\item $r_{\text{avg}} = R'(1) = \frac{1}{\int_{0}^{1} \rho(x) \ dx}$
\item Rate - $r(\lambda,\rho) = 1-\frac{{l_{\text{avg}}}}{{r_{\text{avg}}}} = 1 - \frac{\int_{0}^{1} \rho(x) \ dx}{\int_{0}^{1} \lambda(x) \ dx}$
\end{itemize}
}
\end{column}
\end{columns}

\begin{itemize}
\item VN d.d. from node perspective - $L(x) = \sum_i L_i x^i = \frac{3}{6} x + \frac26 x^2 + \frac16 x^3$
\vspace{1mm}
\pause
\item VN d.d. from edge perspective - $\lambda(x) = \sum_i \lambda_i x^{i-1} = \frac{3}{10} + \frac{4}{10} x + \frac {3}{10} x^2$
\vspace{1mm}
\pause
\item CN d.d. from node perspective - $R(x) = \sum_i R_i x^i = \frac{2}{3}x^3 + \frac13 x^4 $
\vspace{1mm}
\pause
\item CN d.d. from edge perspective - $\rho(x) =\sum_i \rho_i x^{i-1} = \frac{6}{10} x^2+ \frac{4}{10} x^3$
\end{itemize}

\end{frame}
%--------------------------------------------------------------------------------------
\begin{frame}{LDPC code ensemble}
\begin{center}
  %\scalebox{1}{\input{./Figures/LDPC36ensemble.tex}}
\includegraphics[width=2.2in]{LDPCensemble}
\end{center}
\vspace*{-3mm}
\begin{block}{LDPC($n,\lambda,\rho$) ensemble}
\begin{itemize}
\item Ensemble of codes obtained by using different permutations $\pi$
\item Assume there is only one edge between every var node and check node
\item For every $n$, we get an ensemble of codes with the same $(\lambda,\rho)$
%\item Almost all codes in the ensemble have the same rate
\item \alert{Low density parity check} (LDPC) ensemble if graph is of low density
\end{itemize}
\end{block}
\end{frame}
%%--------------------------------------------------------------------------------------
%\begin{frame}{Rate of the code (ensemble)}
%\begin{itemize}
%  \item $l_{\text{avg}} = L'(1) = \frac{1}{\int_{0}^{1} \lambda(x) \ dx}$
%  \item $r_{\text{avg}} = R'(1) = \frac{1}{\int_{0}^{1} \rho(x) \ dx}$
%  \item Rate $\boxed{r(\lambda,\rho) = 1-\frac{{l_{\text{avg}}}}{{r_{\text{avg}}}} = 1 - \frac{\int_{0}^{1} \rho(x) \ dx}{\int_{0}^{1} \lambda(x) \ dx}}$
%\end{itemize}
%\end{frame}
%--------------------------------------------------------------------------------------
\begin{frame}{Analysis of the message passing decoder}
\begin{block}{}
\begin{itemize}
  \item If we pick a code uniformly at random from the LDPC$(n,\lambda,\rho)$ ensemble and use it over a BEC($\epsilon$) with $l$ iterations of message passing decoding, what will be the probability of erasure $P_{e}^n$ in the limit $l,n \rightarrow \infty$ ?
      \pause
    \begin{itemize}
      \item Analyze the average prob. of erasure over the ensemble
      \item For almost all realizations $P_{e}^n$ concentrates around the average
    \end{itemize}
\end{itemize}
\end{block}
\pause
\begin{block}{Relevant literature}
\begin{itemize}
    \item Papers by Luby, Mitzenmacher, Shokrollahi, Spielman, Stemann 97-'02
    \item Explained in Modern coding theory by Richardson and Urbanke
    \item Henry Pfister's course notes on his webpage
\end{itemize}
\end{block}
\end{frame}
%--------------------------------------------------------------------------------------
\begin{frame}{Analysis of the message passing decoder}
\begin{block}{Computation graph}
Computation graph $\mathcal{C}_{l}(x_1,\lambda,\rho)$ of bit $x_{1}$ of depth $l$ ($l$-iterations) is the neighborhood graph of node $x_1$ of radius $l$. \pause Consider the example $\mathcal{C}_{l=1}(\lambda(x)=x,\rho(x)=x^2)$
\end{block}

\begin{columns}
\begin{column}{0.33\textwidth}
\begin{center}
\scalebox{0.6}{\input{./Figures/compGraph1.tex}}
%$G_1$\\$Pr(\mathcal{C}_l(x_1)=G_1)=
\\$1-O(1/n)$
\end{center}
\end{column}

\begin{column}{0.33\textwidth}
\begin{center}
\scalebox{0.6}{\input{./Figures/compGraph2.tex}}
%$G_2$\\ \scriptsize{$Pr(\mathcal{C}_l(x_1)=G_2)=O(1/n)$}
\\$O(1/n)$
\end{center}
\end{column}

\begin{column}{0.33\textwidth}
\begin{center}
\scalebox{0.6}{\input{./Figures/compGraph3.tex}}
%$G_3$\\ $Pr(\mathcal{C}_l(x_1)=G_3)=O(1/n^2)$
\\$O(1/n^2)$
\end{center}
\end{column}

\end{columns}
\pause
\begin{block}{Computation tree}
For fixed $(l_{max},r_{max})$, in the limit of large block lengths a computation graph of depth-$l$ looks like a tree with high probability
\end{block}
\end{frame}
%--------------------------------------------------------------------------------------
\begin{frame}{Analysis of the message passing decoder}

\begin{block}{Computation Tree Ensemble-$\mathcal{T}_{l}(\lambda,\rho)$}
Ensemble of bipartite trees of depth $l$ rooted in a variable node (VN) where
\begin{itemize}
\item Root node has $i$ children(CN's) with probability $L_i$
\item Each VN has $i$ children(CN's) with probability $\lambda_i$
\item Each CN has $i$ children(VN's) with probability $\rho_i$
\end{itemize}
\end{block}

\begin{block}{Example: $\mathcal{C}_{l=1}(\lambda(x)=x,\rho(x)=x^2)$}
\begin{center}
\scalebox{0.6}{\input{./Figures/compGraph1.tex}}
\end{center}
\end{block}

\end{frame}
%---------------------------------------------------------------------------------------------------------------------------
\begin{frame}{Density evolution}
\begin{columns}
\begin{column}{0.65\textwidth}
\begin{centering}
\scalebox{0.73}{\input{./Figures/compTree_DE.tex}}
\end{centering}
\end{column}

\begin{column}{0.35\textwidth}
\begin{block}{Recall}
\begin{itemize}
  \item $\rho(x) = \sum_{i} \rho_i x^{i-1}$
  \item $\sum_i \rho_i = 1$
  \item $\lambda(x) = \sum_{i} \lambda_i x^{i-1}$
  \item $\sum_i \lambda_i = 1$
\end{itemize}
\end{block}
\begin{block}{Recursion}
\begin{eqnarray*}
  x_0 &=& \epsilon \\
\pause
  y_l &=& 1-\rho(1-x_{l-1}) \\
\pause
  x_l &=& \epsilon \lambda(y_l)\\
\pause
  x_l &=& \epsilon \lambda(1-\rho(1-x_{l-1}))
\end{eqnarray*}
\end{block}
\end{column}
\end{columns}
\end{frame}
%--------------------------------------------------------------------------------------
\begin{frame}{Analysis of the message passing decoder}
$\lambda(x)=x^2,\rho(x)=\rho_4 x^3+\rho_5 x^4$
\begin{columns}
\only<1,4>{
\begin{column}{0.33\textwidth}
\begin{center}
\scalebox{0.65}{\input{./Figures/compTree1.tex}}
\\ \scriptsize{$P(T)=\rho_4^2$}
%\\ \scriptsize{$P(T\in\mathcal{T}_1(\lambda,\rho))=\rho_4^2$}
\end{center}
\end{column}
}
\only<2,4>{
\begin{column}{0.33\textwidth}
\begin{center}
\scalebox{0.65}{\input{./Figures/compTree2.tex}}
\\ \scriptsize{$P(T)=2\rho_4\rho_5$}
%\\ \scriptsize{$P(T\in\mathcal{T}_1(\lambda,\rho))=2\rho_4\rho_5$}
\end{center}
\end{column}
}
\only<3,4>{
\begin{column}{0.3\textwidth}
\begin{center}
\scalebox{0.65}{\input{./Figures/compTree3.tex}}
\\ \scriptsize{$P(T)=\rho_5^2$}
%\\ \scriptsize{$P(T\in\mathcal{T}_1(\lambda,\rho))=\rho_5^2$}
\end{center}
\end{column}
}
\end{columns}
\only<4->{
\begin{block}{}
\begin{eqnarray*}
\mathbb{E}_{\text{LDPC}(\lambda,\rho)}[x_1]&=&\sum\limits_{T\in\mathcal{T}_{1}(\lambda,\rho)}P(T)*x_1(T,\epsilon)\\
&=&\epsilon(\rho_4 y_1^{(3)}+\rho_5 y_1^{(4)})^2\\
&=&\epsilon(1-\rho_4 (1-\epsilon)^{3}-\rho_5 (1-\epsilon)^{4})^2\\
&=&\epsilon\lambda\left(1-\rho(1-\epsilon)\right)
%&=&\epsilon(1-\rho(1-\epsilon))^2\\
\end{eqnarray*}
\end{block}
}
\end{frame}
%--------------------------------------------------------------------------------------
\begin{frame}{Threshold}
%\begin{block}{Convergence condition}
%%\begin{eqnarray*}
%%  x_0 &=& \epsilon \\
%%  y_l &=& 1-\rho(1-x_l) \\
%%  x_l &=& \epsilon \lambda(y_{l-1}) \\
%%  x_l &=& \epsilon \lambda(1-\rho(1-x_{l-1})) = f(\epsilon,x_{l-1})
%%\end{eqnarray*}
%
%\end{block}
\vspace{-2mm}
\begin{block}{Convergence condition}
\[ x_l  = \epsilon \lambda(1-\rho(1-x_{l-1})) = f(\epsilon,x_{l-1}) \]
\[
\begin{array}{rl}
\text{$x_l$ converges to 0 if} & f(\epsilon,x) < x, \ x \in (0,\epsilon] \\
\text{There is a fixed point if} & f(\epsilon,x) = x, \ \text{for some} \ x \in (0,\epsilon]
\end{array}
\]
\end{block}
\pause
\begin{center}
  \includegraphics[width=2.5in]{DE36sim}
\end{center}
\pause
\vspace*{-3mm}
\begin{block}{The threshold $\epsilon^{\text{BP}}(\lambda,\rho)$ is defined as}
\[
\epsilon^{\text{BP}}(\lambda,\rho) = \sup \{\epsilon \in [0,1]: x_l \rightarrow 0 \ \text{as} \ l \rightarrow \infty \}
\]
\end{block}

\end{frame}
%--------------------------------------------------------------------------------------
\begin{frame}{Exit charts - Ashikmin, Kramer, ten Brink'04}
\begin{block}{Node functions}
\begin{itemize}
\item Var node function: $v_{\epsilon}(x) = \epsilon \lambda(x)$
\item Check node function: $c(x) = 1- \rho(1-x)$
\end{itemize}
\end{block}
\begin{columns}
\begin{column}{0.47\textwidth}
\begin{center}
\scalebox{0.35}{\input{./Figures/EXIT_eps=0.35_regular(3,6).tex}}
\end{center}
\end{column}

\begin{column}{0.47\textwidth}
\begin{center}
\scalebox{0.35}{\input{./Figures/EXIT_regular(3,6).tex}}
\end{center}

\end{column}

\end{columns}
\end{frame}
%--------------------------------------------------------------------------------------
\begin{frame}{Optimality of EXIT chart matching}
\begin{itemize}
\item Var node function: $v_{\epsilon}(x) = \epsilon \lambda(x)$
\item Check node function: $c(x) = 1- \rho(1-x)$
\end{itemize}
\begin{center}
\scalebox{0.45}{\input{./Figures/EXIT_CapAch.tex}}
\end{center}
\end{frame}
%%------------------------------------------------------------------------------------
%\begin{frame}{Low density generator matrix (LDGM) codes}
%\vspace{-3mm}
%\begin{columns}
%\begin{column}{0.47\textwidth}
%\begin{center}
%\includegraphics[width=2.0in]{LDGMoverbec}
%\end{center}
%\end{column}
%\begin{column}{0.47\textwidth}
%\begin{itemize}
%\item $L(x) = \frac14 x + \frac14 x^2 + \frac12 x^3$
%\vspace{2mm}
%\item $\lambda(x) = \frac19 + \frac29 x + \frac 69 x^2$
%\vspace{2mm}
%\item $R(x) = \frac15 x + \frac45 x^2$
%\vspace{2mm}
%\item $\rho(x) = \frac19 + \frac89 x$
%\vspace{2mm}
%\item Rate $R = \frac{\int_{0}^{1}\lambda(x) \ dx}{\int_{0}^{1} \rho(x) \ dx}$
%\end{itemize}
%\end{column}
%\end{columns}
%
%\begin{columns}
%\column{0.45\textwidth}
%\begin{block}{DE for LDPC}
%\vspace*{-3mm}
%\begin{eqnarray*}
%  x_0 &=& \epsilon \\
%  y_l &=& 1-\rho(1-x_{l-1}) \\
%  x_l &=& \epsilon \lambda(y_l)\\
%  x_l &=& \epsilon \lambda(1-\rho(1-x_{l-1}))
%\end{eqnarray*}
%\end{block}
%
%\column{0.45\textwidth}
%\begin{block}{DE for LDGM}
%\vspace*{-3mm}
%\begin{eqnarray*}
%  x_0 &=& 1 \\
%  y_l &=& 1-(1-\epsilon) \rho(1-x_{l-1}) \\
%  x_l &=& \lambda(y_l) \\
%  x_l &=& \lambda(1-(1-\epsilon)\rho(1-x_{l-1}))
%\end{eqnarray*}
%\end{block}
%\end{columns}
%\end{frame}
%%-------------------------------------------------------------------------------------
%\begin{frame}{What are rateless codes?}
%\begin{center}
%  \includegraphics[width=3.5in]{ratelesssystem}
%\end{center}
%\begin{block}{Multicast with different channel qualities}
%\begin{itemize}
%\item Code should be simultaneously optimal for all $\epsilon$
%\item Want to create potentially endless stream of coded bits
%\end{itemize}
%\end{block}
%\pause
%\begin{block}{Downloading from multiple servers}
%\begin{itemize}
%\item Do not want to download the same coded bit multiple times
%\item Randomized encoding strategy is beneficial
%\end{itemize}
%\end{block}
%\end{frame}
%%--------------------------------------------------------------------------------------
%\begin{frame}{LDGM codes and rateless codes}
%\begin{center}
%  \includegraphics[width=2.75in]{ratelesserasures1}
%\end{center}
%
%\begin{block}{LT Codes, Luby'02}
%\begin{itemize}
%  \item Choose the degree according to a distribution $f_D$
%  \item Pick bits uniformly at random to make linear combination
%  \item Left degree is Poisson : $\lambda(x) = e^{-r_{avg}(1-x)}$
%\end{itemize}
%\end{block}
%\end{frame}
%%--------------------------------------------------------------------------------------
%\begin{frame}{Poisson, soliton pair is optimal for rateless codes}
%\vspace{-3mm}
%\begin{columns}
%\column{0.55\textwidth}
%\begin{center}
%\scalebox{0.45}{\input{./Figures/EXIT_LDGM_PoissSoliton.tex}}
%  %\includegraphics[width=2.2in]{EXIT_PoissSoliton}
%\end{center}
%\column{0.45\textwidth}
%\begin{center}
%  \includegraphics[width=2.2in]{ratelesserasures3}
%\end{center}
%\end{columns}
%%\begin{block}{Poisson, Soliton pair is optimal - $\lambda(x) = e^{-r_{avg}(1-x)}$}
%\begin{itemize}
%%\item Poisson, soliton pair is optimal
%  %\item Left degree is Poisson : $\lambda(x) = e^{-r_{avg}(1-x)}$
%  \item   $x = \lambda(1-(1-\epsilon)\rho(1-x))$
%  \item $\lambda(x) = e^{-\frac{\alpha}{1-\epsilon}(1-x)}$, \alert{optimal right degree is soliton: $\rho(x) = -\frac{1}{\alpha}\ln(1-x)$}
%  %\item \alert{Optimal distribution is soliton: $f_D[i] = \frac{1}{i(i-1)}$}
%\end{itemize}
%\begin{center}
%\begin{tabular}{|l|c|c|c|c|c|c|c|c|}
%\hline
%Degree of nodes & 1 & 2 & 3 & 4 & $\ldots$ & $i$ & \ldots & $K$ \\
%\hline
%Fraction: \alert{ $f_D[i]$ } & $\frac{1}{K}$ & $\frac12$ & $\frac16$ & $\frac{1}{12}$ & $\ldots$ & $\frac{1}{i(i-1)}$ & \ldots & $\frac{1}{K (K-1)}$ \\
%\hline
%\end{tabular}
%\end{center}
%%\end{block}
%\end{frame}
%%--------------------------------------------------------------------------------------
%\begin{frame}{Histogram of required $N$ for $K=10000$}
%\begin{center}
%  \includegraphics[width=4.2in]{fountaincodes10000histogram}
%\end{center}
%\begin{block}{Finite length considerations}
%\begin{itemize}
%  \item Deg. dist. must be adjusted for optimizing finite length performance
%  \item Raptor codes (Shokrollahi'06) is an excellent choice
%\end{itemize}
%\end{block}
%\end{frame}

%---------------------------------------------------------------------------------------
%\begin{frame}\frametitle{Questions?}
%	\begin{figure}[t]
%		\centering
%		\includegraphics[width=2.8in]{questions}
%	\end{figure}
%	\centering
%	\color{blue}
%	%\Huge{Thank you!}
%\end{frame}
%---------------------------------------------------------------------------------------
\begin{frame}{Summary}
\begin{itemize}
  \item Understand what degree distributions $(\lambda(x),\rho(x))$ mean
\pause
  \item Given a $(\lambda,\rho)$ and $\epsilon$, what will be the $P_e^n$ as $l,n \rightarrow \infty$ ?
\pause
  \item Can you compute the threshold?
\pause
  \item Is a $(\lambda(x),\rho(x))$ pair optimal?
\end{itemize}
\end{frame} 