\section{Performance Analysis}
\def\vgap{2pt}
\begin{lemma}[Chernoff Bound for bounded random variables]
\label{Lem:Chernoff}
Let $X_1, X_2,\ldots, X_n$ be a sequence of independent random variables such that $X_i \in\{-1, +1\}$ for all $i$ and $E[X_i]=\mu$ for all $i$. Then for any $\delta>0$:
\begin{align*}
\text{\textbf{Upper Tail}}: &\mbb{P}\left[\sum X_i-\mu\geq \delta\right]\leq e^{-\frac{n\delta^2}{2}}\\
\textbf{Lower Tail}: &\mbb{P}\left[\sum X_i-\mu\leq -\delta\right]\leq e^{-\frac{n\delta^2}{4}}
\end{align*}
\end{lemma}


In this section, we will analyze the overall probability of error involved in finding the correct position of match. This can be done by analyzing the following three error events and then using a union bound to bound the total probability of error.

\begin{itemize}
	\item $\mathcal{E}_1$: Event that a bin is wrongly classified  (Bin Classification)
	\item $\mathcal{E}_2$: Event that a position is wrongly identified  (Position Identification)
	\item $\mathcal{E}_3$: Event that the peeling process fails (Graph Analysis)
\end{itemize}

Let us analyze the three events independently. Let $\underline{Z}_{j} = (Z_{j}[1], Z_{j}[1], \cdots Z_{j}[B])$ denote the observation at bin $j$. 

\begin{remark}
\label{Lem:CorrelationCoefficient}
Let us consider $r[\theta_1],\ldots ,r[\theta_{f_i}]$ where $\theta_i \notin \{\tau_1,\ldots, \tau_L\}$ are not one of the matching positions. Then we can show that $\mbb{P}[x[\theta_i+k]y[k]=+1]$ with probability 1/2 and $\mbb{E}[x[\theta_i+k]y[k]]=0$. We also need to show that  the set of random variables  $\{x[\theta_i+k]y[k]: i\in\{1,2,\ldots f_i\},k\in[M]\}$ are independent. I was able to show this for the case of $M=3$. I don't see a simple way of extending this to the case of general $M$.
\end{remark}

\subsection{\bf Bin Classification}
We employ classification rules based only on the first element of the measurement vector at bin $(i,j)$ which can be given by
\begin{align}
Z[1]=\begin{cases}
\sum\limits_{\ell=0}^{f_{i}-1}\sum\limits_{k=0}^{M-1} n_{l,k}  & ~~\text{ if } ~~ \msc{H}=\msc{H}_z\label{Eqn:BinCombination}\\
\vspace{\vgap}
M_1+\sum\limits_{\ell=0}^{f_{i}-2}\sum\limits_{k=0}^{M-1} n_{l,k}  & ~~\text{ if } ~~ \msc{H}=\msc{H}_s\\
\vspace{\vgap}
M_1+M_2+\sum\limits_{\ell=0}^{f_{i}-3}\sum\limits_{k=0}^{M-1} n_{l,k}  & ~~\text{ if } ~~ \msc{H}=\msc{H}_d\\
\end{cases}
\end{align}
where $n_{l,k}=x[\theta_{\ell}+k]y[k]$ and $\theta_{\ell}\notin\{\tau_1,\tau_2,\ldots,\tau_L\}$. Also for the case of exact matching $M_1=M_2=M$ whereas in the case of approximate matching the values of $M_1,M_2\in[M(1-2\eta):M]$.

\begin{lemma}
The probability of bin classification error at any bin $(i,j)$ can be upper bounded by
\end{lemma}

\begin{proof}
\begin{align*}
\mbb{P}[\mc{E}_1]&=\mbb{P}[\msc{H}_z]\mbb{P}[\mc{E}_1|\widehat{\msc{H}}_{i,j}=\msc{H}_z]~+
						\quad \mbb{P}[\msc{H}_s]\mbb{P}[\mc{E}_1|\widehat{\msc{H}}_{i,j}=\msc{H}_s]~+
						\quad\mbb{P}[\msc{H}_d]\mbb{P}[\mc{E}_1|\widehat{\msc{H}}_{i,j}=\msc{H}_d \cup \msc{H}_m]\\
				&\leq \mbb{P}[\mc{E}_1|\widehat{\msc{H}}_{i,j}=\msc{H}_z]~+
						\quad \mbb{P}[\mc{E}_1|\widehat{\msc{H}}_{i,j}=\msc{H}_s]~+
						\quad \mbb{P}[\mc{E}_1|\widehat{\msc{H}}_{i,j}=\msc{H}_d \cup \msc{H}_m]\\
    			&\leq  e^{-\frac{N^{\mu-\alpha}(1-2\eta)^2}{8}}+2e^{-\frac{N^{\mu-\alpha}(1-4\eta)^2}{16}}+ 2e^{-\frac{N^{\mu-\alpha}(1-6\eta)^2}{16}}+e^{-\frac{N^{\mu-\alpha}(1-6\eta)^2}{16}}\\
    			&\leq 6e^{-\frac{N^{\mu-\alpha}(1-6\eta)^2}{16}}\\
						\end{align*}
						where the inequalities in the third line are due to Lemmas \ref{Lem:ZerotonClassif}, \ref{Lem:SingletonClassif}, \ref{Lem:DoubletonClassif} and \ref{Lem:MultitonClassif} respectively.
\end{proof}

\subsection{\bf Position Identification}
{\bf Exact Matching:}

\[
\mathbf{Z_i} = \begin{bmatrix}
\mathbf{s_1}       & \cdots   & \mathbf{s_p} &\cdots \ &\mathbf{s_A}
\end{bmatrix} \times
    \begin{bmatrix}
n_1 \\
\vdots \\
R_{XY}[p]\\
\vdots\\
n_j \\
\vdots\\
n_{A}\\
\end{bmatrix}
\]


where $n_j \sim \mathcal{N}(0,M)$, $j \neq p$.

\[\begin{array}{ll}
Z_i[1] \ &= \ R_{XY}[p] + \sum_{j \neq p}n_j \\
         &= \ R_{XY}[p] + n 
\end{array}
\]
where $n \sim \mathcal{N}(0,(N^\alpha-1)M)$. We can find the value of $R_{XY}[p]$ with probability of error $Pe_s \leq 2 e^{- \frac{(1-2\eta)^2N^{\mu-\alpha}}{8}}$.

Given that we know $R_{XY}[p]$, 

\[ \mathbf{Z_i} \ = \ R_{XY}[p] ~ \mathbf{s_p}+ \sum_{j \neq p}n_j \mathbf{s_j} \\
\]

The estimated position $p'$ is given by

\[ p' = \underset{l}{\argmax}\ \bf s^T_l ~ Z_i\]

Let us consider two cases:

{\textit{Case 1:} $p' = p$}
 \[
 \begin{array}{ll}
 c_1 \ &= \ \mathbf{s^T_{p'}} ~\mathbf{Z_i} \ = \ R_{XY}[p] {\bf ~s^T_{p'}~ s_p}+ \sum_{j \neq p}n_j {\bf ~s^T_{p'}~ s_j}\\
       &\leq B ~ M +  \sum_{j \neq p} 2 n_j ~ \sqrt{B\log(5N^{\alpha})}  
 \end{array} 
  \]
  
{\textit{Case 2:} $p' \neq p$}
\[
\begin{array}{ll}
c_2 \ &= \ \mathbf{s^T_{p'}} ~\mathbf{Z_i}\ = \ R_{XY}[p]  {\bf ~s^T_{p'}~ s_p}+ \sum_{j \neq p',p}n_j{\bf ~s^T_{p'}~ s_j} \ + \ n_{p'} {\bf ~s^T_{p'}~ s_p} \\
&\leq 2M ~ \sqrt{B\log(5N^{\alpha})} + \sum_{j \neq p',p} \ 2n_j ~ \sqrt{B\log(5N^{\alpha})} \ + \ n_{p'} B
\end{array} 
\]

\[
\begin{array}{ll}
c_1 - c_2 \ &=  \ R_{XY}[p](B - {\bf ~s^T_{p'}~ s_p}) \ - \ n_{p'}(B - {\bf ~s^T_{p'}~ s_p'}) ~ + ~ \sum_{j \neq p',p}n_j ({\bf ~s^T_{p}~ s_j} - {\bf ~s^T_{p'}~ s_j})   \\
&\leq R_{XY}[p] (B-\mu_{max})  \ - \ n_{p'}(B - \mu_{max}) ~ + ~ \sum_{j \neq p',p}n_j \mu_{max} \\
\end{array} 
\]


Notice that $c_1$ and $c_2$ are both Gaussian random variables given by

\[ \begin{array}{ll}
c_1 &\sim  \mathcal{N}(BM,4N^\alpha M B \log(5N)) \\
c_2 &\sim  \mathcal{N}(2M \sqrt{B\log(5N)}, M~B^2 + 4N^\alpha M B \log(5N))\\
c_1 - c_2 &\sim  \mathcal{N}(R_{XY}[p](B-\mu_{max}),M((B-\mu_{max})^2 + (N^{\alpha}-2)\mu_{max}^2)
\end{array}\]

The probability that event $\mathcal{E}_2$ happens, given that $\mathcal{E}_1$ doesn't happen is given by
\[ P(\mathcal{E}_2 / \bar{\mathcal{E}}_1 ) = Pr(c_2 > c_1) \]

%If $c_1$ and $c_2$ are independent, then 

%\[c1-c2 \sim \mathcal{N}(M(B-2\sqrt{B\log5n})\ ,\ 8MBN^\alpha\log5N +  B^2 ) \]

\[ P(\mathcal{E}_2 / \bar{\mathcal{E}}_1 ) \leq Q \left( \sqrt{\frac{R_{XY}^{2}[p](B-\mu_{max})^2}{M(B-\mu_{max})^2 + M(N^{\alpha}-2)\mu_{max}^2)}} \right) \]
	
	where $\mu_{max} = 2\sqrt{B \log 5 N^{\alpha}} $. If $B = O(\log 5 N^{\alpha})$, then the above equation reduces to

\[ P(\mathcal{E}_2 / \bar{\mathcal{E}}_1 ) \leq Q \left( \sqrt{\frac{M}{1 + 4 ~ (N^{\alpha}-2)}} \right) \]

If $\mu > \alpha$, the error vanishes.


{\bf Approximate Matching:}


\[
\mathbf{Z_i} = \begin{bmatrix}
\mathbf{s_1}       & \cdots   & \mathbf{s_p} &\cdots \ &\mathbf{s_A}
\end{bmatrix} \times
\begin{bmatrix}
n_1 \\
\vdots \\
R_{XY}[p]\\
\vdots\\
n_j \\
\vdots\\
n_{A}\\
\end{bmatrix}
\]


where $n_j \sim \mathcal{N}(0,M)$, $j \neq p$.

\[\begin{array}{ll}
Z_i[1] \ &= \ R_{XY}[p] + \sum_{j \neq p}n_j \\
&= \ R_{XY}[p] + n 
\end{array}
\]
where $n \sim \mathcal{N}(0,(N^\alpha-1)M)$. Once we identify a Singleton with probability of error $Pe_s \leq 2 e^{- \frac{(1-2\eta)^2N^{\mu-\alpha}}{8}}$, we can fix the value of $R_{XY}[p] = M(1-\eta/2)$ since we are only interested in finding the positions and not the exact value of correlations. 

Now that we know $R_{XY}[p]$, 

\[ \mathbf{Z_i} \ = \ R_{XY}[p] ~ \mathbf{s_p}+ \sum_{j \neq p}n_j \mathbf{s_j} \\
\]

The estimated position $p'$ is given by

\[ p' = \underset{l}{\argmax}\ \bf s^T_l ~ Z_i\]

Also, let $p_1$ be the position of the previously peeled node from this check-node. There can only be at most one peeled edge as we restrict our peeling process to a doubleton and do not consider other multi-tons.
 
Let us consider two cases:

{\textit{Case 1:} $p' = p$}
\[
\begin{array}{ll}
c_1 \ &= \ \mathbf{s^T_{p'}} ~\mathbf{Z_i} \ = \ R_{XY}[p] {\bf ~s^T_{p'}~ s_p}\ + \ \sum_{j \neq p,p_1}n_j {\bf ~s^T_{p'}~ s_j} \ \pm \  \frac{\eta M}{2} {\bf ~s^T_{p'} ~ s_{p_1}} \\
&\leq B ~ (M-\eta M/2) +  \sum_{j \neq p} 2 n_j ~ \mu_{max} \pm \frac{\eta M}{2} \mu_{max}
\end{array} 
\]

{\textit{Case 2:} $p' \neq p$}
\[
\begin{array}{ll}
c_2 \ &= \ \mathbf{s^T_{p'}} ~\mathbf{Z_i}\ = \ R_{XY}[p]  {\bf ~s^T_{p'}~ s_p}+ \sum_{j \neq p',p,p_1}n_j{\bf ~s^T_{p'}~ s_j} \ + \ n_{p'} {\bf ~s^T_{p'}~ s_p} \ \pm \ \frac{\eta M}{2} {\bf ~s^T_{p'}~ s_{p_1}}\\
&\leq (M-\eta M/2) ~ \mu_{max} + \sum_{j \neq p',p} \ 2n_j ~ \mu_{max} \ + \ n_{p'} B \pm \frac{\eta M}{2} \mu_{max}
\end{array} 
\]

\[
\begin{array}{ll}
c_1 - c_2 \ &=  \ R_{XY}[p](B - {\bf ~s^T_{p'}~ s_p}) \ - \ n_{p'}(B - {\bf ~s^T_{p'}~ s_p'}) ~ + ~ \sum_{j \neq p',p,p_1}n_j ({\bf ~s^T_{p}~ s_j} - {\bf ~s^T_{p'}~ s_j})  \pm \  \eta M {\bf ~s^T_{p'} ~ s_{p_1}}  \\
&\leq (M-\eta M/2)(B-\mu_{max})  \ - \ n_{p'}(B - \mu_{max}) ~ + ~ \sum_{j \neq p',p}n_j \mu_{max} \pm \eta M \mu_{max}\\
\end{array} 
\]


Notice that $c_1$ and $c_2$ are both Gaussian random variables given by

\[ \begin{array}{ll}
c_1 &\sim  \mathcal{N}(B ~ (M-\eta M/2) \pm \frac{\eta M}{2} \mu_{max} \ , \ 4N^\alpha M B \log(5N)) \\
c_2 &\sim  \mathcal{N}(2M \sqrt{B\log(5N)}\ , \ M~B^2 + 4N^\alpha M B \log(5N))\\
c_1 - c_2 &\sim  \mathcal{N}((M-\eta M/2)(B-\mu_{max}) \pm \eta M \mu_{max} \ , \ M((B-\mu_{max})^2 + (N^{\alpha}-2)\mu_{max}^2)
\end{array}\]

The probability that event $\mathcal{E}_2$ happens, given that $\mathcal{E}_1$ doesn't happen is given by
\[ P(\mathcal{E}_2 / \bar{\mathcal{E}}_1 ) = Pr(c_2 > c_1) \]

%If $c_1$ and $c_2$ are independent, then 

%\[c1-c2 \sim \mathcal{N}(M(B-2\sqrt{B\log5n})\ ,\ 8MBN^\alpha\log5N +  B^2 ) \]

\[ P(\mathcal{E}_2 / \bar{\mathcal{E}}_1 ) \leq Q \left( \sqrt{\frac{((M-\eta M/2)(B-\mu_{max}) \pm \eta M \mu_{max})^2}{M(B-\mu_{max})^2 + M(N^{\alpha}-2)\mu_{max}^2)}} \right) \]

where $\mu_{max} = 2\sqrt{B \log 5 N^{\alpha}} $. If $B = O(\log 5 N^{\alpha})$, then the above equation reduces to

\[ P(\mathcal{E}_2 / \bar{\mathcal{E}}_1 ) \leq Q \left( \sqrt{\frac{M(1-3\eta/2)^2}{1 + 4~ (N^{\alpha}-2)}} \right) \]

If $\mu > \alpha$, the error vanishes.